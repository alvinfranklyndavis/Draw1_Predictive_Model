{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ8hbh2bWwoQYXUcWkUzAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Project2023_v3/blob/main/GPT_4_Bard_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r_a3x9wp-p1",
        "outputId": "cd998317-7134-4ded-dad1-b83bf39d3850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.10.1\n",
            "    Uninstalling imbalanced-learn-0.10.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.10.1\n",
            "Successfully installed imbalanced-learn-0.11.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# STEP 1. INSTALL PACKAGES AND IMPORT DATA\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas gdown numpy matplotlib scikit-learn xgboost shap\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import KNNImputer\n",
        "import shap\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive (root directory of Google Drive)\n",
        "drive_dataset_directory = '/content/drive/My Drive/'\n",
        "\n",
        "# Define the local file path in Google Drive for the CSV\n",
        "drive_csv_path = os.path.join(drive_dataset_directory, 'Training_Testing_Hybrid_MA.csv')\n",
        "\n",
        "# Log the start of the dataset loading\n",
        "logger.info(\"Reading the dataset from Google Drive...\")\n",
        "\n",
        "# Read the dataset from Google Drive\n",
        "data = pd.read_csv(drive_csv_path)\n",
        "logger.info(\"Dataset loaded successfully from Google Drive.\")\n",
        "\n",
        "# Basic Data Exploration\n",
        "logger.info(\"Performing basic data exploration...\")\n",
        "logger.info(f\"Dataset Size: {data.shape}\")\n",
        "logger.info(f\"First 5 Rows:\\n{data.head()}\")\n",
        "logger.info(f\"Missing Values:\\n{data.isnull().sum()}\")\n",
        "\n",
        "# Proceed with further data processing...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2. PROCESS DATE FEATURES AND SET UP PREDICTION1 COLUMN\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Processing date features...\")\n",
        "\n",
        "# Display data types and check for missing values\n",
        "logger.info(\"Data types:\\n%s\", data.dtypes)\n",
        "logger.info(\"Missing values:\\n%s\", data.isnull().sum())\n",
        "\n",
        "# Calculate Moving Averages for specified columns\n",
        "window_size = 3\n",
        "columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "# Initialize moving average columns with default values (e.g., 0)\n",
        "for col in target_columns:\n",
        "    data[col] = 0\n",
        "\n",
        "try:\n",
        "    for col, target_col in zip(columns_to_average, target_columns):\n",
        "        data[target_col] = data[col].rolling(window=window_size, min_periods=1).mean()\n",
        "except Exception as e:\n",
        "    logger.error(\"Error in moving average calculation: %s\", e)\n",
        "\n",
        "# Create Target Variable Column for Prediction1\n",
        "data['Prediction1'] = np.nan\n",
        "\n",
        "# Keep only relevant columns for Prediction1\n",
        "selected_columns_p1 = ['Morning', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor']\n",
        "data_p1 = data[selected_columns_p1].assign(Prediction1=np.nan)\n",
        "\n",
        "logger.info(\"Saving the Prediction1 data to CSV...\")\n",
        "data_p1.to_csv('/content/train_data_prediction1.csv', index=False)\n",
        "logger.info(\"Date features processed successfully.\")\n",
        "logger.info(\"First few rows of Prediction1 data:\\n%s\", data_p1.head())\n"
      ],
      "metadata": {
        "id": "VshK68czukkS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using print statements to display missing values information\n",
        "print(\"Missing values in each column (excluding 'Prediction1'):\")\n",
        "print(data.drop(columns=['Prediction1']).isnull().sum())\n",
        "\n",
        "print(\"\\nPercentage of missing values in each column (excluding 'Prediction1'):\")\n",
        "print(data.drop(columns=['Prediction1']).isnull().mean() * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMiB868RwRYx",
        "outputId": "247de6f4-20ec-4769-e3db-b13704ab174d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column (excluding 'Prediction1'):\n",
            "Date                0\n",
            "Day of the Week     0\n",
            "Morning             0\n",
            "Prev_Week           0\n",
            "Rep_Prev_Week       0\n",
            "Prev_Entry          0\n",
            "Rep_Prev_Entry      0\n",
            "Mov_Avg_Mor         0\n",
            "Afternoon           0\n",
            "Prev_Week.1         0\n",
            "Rep_Prev_Week.1     0\n",
            "Prev_Entry.1        0\n",
            "Rep_Prev_Entry.1    0\n",
            "Mov_Avg_Aft         0\n",
            "Evening             0\n",
            "Prev_Week.2         0\n",
            "Rep_Prev_Week.2     0\n",
            "Prev_Entry.2        0\n",
            "Rep_Prev_Entry.2    0\n",
            "Mov_Avg_Eve         0\n",
            "Night               0\n",
            "Prev_Week.3         0\n",
            "Rep_Prev_Week.3     0\n",
            "Prev_Entry.3        0\n",
            "Rep_Prev_Entry.3    0\n",
            "Mov_Avg_Nig         0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column (excluding 'Prediction1'):\n",
            "Date                0.0\n",
            "Day of the Week     0.0\n",
            "Morning             0.0\n",
            "Prev_Week           0.0\n",
            "Rep_Prev_Week       0.0\n",
            "Prev_Entry          0.0\n",
            "Rep_Prev_Entry      0.0\n",
            "Mov_Avg_Mor         0.0\n",
            "Afternoon           0.0\n",
            "Prev_Week.1         0.0\n",
            "Rep_Prev_Week.1     0.0\n",
            "Prev_Entry.1        0.0\n",
            "Rep_Prev_Entry.1    0.0\n",
            "Mov_Avg_Aft         0.0\n",
            "Evening             0.0\n",
            "Prev_Week.2         0.0\n",
            "Rep_Prev_Week.2     0.0\n",
            "Prev_Entry.2        0.0\n",
            "Rep_Prev_Entry.2    0.0\n",
            "Mov_Avg_Eve         0.0\n",
            "Night               0.0\n",
            "Prev_Week.3         0.0\n",
            "Rep_Prev_Week.3     0.0\n",
            "Prev_Entry.3        0.0\n",
            "Rep_Prev_Entry.3    0.0\n",
            "Mov_Avg_Nig         0.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3.1. LOAD TRAIN DATAFRAME FOR PREDICTION1\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\n",
        "import joblib\n",
        "from imblearn.over_sampling import SMOTE  # For data augmentation\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the train dataframe\n",
        "train_data = pd.read_csv('/content/train_data_prediction1.csv')\n",
        "logger.info(\"Loading train dataframe for Prediction1...\")\n",
        "\n",
        "# Optional: Data Augmentation using SMOTE (if applicable)\n",
        "# smote = SMOTE()\n",
        "# X_p1, y_p1 = smote.fit_resample(X_p1, y_p1)\n",
        "\n",
        "# Separate features and the target variable for Prediction1\n",
        "X_p1 = train_data.drop('Prediction1', axis=1)  # Features\n",
        "y_p1 = train_data['Prediction1']  # Target variable\n",
        "\n",
        "logger.info(\"Train dataframe for Prediction1 loaded successfully.\")\n",
        "logger.info(f\"Data Shape - Features: {X_p1.shape}, Target: {y_p1.shape}\")\n",
        "\n",
        "# STEP 3.2. SPLIT DATA INTO TRAINING AND TESTING SETS FOR PREDICTION1\n",
        "\n",
        "print(f\"Number of samples in historical data: {len(historical_data)}\")\n",
        "\n",
        "random_seed = 42\n",
        "# Stratified split (adjust or remove if not applicable)\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
        "\n",
        "# Training on historical data with known 'Prediction1' values\n",
        "historical_data = train_data[train_data['Prediction1'].notna()]\n",
        "X_hist, y_hist = historical_data.drop('Prediction1', axis=1), historical_data['Prediction1']\n",
        "\n",
        "# Training and validation split\n",
        "for train_index, val_index in stratified_kfold.split(X_hist, y_hist):\n",
        "    X_train_hist, X_val_hist = X_hist.iloc[train_index], X_hist.iloc[val_index]\n",
        "    y_train_hist, y_val_hist = y_hist.iloc[train_index], y_hist.iloc[val_index]\n",
        "\n",
        "    # Initialize and train the Random Forest model\n",
        "    random_forest_model = RandomForestRegressor(random_state=random_seed)\n",
        "    random_forest_model.fit(X_train_hist, y_train_hist)\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    y_pred_val = random_forest_model.predict(X_val_hist)\n",
        "    mse_val = mean_squared_error(y_val_hist, y_pred_val)\n",
        "    r2_score_val = r2_score(y_val_hist, y_pred_val)\n",
        "\n",
        "    logger.info(f\"Validation MSE: {mse_val}, R2 Score: {r2_score_val}\")\n",
        "\n",
        "# Save the trained model\n",
        "model_path = '/content/random_forest_prediction_model_hist.pkl'\n",
        "joblib.dump(random_forest_model, model_path)\n",
        "logger.info(f\"RandomForest model trained on historical data saved to {model_path}.\")\n",
        "\n",
        "# STEP 3.4. APPLY MODEL TO CURRENT DATA\n",
        "\n",
        "# Apply the trained model to current data (where 'Prediction1' is NaN)\n",
        "current_data = train_data[train_data['Prediction1'].isna()]\n",
        "X_current = current_data.drop('Prediction1', axis=1)\n",
        "\n",
        "# Generate predictions for the current data\n",
        "y_pred_current = random_forest_model.predict(X_current)\n",
        "current_data['Prediction1'] = y_pred_current\n",
        "\n",
        "logger.info(\"Predictions applied to current data.\")\n",
        "\n",
        "# Optional: Save the current data with predictions\n",
        "current_data.to_csv('/content/current_data_with_predictions.csv', index=False)\n",
        "logger.info(\"Current data with predictions saved.\")\n",
        "\n",
        "logger.info(\"Model training and application to current data completed.\")\n"
      ],
      "metadata": {
        "id": "0CNfE2mY1nB3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "2f9aa5ff-21d1-49af-dc1c-9367088d2869"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in historical data: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-703d418dfc3c>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Training and validation split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstratified_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mX_train_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0my_train_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4.1. MODEL INTERPRETATION\n",
        "\n",
        "# Check for NaN values in y_test_p1\n",
        "nan_count = y_test_p1.isnull().sum()\n",
        "logger.info(f\"Number of NaN values in y_test_p1: {nan_count}\")\n",
        "\n",
        "# If NaN values exist, print some examples\n",
        "if nan_count > 0:\n",
        "    logger.info(\"Examples of NaN values in y_test_p1:\")\n",
        "    logger.info(y_test_p1[y_test_p1.isnull()])\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the finalized model\n",
        "final_model = joblib.load('/content/random_forest_prediction_model_imputed.pkl')\n",
        "\n",
        "# Apply the same imputation and feature modification to X_test_p1\n",
        "imputed_test = imputer.transform(X_test_p1)\n",
        "X_test_p1_imputed, missing_indicator_test = imputed_test[:, :-1], imputed_test[:, -1]\n",
        "X_test_p1_imputed['Target_Missing'] = missing_indicator_test\n",
        "\n",
        "# Using SHAP to interpret the model\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_test_p1_imputed)\n",
        "shap.summary_plot(shap_values, X_test_p1_imputed, plot_type=\"bar\")\n",
        "\n",
        "# STEP 4.2. FINAL MODEL SELECTION AND REPORTING\n",
        "\n",
        "# Report the chosen model's evaluation metrics\n",
        "y_pred_final = final_model.predict(X_test_p1_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_p1, y_pred_final)\n",
        "precision = precision_score(y_test_p1, y_pred_final, average='macro')\n",
        "recall = recall_score(y_test_p1, y_pred_final, average='macro')\n",
        "f1 = f1_score(y_test_p1, y_pred_final, average='macro')\n",
        "\n",
        "logger.info(f\"Model Performance Metrics:\\n Accuracy: {accuracy}\\n Precision: {precision}\\n Recall: {recall}\\n F1 Score: {f1}\")\n",
        "\n",
        "# Add cross-validation implementation here if applicable\n",
        "# ...code for cross-validation...\n",
        "\n",
        "# Analysis of model performance\n",
        "# ...code/logic for detailed analysis of errors, biases, etc...\n",
        "\n",
        "# STEP 4.3. PREPARATION FOR DEPLOYMENT\n",
        "\n",
        "# ...code for deployment preparation...\n",
        "# Assuming the model will be deployed in a specific environment\n",
        "# Include any necessary steps for preparing the model for deployment\n",
        "# This might include serialization, testing the model in a deployment-like environment, etc.\n",
        "\n",
        "# STEP 4.4. DOCUMENTATION AND REPORTING\n",
        "\n",
        "# Prepare a comprehensive report\n",
        "report = f\"\"\"\n",
        "Model Selection Rationale:\n",
        "- The chosen model (e.g., Random Forest) was selected due to its superior performance in terms of accuracy, precision, and recall.\n",
        "\n",
        "Model Performance:\n",
        "- Accuracy: {accuracy}\n",
        "- Precision: {precision}\n",
        "- Recall: {recall}\n",
        "- F1-Score: {f1}\n",
        "\n",
        "Additional Model Analysis:\n",
        "- Detailed error analysis, biases, etc.\n",
        "- Results from cross-validation (if performed).\n",
        "\n",
        "Limitations and Recommendations:\n",
        "- The model may have limitations in terms of scalability or real-time prediction.\n",
        "- Future work could explore more advanced models or feature engineering techniques.\n",
        "\n",
        "Deployment Steps:\n",
        "- The model will be deployed in a cloud-based environment.\n",
        "- Necessary steps for deployment include serialization and environment setup.\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Model documentation and reporting completed.\")\n",
        "\n",
        "# Final Checks and Tests (if applicable)\n",
        "# Include any additional code for final testing or checks before deployment\n",
        "\n",
        "logger.info(\"Final checks and tests completed.\")\n",
        "\n",
        "logger.info(\"Cell 4 tasks completed successfully.\")\n"
      ],
      "metadata": {
        "id": "V28FJ39EHF8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5. CROSS-VALIDATION AND ADDITIONAL METRICS ANALYSIS\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming you are using RandomForestRegressor as your model\n",
        "model = RandomForestRegressor(random_state=random_seed)\n",
        "\n",
        "# Define your scoring metrics\n",
        "# Remove ROC AUC score if your task is regression\n",
        "scoring_metrics = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='macro'),\n",
        "    'recall': make_scorer(recall_score, average='macro'),\n",
        "    'f1': make_scorer(f1_score, average='macro')\n",
        "}\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "k_folds = 10  # Number of folds\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    scores = cross_val_score(model, X_p1, y_p1.fillna(-999), scoring=scorer, cv=k_folds)\n",
        "    cv_results[metric_name] = scores\n",
        "    logger.info(f\"{metric_name} scores for each fold: {scores}\")\n",
        "    logger.info(f\"Average {metric_name} over {k_folds} folds: {scores.mean()}\")\n",
        "\n",
        "# Additional metrics analysis and error/bias exploration\n",
        "# ... Add your code for detailed analysis of errors, biases, etc. ...\n",
        "logger.info(\"Cross-validation and additional metrics analysis completed.\")\n",
        "\n",
        "# Feature Importance Analysis using SHAP\n",
        "# Assuming 'final_prediction_model' is your trained RandomForestRegressor model\n",
        "import shap\n",
        "\n",
        "# Load the trained model (if not already loaded)\n",
        "final_prediction_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(final_prediction_model)\n",
        "shap_values = explainer.shap_values(X_train_p1)\n",
        "\n",
        "# Plot summary plot using SHAP values\n",
        "shap.summary_plot(shap_values, X_train_p1)\n",
        "\n",
        "logger.info(\"Feature importance analysis using SHAP completed.\")"
      ],
      "metadata": {
        "id": "YupgSJuhz3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6. DETAILED ERROR AND BIAS ANALYSIS\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive to access the predictions_df.csv file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in Google Drive where the predictions DataFrame is saved\n",
        "predictions_df_path = '/content/drive/My Drive/predictions_df.csv'\n",
        "\n",
        "# Load the DataFrame from the CSV file if it exists, otherwise create it\n",
        "if os.path.exists(predictions_df_path):\n",
        "    predictions_df = pd.read_csv(predictions_df_path)\n",
        "    logger.info(\"predictions_df loaded from Google Drive successfully.\")\n",
        "else:\n",
        "    # Make sure final_model is loaded\n",
        "    final_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred_final = final_model.predict(X_test_p1)\n",
        "\n",
        "    # Create the predictions DataFrame\n",
        "    predictions_df = pd.DataFrame({'Actual': y_test_p1, 'Predicted': y_pred_final})\n",
        "\n",
        "    # Save the new predictions_df to Google Drive for future use\n",
        "    predictions_df.to_csv(predictions_df_path, index=False)\n",
        "    logger.info(\"predictions_df saved to Google Drive successfully.\")\n",
        "\n",
        "# Merge 'Prev_Week' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Week']], left_index=True, right_index=True)\n",
        "# Merge 'Prev_Entry' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Entry']], left_index=True, right_index=True)\n",
        "\n",
        "# Proceed with error analysis only if predictions_df is loaded or created\n",
        "if 'predictions_df' in locals():\n",
        "    # Analyze error distribution\n",
        "    predictions_df['Error'] = predictions_df['Predicted'] - predictions_df['Actual']\n",
        "    predictions_df['Absolute_Error'] = predictions_df['Error'].abs()\n",
        "\n",
        "    # Plotting error distribution\n",
        "    plt.hist(predictions_df['Error'], bins=30)\n",
        "    plt.title('Error Distribution')\n",
        "    plt.xlabel('Prediction Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Week'\n",
        "prev_week_performance = predictions_df.groupby('Prev_Week').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_week_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Week')\n",
        "plt.xlabel('Previous Week Draw')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Entry'\n",
        "prev_entry_performance = predictions_df.groupby('Prev_Entry').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_entry_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Entry')\n",
        "plt.xlabel('Previous Entry')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Document findings\n",
        "error_bias_report = \"\"\"\n",
        "Detailed Error Analysis:\n",
        "- Error Distribution Insights: {'Describe your findings from the error distribution here'}\n",
        "- Largest Errors: {'Describe characteristics of instances with largest errors here'}\n",
        "\n",
        "Bias Exploration:\n",
        "- Performance by Previous Week: {'Describe performance variations based on the previous week here'}\n",
        "- Performance by Previous Entry: {'Describe performance variations based on the previous entry here'}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Error and bias analysis completed.\")\n",
        "logger.info(error_bias_report)\n"
      ],
      "metadata": {
        "id": "mr21I98826sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7. FINAL REVIEW, DEPLOYMENT PREPARATION, AND DOCUMENTATION\n",
        "\n",
        "# Final Model Review and Refinement\n",
        "# ... Code/comments for any last adjustments to the model ...\n",
        "\n",
        "# Deployment Preparation\n",
        "# Serialize the final model\n",
        "joblib.dump(final_prediction_model, '/content/final_prediction_model_for_deployment.pkl')\n",
        "\n",
        "# Comprehensive Documentation Update\n",
        "# ... Update your comprehensive report with all final findings and methodologies ...\n",
        "\n",
        "# Final Checks and Tests\n",
        "# ... Code/comments for final tests and checks ...\n",
        "\n",
        "# Planning for Future Improvements\n",
        "future_improvement_plan = \"\"\"\n",
        "Future Improvement Plans:\n",
        "- Areas for further research: {describe areas for future research}\n",
        "- Methodologies to explore: {describe potential methodologies for future iterations}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Final review and deployment preparation completed.\")\n",
        "logger.info(future_improvement_plan)\n"
      ],
      "metadata": {
        "id": "qssa3uxO9tru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "u8Y77oV7koS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "WsQJRnWtkpPO"
      }
    }
  ]
}