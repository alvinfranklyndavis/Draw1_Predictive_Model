{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5pnVIcnYNkSqv1be9Ti2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Project2023_v3/blob/main/GPT_4_Bard_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.1: Package Installation\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas gdown numpy matplotlib scikit-learn xgboost shap\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install black  # Install Black for code formatting\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plnc-ffhAUCk",
        "outputId": "4cfb7767-662f-4abf-cb13-c3d13df2d6f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (23.12.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (23.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.5.0)\n",
            "Requirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from black) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.7.4->black) (3.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.2: Import Libraries and Set Up Logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import KNNImputer\n",
        "import shap\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "fP_Q74gUBGQi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.3: Load Data from Google Drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive (root directory of Google Drive)\n",
        "drive_dataset_directory = '/content/drive/My Drive/'\n",
        "\n",
        "# Define the path to the CSV file\n",
        "drive_csv_path = os.path.join(drive_dataset_directory, 'initial_data.csv')\n",
        "\n",
        "# Log the start of dataset loading\n",
        "logger.info(\"Reading the dataset from Google Drive...\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(drive_csv_path)\n",
        "logger.info(\"Dataset loaded successfully from Google Drive.\")\n",
        "\n",
        "# Basic Data Exploration\n",
        "logger.info(\"Performing basic data exploration...\")\n",
        "logger.info(f\"Dataset Size: {data.shape}\")\n",
        "logger.info(f\"First 5 Rows:\\n{data.head()}\")\n",
        "logger.info(f\"Missing Values:\\n{data.isnull().sum()}\")\n",
        "\n",
        "# Proceed with further data processing..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNvqJzaKBU7l",
        "outputId": "53bd46d4-6d6b-4e43-e898-71e3e7e28c08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.1: Date Feature Processing and Prediction1 Setup\n",
        "\n",
        "# Import necessary libraries\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Processing date features...\")\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# Convert 'Date' to datetime and extract 'Year', 'Month', and 'Day'\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "data['Day'] = data['Date'].dt.day\n",
        "\n",
        "# Display data types and check for missing values\n",
        "logger.info(\"Data types:\\n%s\", data.dtypes)\n",
        "logger.info(\"Missing values:\\n%s\", data.isnull().sum())\n",
        "\n",
        "# Calculate Moving Averages for specified columns\n",
        "window_size = 3\n",
        "columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "# Initialize moving average columns with default values (e.g., 0)\n",
        "for col in target_columns:\n",
        "    data[col] = 0\n",
        "\n",
        "try:\n",
        "    for col, target_col in zip(columns_to_average, target_columns):\n",
        "        data[target_col] = data[col].rolling(window=window_size, min_periods=1).mean()\n",
        "except Exception as e:\n",
        "    logger.error(\"Error in moving average calculation: %s\", e)\n",
        "\n",
        "# Adjust entries to use previous day's data\n",
        "data['Prev_Morning'] = data['Morning'].shift(1)\n",
        "data['Prev_Afternoon'] = data['Afternoon'].shift(1)\n",
        "data['Prev_Evening'] = data['Evening'].shift(1)\n",
        "\n",
        "# Keep only relevant columns for Prediction1\n",
        "selected_columns = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "X = data[selected_columns]\n",
        "y = data['Morning']  # Using 'Morning' as the target variable\n",
        "\n",
        "logger.info(\"Saving the Prediction1 data to CSV...\")\n",
        "X.to_csv('/content/processed_date_features_for_prediction1.csv', index=False)\n",
        "y.to_csv('/content/target_variable_for_prediction1.csv', index=False)\n",
        "logger.info(\"Date features processed and saved successfully.\")\n",
        "logger.info(\"First few rows of feature data:\\n%s\", X.head())\n",
        "logger.info(\"First few rows of target data:\\n%s\", y.head())\n"
      ],
      "metadata": {
        "id": "GGRPjzS2ILM3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.2: Additional Data Insights\n",
        "\n",
        "logger.info(\"Exploring additional data insights...\")\n",
        "\n",
        "# Print the number of missing values in each column\n",
        "print(\"Missing values in each column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Print the percentage of missing values in each column\n",
        "print(\"\\nPercentage of missing values in each column:\")\n",
        "print(data.isnull().mean() * 100)\n",
        "\n",
        "# Print summary statistics of the features (X)\n",
        "print(\"\\nSummary statistics of features (X):\")\n",
        "print(X.describe())\n",
        "\n",
        "# Print summary statistics of the target variable (y)\n",
        "print(\"\\nSummary statistics of target variable (y):\")\n",
        "print(y.describe())\n",
        "\n",
        "# Print first few rows of the features (X)\n",
        "print(\"\\nFirst few rows of feature data (X):\")\n",
        "print(X.head())\n",
        "\n",
        "# Print first few rows of the target variable (y)\n",
        "print(\"\\nFirst few rows of target data (y):\")\n",
        "print(y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQoG9bDsPOtj",
        "outputId": "3b8d0538-5793-42e5-8a03-77d3a93220c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Date                0\n",
            "Day of the Week     0\n",
            "Morning             0\n",
            "Prev_Week           0\n",
            "Rep_Prev_Week       0\n",
            "Prev_Entry          0\n",
            "Rep_Prev_Entry      0\n",
            "Mov_Avg_Mor         0\n",
            "Afternoon           0\n",
            "Prev_Week.1         0\n",
            "Rep_Prev_Week.1     0\n",
            "Prev_Entry.1        0\n",
            "Rep_Prev_Entry.1    0\n",
            "Mov_Avg_Aft         0\n",
            "Evening             0\n",
            "Prev_Week.2         0\n",
            "Rep_Prev_Week.2     0\n",
            "Prev_Entry.2        0\n",
            "Rep_Prev_Entry.2    0\n",
            "Mov_Avg_Eve         0\n",
            "Night               0\n",
            "Prev_Week.3         0\n",
            "Rep_Prev_Week.3     0\n",
            "Prev_Entry.3        0\n",
            "Rep_Prev_Entry.3    0\n",
            "Mov_Avg_Nig         0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        1\n",
            "Prev_Afternoon      1\n",
            "Prev_Evening        1\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Date                0.000000\n",
            "Day of the Week     0.000000\n",
            "Morning             0.000000\n",
            "Prev_Week           0.000000\n",
            "Rep_Prev_Week       0.000000\n",
            "Prev_Entry          0.000000\n",
            "Rep_Prev_Entry      0.000000\n",
            "Mov_Avg_Mor         0.000000\n",
            "Afternoon           0.000000\n",
            "Prev_Week.1         0.000000\n",
            "Rep_Prev_Week.1     0.000000\n",
            "Prev_Entry.1        0.000000\n",
            "Rep_Prev_Entry.1    0.000000\n",
            "Mov_Avg_Aft         0.000000\n",
            "Evening             0.000000\n",
            "Prev_Week.2         0.000000\n",
            "Rep_Prev_Week.2     0.000000\n",
            "Prev_Entry.2        0.000000\n",
            "Rep_Prev_Entry.2    0.000000\n",
            "Mov_Avg_Eve         0.000000\n",
            "Night               0.000000\n",
            "Prev_Week.3         0.000000\n",
            "Rep_Prev_Week.3     0.000000\n",
            "Prev_Entry.3        0.000000\n",
            "Rep_Prev_Entry.3    0.000000\n",
            "Mov_Avg_Nig         0.000000\n",
            "Year                0.000000\n",
            "Month               0.000000\n",
            "Day                 0.000000\n",
            "Prev_Morning        0.070972\n",
            "Prev_Afternoon      0.070972\n",
            "Prev_Evening        0.070972\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics of features (X):\n",
            "              Year        Month          Day    Prev_Week   Prev_Entry  \\\n",
            "count  1409.000000  1409.000000  1409.000000  1409.000000  1409.000000   \n",
            "mean   2020.575586     6.630234    15.609652    18.191625    18.122072   \n",
            "std       1.556739     3.563382     8.759058    10.576569    10.370515   \n",
            "min    2018.000000     1.000000     1.000000     0.000000     1.000000   \n",
            "25%    2019.000000     3.000000     8.000000     9.000000     9.000000   \n",
            "50%    2021.000000     7.000000    16.000000    18.000000    18.000000   \n",
            "75%    2022.000000    10.000000    23.000000    27.000000    27.000000   \n",
            "max    2023.000000    12.000000    31.000000    36.000000    36.000000   \n",
            "\n",
            "       Mov_Avg_Mor  Prev_Morning  Prev_Afternoon  Prev_Evening  \n",
            "count  1409.000000   1408.000000     1408.000000   1408.000000  \n",
            "mean     18.762716     18.762074       18.620739     18.535511  \n",
            "std       5.742935     10.278541       10.356869     10.239569  \n",
            "min       4.000000      1.000000        1.000000      1.000000  \n",
            "25%      14.666667     10.000000        9.000000     10.000000  \n",
            "50%      18.666667     19.000000       19.000000     19.000000  \n",
            "75%      22.666667     28.000000       27.000000     27.000000  \n",
            "max      35.333333     36.000000       36.000000     36.000000  \n",
            "\n",
            "Summary statistics of target variable (y):\n",
            "count    1409.000000\n",
            "mean       18.766501\n",
            "std        10.276234\n",
            "min         1.000000\n",
            "25%        10.000000\n",
            "50%        19.000000\n",
            "75%        28.000000\n",
            "max        36.000000\n",
            "Name: Morning, dtype: float64\n",
            "\n",
            "First few rows of feature data (X):\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    1          7          23    19.000000           NaN   \n",
            "1  2018      8    2         11           9    25.000000          19.0   \n",
            "2  2018      8    3         19          12    21.666667          31.0   \n",
            "3  2018      8    4         35          35    25.666667          15.0   \n",
            "4  2018      8    6         18          16    25.666667          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0             NaN           NaN  \n",
            "1            14.0          33.0  \n",
            "2             3.0          35.0  \n",
            "3             9.0          23.0  \n",
            "4            21.0          29.0  \n",
            "\n",
            "First few rows of target data (y):\n",
            "0    19\n",
            "1    31\n",
            "2    15\n",
            "3    31\n",
            "4    31\n",
            "Name: Morning, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.3: Setting Bounds for Numerical Range\n",
        "\n",
        "logger.info(\"Enforcing numerical bounds...\")\n",
        "\n",
        "# Define the columns that should have values in the range of 1 to 36\n",
        "columns_to_check = ['Morning', 'Afternoon', 'Evening', 'Night', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "\n",
        "# Loop through these columns and enforce the range\n",
        "for col in columns_to_check:\n",
        "    # Find values outside the range\n",
        "    outliers = data[(data[col] < 1) | (data[col] > 36)]\n",
        "\n",
        "    # Report if any outliers are found\n",
        "    if not outliers.empty:\n",
        "        print(f\"Outliers found in {col}:\")\n",
        "        print(outliers)\n",
        "\n",
        "    # Enforce the range by clipping values\n",
        "    data[col] = data[col].clip(lower=1, upper=36)\n",
        "\n",
        "# Ensure changes are reflected\n",
        "print(data[columns_to_check].describe())\n",
        "\n",
        "# Prepare the current data with NaNs in 'Prediction1' for testing\n",
        "current_data = data[selected_columns].copy()  # Use .copy() to create an independent copy\n",
        "current_data['Prediction1'] = np.nan  # Initialize 'Prediction1' with NaN\n",
        "\n",
        "# Save 'current_data' as a CSV file for loading in Step 3.1\n",
        "current_data.to_csv('/content/current_data_for_prediction1.csv', index=False)\n",
        "logger.info(\"Current data with 'Prediction1' as NaN saved as 'current_data_for_prediction1.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi7kZT26PrzA",
        "outputId": "82f2efbe-c671-4750-b276-64857d35279f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Morning    Afternoon      Evening        Night  Prev_Morning  \\\n",
            "count  1409.000000  1409.000000  1409.000000  1409.000000   1408.000000   \n",
            "mean     18.766501    18.613911    18.527324    18.109297     18.762074   \n",
            "std      10.276234    10.356362    10.240544    10.375588     10.278541   \n",
            "min       1.000000     1.000000     1.000000     1.000000      1.000000   \n",
            "25%      10.000000     9.000000    10.000000     9.000000     10.000000   \n",
            "50%      19.000000    19.000000    19.000000    18.000000     19.000000   \n",
            "75%      28.000000    27.000000    27.000000    27.000000     28.000000   \n",
            "max      36.000000    36.000000    36.000000    36.000000     36.000000   \n",
            "\n",
            "       Prev_Afternoon  Prev_Evening  \n",
            "count     1408.000000   1408.000000  \n",
            "mean        18.620739     18.535511  \n",
            "std         10.356869     10.239569  \n",
            "min          1.000000      1.000000  \n",
            "25%          9.000000     10.000000  \n",
            "50%         19.000000     19.000000  \n",
            "75%         27.000000     27.000000  \n",
            "max         36.000000     36.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.1: Data Preparation for Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading historical dataset for data preparation...\")\n",
        "\n",
        "# Load the historical dataset\n",
        "historical_data = pd.read_csv('/content/processed_date_features_for_prediction1.csv')\n",
        "y_hist = pd.read_csv('/content/target_variable_for_prediction1.csv')\n",
        "logger.info(\"Historical dataset loaded successfully.\")\n",
        "\n",
        "# Fill NaN values and exclude specific rows in the entire dataset\n",
        "historical_data['Prev_Morning'].fillna(18, inplace=True)\n",
        "historical_data['Prev_Afternoon'].fillna(18, inplace=True)\n",
        "historical_data['Prev_Evening'].fillna(18, inplace=True)\n",
        "\n",
        "# Exclude the first row and row 518 from the entire dataset\n",
        "historical_data = historical_data.iloc[1:].drop(index=518).reset_index(drop=True)\n",
        "y_hist = y_hist.iloc[1:].drop(index=518).reset_index(drop=True)\n",
        "\n",
        "# Split historical data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(historical_data, y_hist, test_size=0.20, random_state=42)\n",
        "logger.info(f\"Data split into training and validation sets. Training set size: {X_train.shape}, Validation set size: {X_val.shape}\")\n",
        "\n",
        "# Save the split datasets to CSV files\n",
        "X_train.to_csv('/content/inspected_X_train.csv', index=False)\n",
        "y_train.to_csv('/content/y_train_split_for_training.csv', index=False)\n",
        "X_val.to_csv('/content/inspected_X_val.csv', index=False)\n",
        "y_val.to_csv('/content/inspected_y_val.csv', index=False)\n",
        "\n",
        "logger.info(\"Datasets saved for manual inspection and model training.\")\n",
        "\n",
        "# Save the cleaned X_train and X_val for inspection\n",
        "X_train.to_csv('/content/inspected_X_train.csv', index=False)\n",
        "X_val.to_csv('/content/inspected_X_val.csv', index=False)\n",
        "logger.info(\"X_train and X_val saved for manual inspection.\")\n",
        "\n",
        "# Save the cleaned y_val for evaluation\n",
        "y_val.to_csv('/content/inspected_y_val.csv', index=False)\n",
        "logger.info(\"y_val saved for evaluation.\")\n",
        "\n",
        "# Load the current dataset with NaNs in 'Prediction1'\n",
        "current_data = pd.read_csv('/content/current_data_for_prediction1.csv')\n",
        "X_current = current_data.drop('Prediction1', axis=1)\n",
        "\n",
        "# Fill NaN values in the first row of specific columns of X_current\n",
        "X_current.at[0, 'Prev_Morning'] = 18\n",
        "X_current.at[0, 'Prev_Afternoon'] = 18\n",
        "X_current.at[0, 'Prev_Evening'] = 18\n",
        "\n",
        "# Save the prepared current_data\n",
        "current_data.to_csv('/content/prepared_current_data_for_prediction.csv', index=False)\n",
        "logger.info(\"Prepared current data saved.\")\n",
        "\n",
        "# Check for remaining NaNs in X_train and X_current\n",
        "logger.info(\"Checking for NaNs in X_train and X_current...\")\n",
        "logger.info(f\"NaN values in X_train: {X_train.isnull().sum()}\")\n",
        "logger.info(f\"NaN values in X_current: {X_current.isnull().sum()}\")\n"
      ],
      "metadata": {
        "id": "VshK68czukkS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.2: Model Training and Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading training and validation data for model training...\")\n",
        "\n",
        "# Load the training data\n",
        "X_train = pd.read_csv('/content/inspected_X_train.csv')\n",
        "y_train = pd.read_csv('/content/y_train_split_for_training.csv')\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "random_forest_model = RandomForestRegressor(random_state=42)\n",
        "random_forest_model.fit(X_train, y_train.values.ravel())\n",
        "logger.info(\"Random Forest model trained on historical data.\")\n",
        "\n",
        "# Load the validation set for model evaluation\n",
        "X_val = pd.read_csv('/content/inspected_X_val.csv')\n",
        "y_val = pd.read_csv('/content/inspected_y_val.csv')\n",
        "\n",
        "# Evaluation on the validation set\n",
        "y_val_pred = random_forest_model.predict(X_val)\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "r2_score_val = r2_score(y_val, y_val_pred)\n",
        "logger.info(f\"Validation MSE: {mse_val}, R2 Score: {r2_score_val}\")\n",
        "\n",
        "# Save the trained model\n",
        "model_path = '/content/random_forest_prediction_model.pkl'\n",
        "joblib.dump(random_forest_model, model_path)\n",
        "logger.info(\"Trained RandomForest model saved.\")\n",
        "\n",
        "# Load the prepared current_data for prediction\n",
        "current_data = pd.read_csv('/content/prepared_current_data_for_prediction.csv')\n",
        "X_current_for_prediction = current_data.drop('Prediction1', axis=1)\n",
        "\n",
        "# Generate predictions for the current data\n",
        "y_pred_current = random_forest_model.predict(X_current_for_prediction)\n",
        "current_data['Prediction1'] = y_pred_current\n",
        "\n",
        "# Save the current data with predictions\n",
        "current_data.to_csv('/content/current_data_with_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "oMiB868RwRYx",
        "outputId": "6e360ea5-85f7-40e3-d8e8-1d1d070fa6e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (1125, 9)\n",
            "Shape of y_train: (1125, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fc30c0216eec>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Generate predictions for the current data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0my_pred_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_current_for_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mcurrent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prediction1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    597\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[1;32m    598\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.3: NaN Check in 'Prediction1' (Optional)\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading current data with predictions for NaN check...\")\n",
        "\n",
        "# Load the current data with predictions\n",
        "current_data_with_predictions = pd.read_csv('/content/current_data_with_predictions.csv')\n",
        "\n",
        "# Check for NaN values in 'Prediction1'\n",
        "nan_count_prediction1 = current_data_with_predictions['Prediction1'].isnull().sum()\n",
        "logger.info(f\"Number of NaN values in 'Prediction1': {nan_count_prediction1}\")\n",
        "\n",
        "# Perform analysis of any remaining NaNs\n",
        "if nan_count_prediction1 > 0:\n",
        "    logger.info(\"Rows with NaN in 'Prediction1':\")\n",
        "    logger.info(current_data_with_predictions[current_data_with_predictions['Prediction1'].isnull()])\n",
        "\n",
        "    # Additional analysis or handling of NaNs can be added here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "lm8OBUPe0MN6",
        "outputId": "e4fdfe4c-7479-4170-adb9-acdbc9570947"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6466c9ef01d8>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the current data with predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcurrent_data_with_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/current_data_with_predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Check for NaN values in 'Prediction1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/current_data_with_predictions.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CNfE2mY1nB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5pt66epbDL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3UAPiSVEzIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZCCtfy7E0nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Check for NaN Values in All Axes\n",
        "\n",
        "# Check in X_train\n",
        "print(\"NaN values in X_train:\")\n",
        "print(X_train.isnull().sum())\n",
        "\n",
        "# Check in y_train\n",
        "print(\"\\nNaN values in y_train:\")\n",
        "print(y_train.isnull().sum())\n",
        "\n",
        "# Check in X_current\n",
        "print(\"\\nNaN values in X_current:\")\n",
        "print(X_current.isnull().sum())\n",
        "\n",
        "# Check in current_data\n",
        "print(\"\\nNaN values in current_data:\")\n",
        "print(current_data.isnull().sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "UDssvACI2Aul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4.1. MODEL INTERPRETATION\n",
        "\n",
        "# Check for NaN values in y_test_p1\n",
        "nan_count = y_test_p1.isnull().sum()\n",
        "logger.info(f\"Number of NaN values in y_test_p1: {nan_count}\")\n",
        "\n",
        "# If NaN values exist, print some examples\n",
        "if nan_count > 0:\n",
        "    logger.info(\"Examples of NaN values in y_test_p1:\")\n",
        "    logger.info(y_test_p1[y_test_p1.isnull()])\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the finalized model\n",
        "final_model = joblib.load('/content/random_forest_prediction_model_imputed.pkl')\n",
        "\n",
        "# Apply the same imputation and feature modification to X_test_p1\n",
        "imputed_test = imputer.transform(X_test_p1)\n",
        "X_test_p1_imputed, missing_indicator_test = imputed_test[:, :-1], imputed_test[:, -1]\n",
        "X_test_p1_imputed['Target_Missing'] = missing_indicator_test\n",
        "\n",
        "# Using SHAP to interpret the model\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_test_p1_imputed)\n",
        "shap.summary_plot(shap_values, X_test_p1_imputed, plot_type=\"bar\")\n",
        "\n",
        "# STEP 4.2. FINAL MODEL SELECTION AND REPORTING\n",
        "\n",
        "# Report the chosen model's evaluation metrics\n",
        "y_pred_final = final_model.predict(X_test_p1_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_p1, y_pred_final)\n",
        "precision = precision_score(y_test_p1, y_pred_final, average='macro')\n",
        "recall = recall_score(y_test_p1, y_pred_final, average='macro')\n",
        "f1 = f1_score(y_test_p1, y_pred_final, average='macro')\n",
        "\n",
        "logger.info(f\"Model Performance Metrics:\\n Accuracy: {accuracy}\\n Precision: {precision}\\n Recall: {recall}\\n F1 Score: {f1}\")\n",
        "\n",
        "# Add cross-validation implementation here if applicable\n",
        "# ...code for cross-validation...\n",
        "\n",
        "# Analysis of model performance\n",
        "# ...code/logic for detailed analysis of errors, biases, etc...\n",
        "\n",
        "# STEP 4.3. PREPARATION FOR DEPLOYMENT\n",
        "\n",
        "# ...code for deployment preparation...\n",
        "# Assuming the model will be deployed in a specific environment\n",
        "# Include any necessary steps for preparing the model for deployment\n",
        "# This might include serialization, testing the model in a deployment-like environment, etc.\n",
        "\n",
        "# STEP 4.4. DOCUMENTATION AND REPORTING\n",
        "\n",
        "# Prepare a comprehensive report\n",
        "report = f\"\"\"\n",
        "Model Selection Rationale:\n",
        "- The chosen model (e.g., Random Forest) was selected due to its superior performance in terms of accuracy, precision, and recall.\n",
        "\n",
        "Model Performance:\n",
        "- Accuracy: {accuracy}\n",
        "- Precision: {precision}\n",
        "- Recall: {recall}\n",
        "- F1-Score: {f1}\n",
        "\n",
        "Additional Model Analysis:\n",
        "- Detailed error analysis, biases, etc.\n",
        "- Results from cross-validation (if performed).\n",
        "\n",
        "Limitations and Recommendations:\n",
        "- The model may have limitations in terms of scalability or real-time prediction.\n",
        "- Future work could explore more advanced models or feature engineering techniques.\n",
        "\n",
        "Deployment Steps:\n",
        "- The model will be deployed in a cloud-based environment.\n",
        "- Necessary steps for deployment include serialization and environment setup.\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Model documentation and reporting completed.\")\n",
        "\n",
        "# Final Checks and Tests (if applicable)\n",
        "# Include any additional code for final testing or checks before deployment\n",
        "\n",
        "logger.info(\"Final checks and tests completed.\")\n",
        "\n",
        "logger.info(\"Cell 4 tasks completed successfully.\")\n"
      ],
      "metadata": {
        "id": "V28FJ39EHF8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5. CROSS-VALIDATION AND ADDITIONAL METRICS ANALYSIS\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming you are using RandomForestRegressor as your model\n",
        "model = RandomForestRegressor(random_state=random_seed)\n",
        "\n",
        "# Define your scoring metrics\n",
        "# Remove ROC AUC score if your task is regression\n",
        "scoring_metrics = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='macro'),\n",
        "    'recall': make_scorer(recall_score, average='macro'),\n",
        "    'f1': make_scorer(f1_score, average='macro')\n",
        "}\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "k_folds = 10  # Number of folds\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    scores = cross_val_score(model, X_p1, y_p1.fillna(-999), scoring=scorer, cv=k_folds)\n",
        "    cv_results[metric_name] = scores\n",
        "    logger.info(f\"{metric_name} scores for each fold: {scores}\")\n",
        "    logger.info(f\"Average {metric_name} over {k_folds} folds: {scores.mean()}\")\n",
        "\n",
        "# Additional metrics analysis and error/bias exploration\n",
        "# ... Add your code for detailed analysis of errors, biases, etc. ...\n",
        "logger.info(\"Cross-validation and additional metrics analysis completed.\")\n",
        "\n",
        "# Feature Importance Analysis using SHAP\n",
        "# Assuming 'final_prediction_model' is your trained RandomForestRegressor model\n",
        "import shap\n",
        "\n",
        "# Load the trained model (if not already loaded)\n",
        "final_prediction_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(final_prediction_model)\n",
        "shap_values = explainer.shap_values(X_train_p1)\n",
        "\n",
        "# Plot summary plot using SHAP values\n",
        "shap.summary_plot(shap_values, X_train_p1)\n",
        "\n",
        "logger.info(\"Feature importance analysis using SHAP completed.\")"
      ],
      "metadata": {
        "id": "YupgSJuhz3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6. DETAILED ERROR AND BIAS ANALYSIS\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive to access the predictions_df.csv file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in Google Drive where the predictions DataFrame is saved\n",
        "predictions_df_path = '/content/drive/My Drive/predictions_df.csv'\n",
        "\n",
        "# Load the DataFrame from the CSV file if it exists, otherwise create it\n",
        "if os.path.exists(predictions_df_path):\n",
        "    predictions_df = pd.read_csv(predictions_df_path)\n",
        "    logger.info(\"predictions_df loaded from Google Drive successfully.\")\n",
        "else:\n",
        "    # Make sure final_model is loaded\n",
        "    final_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred_final = final_model.predict(X_test_p1)\n",
        "\n",
        "    # Create the predictions DataFrame\n",
        "    predictions_df = pd.DataFrame({'Actual': y_test_p1, 'Predicted': y_pred_final})\n",
        "\n",
        "    # Save the new predictions_df to Google Drive for future use\n",
        "    predictions_df.to_csv(predictions_df_path, index=False)\n",
        "    logger.info(\"predictions_df saved to Google Drive successfully.\")\n",
        "\n",
        "# Merge 'Prev_Week' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Week']], left_index=True, right_index=True)\n",
        "# Merge 'Prev_Entry' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Entry']], left_index=True, right_index=True)\n",
        "\n",
        "# Proceed with error analysis only if predictions_df is loaded or created\n",
        "if 'predictions_df' in locals():\n",
        "    # Analyze error distribution\n",
        "    predictions_df['Error'] = predictions_df['Predicted'] - predictions_df['Actual']\n",
        "    predictions_df['Absolute_Error'] = predictions_df['Error'].abs()\n",
        "\n",
        "    # Plotting error distribution\n",
        "    plt.hist(predictions_df['Error'], bins=30)\n",
        "    plt.title('Error Distribution')\n",
        "    plt.xlabel('Prediction Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Week'\n",
        "prev_week_performance = predictions_df.groupby('Prev_Week').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_week_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Week')\n",
        "plt.xlabel('Previous Week Draw')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Entry'\n",
        "prev_entry_performance = predictions_df.groupby('Prev_Entry').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_entry_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Entry')\n",
        "plt.xlabel('Previous Entry')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Document findings\n",
        "error_bias_report = \"\"\"\n",
        "Detailed Error Analysis:\n",
        "- Error Distribution Insights: {'Describe your findings from the error distribution here'}\n",
        "- Largest Errors: {'Describe characteristics of instances with largest errors here'}\n",
        "\n",
        "Bias Exploration:\n",
        "- Performance by Previous Week: {'Describe performance variations based on the previous week here'}\n",
        "- Performance by Previous Entry: {'Describe performance variations based on the previous entry here'}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Error and bias analysis completed.\")\n",
        "logger.info(error_bias_report)\n"
      ],
      "metadata": {
        "id": "mr21I98826sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7. FINAL REVIEW, DEPLOYMENT PREPARATION, AND DOCUMENTATION\n",
        "\n",
        "# Final Model Review and Refinement\n",
        "# ... Code/comments for any last adjustments to the model ...\n",
        "\n",
        "# Deployment Preparation\n",
        "# Serialize the final model\n",
        "joblib.dump(final_prediction_model, '/content/final_prediction_model_for_deployment.pkl')\n",
        "\n",
        "# Comprehensive Documentation Update\n",
        "# ... Update your comprehensive report with all final findings and methodologies ...\n",
        "\n",
        "# Final Checks and Tests\n",
        "# ... Code/comments for final tests and checks ...\n",
        "\n",
        "# Planning for Future Improvements\n",
        "future_improvement_plan = \"\"\"\n",
        "Future Improvement Plans:\n",
        "- Areas for further research: {describe areas for future research}\n",
        "- Methodologies to explore: {describe potential methodologies for future iterations}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Final review and deployment preparation completed.\")\n",
        "logger.info(future_improvement_plan)\n"
      ],
      "metadata": {
        "id": "qssa3uxO9tru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "u8Y77oV7koS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "WsQJRnWtkpPO"
      }
    }
  ]
}