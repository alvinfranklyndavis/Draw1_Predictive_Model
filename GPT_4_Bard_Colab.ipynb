{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM78RX2Xe814ZXr3vESz3Jj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Project2023_v3/blob/main/GPT_4_Bard_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.1: Package Installation\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas gdown numpy matplotlib scikit-learn xgboost shap\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install black  # Install Black for code formatting\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plnc-ffhAUCk",
        "outputId": "74fc590d-0ebf-4413-b7f2-35f2d0b0a2a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (23.12.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (23.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.5.0)\n",
            "Requirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from black) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4->black) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.7.4->black) (3.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.2: Import Libraries and Set Up Logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import KNNImputer\n",
        "import shap\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "fP_Q74gUBGQi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.3: Load Data from Google Drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive (root directory of Google Drive)\n",
        "drive_dataset_directory = '/content/drive/My Drive/'\n",
        "\n",
        "# Define the path to the CSV file\n",
        "drive_csv_path = os.path.join(drive_dataset_directory, 'initial_data.csv')\n",
        "\n",
        "# Log the start of dataset loading\n",
        "logger.info(\"Reading the dataset from Google Drive...\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(drive_csv_path)\n",
        "logger.info(\"Dataset loaded successfully from Google Drive.\")\n",
        "\n",
        "# Basic Data Exploration\n",
        "logger.info(\"Performing basic data exploration...\")\n",
        "logger.info(f\"Dataset Size: {data.shape}\")\n",
        "logger.info(f\"First 5 Rows:\\n{data.head()}\")\n",
        "logger.info(f\"Missing Values:\\n{data.isnull().sum()}\")\n",
        "\n",
        "# Proceed with further data processing..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNvqJzaKBU7l",
        "outputId": "9cb33cb5-6047-4d83-d3c1-ef28f53d6c7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.1: Date Feature Processing and Prediction1 Setup\n",
        "\n",
        "# Import necessary libraries\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Starting date feature processing for Prediction1\")\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# Convert 'Date' to datetime and extract 'Year', 'Month', and 'Day'\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "data['Day'] = data['Date'].dt.day\n",
        "\n",
        "logger.info(\"Extracted year, month, and day from 'Date' column\")\n",
        "\n",
        "# Display data types and check for missing values\n",
        "logger.info(\"Data types:\\n%s\", data.dtypes)\n",
        "logger.info(\"Missing values before handling:\\n%s\", data.isnull().sum())\n",
        "\n",
        "# Calculate Moving Averages for specified columns\n",
        "window_size = 3\n",
        "columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "# Initialize moving average columns with default values (e.g., 0)\n",
        "for col in target_columns:\n",
        "    data[col] = 0\n",
        "\n",
        "try:\n",
        "    for col, target_col in zip(columns_to_average, target_columns):\n",
        "        data[target_col] = data[col].rolling(window=window_size, min_periods=1).mean()\n",
        "    logger.info(\"Calculated moving averages for specified columns\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Error in moving average calculation: %s\", e)\n",
        "\n",
        "# Adjust entries to use previous day's data\n",
        "data['Prev_Morning'] = data['Morning'].shift(1)\n",
        "data['Prev_Afternoon'] = data['Afternoon'].shift(1)\n",
        "data['Prev_Evening'] = data['Evening'].shift(1)\n",
        "\n",
        "logger.info(\"Created previous day columns\")\n",
        "\n",
        "# Handle NaN values for new columns and exclude specific rows\n",
        "data['Prev_Morning'].fillna(18, inplace=True)\n",
        "data['Prev_Afternoon'].fillna(18, inplace=True)\n",
        "data['Prev_Evening'].fillna(18, inplace=True)\n",
        "data = data.iloc[1:].drop(index=518).reset_index(drop=True)\n",
        "\n",
        "logger.info(\"Handled NaN values and excluded specific rows\")\n",
        "\n",
        "# Keep only relevant columns for Prediction1\n",
        "selected_columns = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "X = data[selected_columns]\n",
        "y = data['Morning']  # Using 'Morning' as the target variable\n",
        "\n",
        "logger.info(\"Saving the Prediction1 data to CSV...\")\n",
        "X.to_csv('/content/processed_date_features_for_prediction1.csv', index=False)\n",
        "y.to_csv('/content/target_variable_for_prediction1.csv', index=False)\n",
        "logger.info(\"Date features processed and saved successfully.\")\n",
        "logger.info(\"First few rows of feature data:\\n%s\", X.head())\n",
        "logger.info(\"First few rows of target data:\\n%s\", y.head())\n"
      ],
      "metadata": {
        "id": "GGRPjzS2ILM3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.2: Additional Data Insights\n",
        "\n",
        "logger.info(\"Exploring additional data insights...\")\n",
        "\n",
        "# Print the number of missing values in each column\n",
        "print(\"Missing values in each column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Print the percentage of missing values in each column\n",
        "print(\"\\nPercentage of missing values in each column:\")\n",
        "print(data.isnull().mean() * 100)\n",
        "\n",
        "# Print summary statistics of the features (X)\n",
        "print(\"\\nSummary statistics of features (X):\")\n",
        "print(X.describe())\n",
        "\n",
        "# Print summary statistics of the target variable (y)\n",
        "print(\"\\nSummary statistics of target variable (y):\")\n",
        "print(y.describe())\n",
        "\n",
        "# Print first few rows of the features (X)\n",
        "print(\"\\nFirst few rows of feature data (X):\")\n",
        "print(X.head())\n",
        "\n",
        "# Print first few rows of the target variable (y)\n",
        "print(\"\\nFirst few rows of target data (y):\")\n",
        "print(y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQoG9bDsPOtj",
        "outputId": "54445712-ef20-41b6-c3de-009debfd2c5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Date                0\n",
            "Day of the Week     0\n",
            "Morning             0\n",
            "Prev_Week           0\n",
            "Rep_Prev_Week       0\n",
            "Prev_Entry          0\n",
            "Rep_Prev_Entry      0\n",
            "Mov_Avg_Mor         0\n",
            "Afternoon           0\n",
            "Prev_Week.1         0\n",
            "Rep_Prev_Week.1     0\n",
            "Prev_Entry.1        0\n",
            "Rep_Prev_Entry.1    0\n",
            "Mov_Avg_Aft         0\n",
            "Evening             0\n",
            "Prev_Week.2         0\n",
            "Rep_Prev_Week.2     0\n",
            "Prev_Entry.2        0\n",
            "Rep_Prev_Entry.2    0\n",
            "Mov_Avg_Eve         0\n",
            "Night               0\n",
            "Prev_Week.3         0\n",
            "Rep_Prev_Week.3     0\n",
            "Prev_Entry.3        0\n",
            "Rep_Prev_Entry.3    0\n",
            "Mov_Avg_Nig         0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        0\n",
            "Prev_Afternoon      0\n",
            "Prev_Evening        0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Date                0.0\n",
            "Day of the Week     0.0\n",
            "Morning             0.0\n",
            "Prev_Week           0.0\n",
            "Rep_Prev_Week       0.0\n",
            "Prev_Entry          0.0\n",
            "Rep_Prev_Entry      0.0\n",
            "Mov_Avg_Mor         0.0\n",
            "Afternoon           0.0\n",
            "Prev_Week.1         0.0\n",
            "Rep_Prev_Week.1     0.0\n",
            "Prev_Entry.1        0.0\n",
            "Rep_Prev_Entry.1    0.0\n",
            "Mov_Avg_Aft         0.0\n",
            "Evening             0.0\n",
            "Prev_Week.2         0.0\n",
            "Rep_Prev_Week.2     0.0\n",
            "Prev_Entry.2        0.0\n",
            "Rep_Prev_Entry.2    0.0\n",
            "Mov_Avg_Eve         0.0\n",
            "Night               0.0\n",
            "Prev_Week.3         0.0\n",
            "Rep_Prev_Week.3     0.0\n",
            "Prev_Entry.3        0.0\n",
            "Rep_Prev_Entry.3    0.0\n",
            "Mov_Avg_Nig         0.0\n",
            "Year                0.0\n",
            "Month               0.0\n",
            "Day                 0.0\n",
            "Prev_Morning        0.0\n",
            "Prev_Afternoon      0.0\n",
            "Prev_Evening        0.0\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics of features (X):\n",
            "              Year        Month          Day    Prev_Week   Prev_Entry  \\\n",
            "count  1407.000000  1407.000000  1407.000000  1407.000000  1407.000000   \n",
            "mean   2020.577825     6.629709    15.626866    18.212509    18.130064   \n",
            "std       1.556254     3.565689     8.752854    10.568730    10.368159   \n",
            "min    2018.000000     1.000000     1.000000     0.000000     1.000000   \n",
            "25%    2019.000000     3.000000     8.000000     9.000000     9.000000   \n",
            "50%    2021.000000     7.000000    16.000000    18.000000    18.000000   \n",
            "75%    2022.000000    10.000000    23.000000    27.000000    27.000000   \n",
            "max    2023.000000    12.000000    31.000000    36.000000    36.000000   \n",
            "\n",
            "       Mov_Avg_Mor  Prev_Morning  Prev_Afternoon  Prev_Evening  \n",
            "count  1407.000000   1407.000000     1407.000000   1407.000000  \n",
            "mean     18.766169     18.770434       18.623312     18.528785  \n",
            "std       5.745406     10.277406       10.360101     10.240097  \n",
            "min       4.000000      1.000000        1.000000      1.000000  \n",
            "25%      14.666667     10.000000        9.000000     10.000000  \n",
            "50%      18.666667     19.000000       19.000000     19.000000  \n",
            "75%      22.666667     28.000000       27.000000     27.000000  \n",
            "max      35.333333     36.000000       36.000000     36.000000  \n",
            "\n",
            "Summary statistics of target variable (y):\n",
            "count    1407.00000\n",
            "mean       18.77683\n",
            "std        10.27599\n",
            "min         1.00000\n",
            "25%        10.00000\n",
            "50%        19.00000\n",
            "75%        28.00000\n",
            "max        36.00000\n",
            "Name: Morning, dtype: float64\n",
            "\n",
            "First few rows of feature data (X):\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    2         11           9    25.000000          19.0   \n",
            "1  2018      8    3         19          12    21.666667          31.0   \n",
            "2  2018      8    4         35          35    25.666667          15.0   \n",
            "3  2018      8    6         18          16    25.666667          31.0   \n",
            "4  2018      8    7         13          18    27.666667          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            14.0          33.0  \n",
            "1             3.0          35.0  \n",
            "2             9.0          23.0  \n",
            "3            21.0          29.0  \n",
            "4            31.0          15.0  \n",
            "\n",
            "First few rows of target data (y):\n",
            "0    31\n",
            "1    15\n",
            "2    31\n",
            "3    31\n",
            "4    21\n",
            "Name: Morning, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.3: Setting Bounds for Numerical Range\n",
        "\n",
        "logger.info(\"Enforcing numerical bounds...\")\n",
        "\n",
        "# Define the columns that should have values in the range of 1 to 36\n",
        "columns_to_check = ['Morning', 'Afternoon', 'Evening', 'Night', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "\n",
        "# Loop through these columns and enforce the range\n",
        "for col in columns_to_check:\n",
        "    # Find values outside the range\n",
        "    outliers = data[(data[col] < 1) | (data[col] > 36)]\n",
        "\n",
        "    # Report if any outliers are found\n",
        "    if not outliers.empty:\n",
        "        print(f\"Outliers found in {col}:\")\n",
        "        print(outliers)\n",
        "\n",
        "    # Enforce the range by clipping values\n",
        "    data[col] = data[col].clip(lower=1, upper=36)\n",
        "\n",
        "# Ensure changes are reflected\n",
        "print(data[columns_to_check].describe())\n",
        "\n",
        "# Prepare the current data with NaNs in 'Prediction1' for testing\n",
        "current_data = data[selected_columns].copy()  # Use .copy() to create an independent copy\n",
        "current_data['Prediction1'] = np.nan  # Initialize 'Prediction1' with NaN\n",
        "\n",
        "# Save 'current_data' as a CSV file for loading in Step 3.1\n",
        "current_data.to_csv('/content/current_data_for_prediction1.csv', index=False)\n",
        "logger.info(\"Current data with 'Prediction1' as NaN saved as 'current_data_for_prediction1.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi7kZT26PrzA",
        "outputId": "1c765aeb-6be7-4ae9-e23c-7ad6f6ab715a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Morning    Afternoon      Evening        Night  Prev_Morning  \\\n",
            "count  1409.000000  1409.000000  1409.000000  1409.000000   1408.000000   \n",
            "mean     18.766501    18.613911    18.527324    18.109297     18.762074   \n",
            "std      10.276234    10.356362    10.240544    10.375588     10.278541   \n",
            "min       1.000000     1.000000     1.000000     1.000000      1.000000   \n",
            "25%      10.000000     9.000000    10.000000     9.000000     10.000000   \n",
            "50%      19.000000    19.000000    19.000000    18.000000     19.000000   \n",
            "75%      28.000000    27.000000    27.000000    27.000000     28.000000   \n",
            "max      36.000000    36.000000    36.000000    36.000000     36.000000   \n",
            "\n",
            "       Prev_Afternoon  Prev_Evening  \n",
            "count     1408.000000   1408.000000  \n",
            "mean        18.620739     18.535511  \n",
            "std         10.356869     10.239569  \n",
            "min          1.000000      1.000000  \n",
            "25%          9.000000     10.000000  \n",
            "50%         19.000000     19.000000  \n",
            "75%         27.000000     27.000000  \n",
            "max         36.000000     36.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.1: Data Preparation for Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading historical dataset for data preparation...\")\n",
        "\n",
        "# Load the historical dataset\n",
        "historical_data = pd.read_csv('/content/processed_date_features_for_prediction1.csv')\n",
        "y_hist = pd.read_csv('/content/target_variable_for_prediction1.csv')\n",
        "logger.info(\"Historical dataset loaded successfully.\")\n",
        "\n",
        "# Split historical data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(historical_data, y_hist, test_size=0.20, random_state=42)\n",
        "logger.info(f\"Data split into training and validation sets. Training set size: {X_train.shape}, Validation set size: {X_val.shape}\")\n",
        "\n",
        "# Save the split datasets to CSV files\n",
        "X_train.to_csv('/content/inspected_X_train.csv', index=False)\n",
        "y_train.to_csv('/content/y_train_split_for_training.csv', index=False)\n",
        "X_val.to_csv('/content/inspected_X_val.csv', index=False)\n",
        "y_val.to_csv('/content/inspected_y_val.csv', index=False)\n",
        "logger.info(\"Datasets saved for manual inspection and model training.\")\n",
        "\n",
        "# Save the cleaned X_train and X_val for inspection\n",
        "X_train.to_csv('/content/inspected_X_train.csv', index=False)\n",
        "X_val.to_csv('/content/inspected_X_val.csv', index=False)\n",
        "logger.info(\"X_train and X_val saved for manual inspection.\")\n",
        "\n",
        "# Save the cleaned y_val for evaluation\n",
        "y_val.to_csv('/content/inspected_y_val.csv', index=False)\n",
        "logger.info(\"y_val saved for evaluation.\")\n",
        "\n",
        "# Load the current dataset with NaNs in 'Prediction1'\n",
        "current_data = pd.read_csv('/content/current_data_for_prediction1.csv')\n",
        "X_current = current_data.drop('Prediction1', axis=1)\n",
        "\n",
        "# Drop the first row to eliminate remaining NaNs\n",
        "X_current.drop(index=0, inplace=True)\n",
        "\n",
        "# Save the prepared current_data\n",
        "current_data.to_csv('/content/prepared_current_data_for_prediction.csv', index=False)\n",
        "logger.info(\"Prepared current data saved.\")\n",
        "\n",
        "# Check for remaining NaNs in X_train and X_current\n",
        "logger.info(\"Checking for NaNs in X_train and X_current...\")\n",
        "logger.info(f\"NaN values in X_train: {X_train.isnull().sum()}\")\n",
        "logger.info(f\"NaN values in X_current: {X_current.isnull().sum()}\")\n",
        "\n",
        "# Print command to recheck for NaNs\n",
        "print(\"NaN check after Cell 3.1 execution:\")\n",
        "print(\"NaN values in X_train:\\n\", X_train.isnull().sum())\n",
        "print(\"NaN values in X_current:\\n\", X_current.isnull().sum())\n"
      ],
      "metadata": {
        "id": "VshK68czukkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca185720-e735-42a5-9be1-30f3bc46a059"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN check after Cell 3.1 execution:\n",
            "NaN values in X_train:\n",
            " Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n",
            "NaN values in X_current:\n",
            " Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First few rows of X_current:\")\n",
        "print(X_current.head())\n",
        "print(\"Rows 1 and 518 in X_current:\")\n",
        "print(X_current.iloc[[0, 517]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydlG5zchqHHG",
        "outputId": "97204b70-62b0-428e-f724-b22bb8e4b7a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of X_current:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "1  2018      8    2         11           9    25.000000          19.0   \n",
            "2  2018      8    3         19          12    21.666667          31.0   \n",
            "3  2018      8    4         35          35    25.666667          15.0   \n",
            "4  2018      8    6         18          16    25.666667          31.0   \n",
            "5  2018      8    7         13          18    27.666667          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "1            14.0          33.0  \n",
            "2             3.0          35.0  \n",
            "3             9.0          23.0  \n",
            "4            21.0          29.0  \n",
            "5            31.0          15.0  \n",
            "Rows 1 and 518 in X_current:\n",
            "     Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "1    2018      8    2         11           9    25.000000          19.0   \n",
            "518  2020      6    6          0           2    13.666667           7.0   \n",
            "\n",
            "     Prev_Afternoon  Prev_Evening  \n",
            "1              14.0          33.0  \n",
            "518            15.0          28.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.2: Model Training and Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading training and validation data for model training...\")\n",
        "\n",
        "# Load the training data\n",
        "X_train = pd.read_csv('/content/inspected_X_train.csv')\n",
        "y_train = pd.read_csv('/content/y_train_split_for_training.csv')\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "random_forest_model = RandomForestRegressor(random_state=42)\n",
        "random_forest_model.fit(X_train, y_train.values.ravel())\n",
        "logger.info(\"Random Forest model trained on historical data.\")\n",
        "\n",
        "# Load the validation set for model evaluation\n",
        "X_val = pd.read_csv('/content/inspected_X_val.csv')\n",
        "y_val = pd.read_csv('/content/inspected_y_val.csv')\n",
        "\n",
        "# Evaluation on the validation set\n",
        "y_val_pred = random_forest_model.predict(X_val)\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "r2_score_val = r2_score(y_val, y_val_pred)\n",
        "logger.info(f\"Validation MSE: {mse_val}, R2 Score: {r2_score_val}\")\n",
        "\n",
        "# Save the trained model\n",
        "model_path = '/content/random_forest_prediction_model.pkl'\n",
        "joblib.dump(random_forest_model, model_path)\n",
        "logger.info(\"Trained RandomForest model saved.\")\n",
        "\n",
        "# Load the prepared current_data for prediction\n",
        "current_data = pd.read_csv('/content/prepared_current_data_for_prediction.csv')\n",
        "X_current_for_prediction = current_data.drop('Prediction1', axis=1)\n",
        "\n",
        "# Forward fill NaNs in the first row\n",
        "X_current_for_prediction.iloc[0] = X_current_for_prediction.iloc[0].fillna(method='ffill')\n",
        "\n",
        "print(\"NaN values in X_current_for_prediction:\\n\", X_current_for_prediction.isnull().sum())\n",
        "print(\"First few rows of X_current_for_prediction:\\n\", X_current_for_prediction.head())\n",
        "\n",
        "# Generate predictions for the current data\n",
        "y_pred_current = random_forest_model.predict(X_current_for_prediction)\n",
        "current_data['Prediction1'] = y_pred_current\n",
        "\n",
        "# Save the current data with predictions\n",
        "current_data.to_csv('/content/current_data_with_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMiB868RwRYx",
        "outputId": "d8581da7-9b85-4076-d5a5-2ddc50609df9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in X_current_for_prediction:\n",
            " Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n",
            "First few rows of X_current_for_prediction:\n",
            "    Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    1          7          23    19.000000          19.0   \n",
            "1  2018      8    2         11           9    25.000000          19.0   \n",
            "2  2018      8    3         19          12    21.666667          31.0   \n",
            "3  2018      8    4         35          35    25.666667          15.0   \n",
            "4  2018      8    6         18          16    25.666667          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            19.0          19.0  \n",
            "1            14.0          33.0  \n",
            "2             3.0          35.0  \n",
            "3             9.0          23.0  \n",
            "4            21.0          29.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.3: Enhanced NaN Check and Handling in 'Prediction1'\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading current data with predictions for enhanced NaN handling...\")\n",
        "\n",
        "# Load the current data with predictions\n",
        "current_data_with_predictions = pd.read_csv('/content/current_data_with_predictions.csv')\n",
        "\n",
        "# Forward fill NaNs in the first row\n",
        "current_data_with_predictions.iloc[0] = current_data_with_predictions.iloc[0].fillna(method='ffill')\n",
        "\n",
        "# Print the first few rows to verify NaN handling\n",
        "print(\"First few rows of current data after NaN handling:\\n\", current_data_with_predictions.head())\n",
        "\n",
        "# Check for NaN values in 'Prediction1'\n",
        "nan_count_prediction1 = current_data_with_predictions['Prediction1'].isnull().sum()\n",
        "logger.info(f\"Number of NaN values in 'Prediction1': {nan_count_prediction1}\")\n",
        "\n",
        "if nan_count_prediction1 > 0:\n",
        "    logger.warning(\"NaNs detected in 'Prediction1'. Here are the details:\")\n",
        "    nan_rows = current_data_with_predictions[current_data_with_predictions['Prediction1'].isnull()]\n",
        "    print(\"Rows with NaN in 'Prediction1':\\n\", nan_rows)\n",
        "else:\n",
        "    print(\"No NaN values found in 'Prediction1'.\")\n",
        "\n",
        "# Additional analysis or handling of NaNs can be added here if needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm8OBUPe0MN6",
        "outputId": "4dbf3f87-78a0-499e-f612-1ff801a7d2f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of current data after NaN handling:\n",
            "    Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    1          7          23    19.000000          19.0   \n",
            "1  2018      8    2         11           9    25.000000          19.0   \n",
            "2  2018      8    3         19          12    21.666667          31.0   \n",
            "3  2018      8    4         35          35    25.666667          15.0   \n",
            "4  2018      8    6         18          16    25.666667          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  Prediction1  \n",
            "0            19.0          19.0        18.19  \n",
            "1            14.0          33.0        30.34  \n",
            "2             3.0          35.0        15.94  \n",
            "3             9.0          23.0        30.40  \n",
            "4            21.0          29.0        28.07  \n",
            "No NaN values found in 'Prediction1'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CNfE2mY1nB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5pt66epbDL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3UAPiSVEzIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZCCtfy7E0nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDssvACI2Aul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4.1. MODEL INTERPRETATION\n",
        "\n",
        "# Check for NaN values in y_test_p1\n",
        "nan_count = y_test_p1.isnull().sum()\n",
        "logger.info(f\"Number of NaN values in y_test_p1: {nan_count}\")\n",
        "\n",
        "# If NaN values exist, print some examples\n",
        "if nan_count > 0:\n",
        "    logger.info(\"Examples of NaN values in y_test_p1:\")\n",
        "    logger.info(y_test_p1[y_test_p1.isnull()])\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the finalized model\n",
        "final_model = joblib.load('/content/random_forest_prediction_model_imputed.pkl')\n",
        "\n",
        "# Apply the same imputation and feature modification to X_test_p1\n",
        "imputed_test = imputer.transform(X_test_p1)\n",
        "X_test_p1_imputed, missing_indicator_test = imputed_test[:, :-1], imputed_test[:, -1]\n",
        "X_test_p1_imputed['Target_Missing'] = missing_indicator_test\n",
        "\n",
        "# Using SHAP to interpret the model\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_test_p1_imputed)\n",
        "shap.summary_plot(shap_values, X_test_p1_imputed, plot_type=\"bar\")\n",
        "\n",
        "# STEP 4.2. FINAL MODEL SELECTION AND REPORTING\n",
        "\n",
        "# Report the chosen model's evaluation metrics\n",
        "y_pred_final = final_model.predict(X_test_p1_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_p1, y_pred_final)\n",
        "precision = precision_score(y_test_p1, y_pred_final, average='macro')\n",
        "recall = recall_score(y_test_p1, y_pred_final, average='macro')\n",
        "f1 = f1_score(y_test_p1, y_pred_final, average='macro')\n",
        "\n",
        "logger.info(f\"Model Performance Metrics:\\n Accuracy: {accuracy}\\n Precision: {precision}\\n Recall: {recall}\\n F1 Score: {f1}\")\n",
        "\n",
        "# Add cross-validation implementation here if applicable\n",
        "# ...code for cross-validation...\n",
        "\n",
        "# Analysis of model performance\n",
        "# ...code/logic for detailed analysis of errors, biases, etc...\n",
        "\n",
        "# STEP 4.3. PREPARATION FOR DEPLOYMENT\n",
        "\n",
        "# ...code for deployment preparation...\n",
        "# Assuming the model will be deployed in a specific environment\n",
        "# Include any necessary steps for preparing the model for deployment\n",
        "# This might include serialization, testing the model in a deployment-like environment, etc.\n",
        "\n",
        "# STEP 4.4. DOCUMENTATION AND REPORTING\n",
        "\n",
        "# Prepare a comprehensive report\n",
        "report = f\"\"\"\n",
        "Model Selection Rationale:\n",
        "- The chosen model (e.g., Random Forest) was selected due to its superior performance in terms of accuracy, precision, and recall.\n",
        "\n",
        "Model Performance:\n",
        "- Accuracy: {accuracy}\n",
        "- Precision: {precision}\n",
        "- Recall: {recall}\n",
        "- F1-Score: {f1}\n",
        "\n",
        "Additional Model Analysis:\n",
        "- Detailed error analysis, biases, etc.\n",
        "- Results from cross-validation (if performed).\n",
        "\n",
        "Limitations and Recommendations:\n",
        "- The model may have limitations in terms of scalability or real-time prediction.\n",
        "- Future work could explore more advanced models or feature engineering techniques.\n",
        "\n",
        "Deployment Steps:\n",
        "- The model will be deployed in a cloud-based environment.\n",
        "- Necessary steps for deployment include serialization and environment setup.\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Model documentation and reporting completed.\")\n",
        "\n",
        "# Final Checks and Tests (if applicable)\n",
        "# Include any additional code for final testing or checks before deployment\n",
        "\n",
        "logger.info(\"Final checks and tests completed.\")\n",
        "\n",
        "logger.info(\"Cell 4 tasks completed successfully.\")\n"
      ],
      "metadata": {
        "id": "V28FJ39EHF8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5. CROSS-VALIDATION AND ADDITIONAL METRICS ANALYSIS\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming you are using RandomForestRegressor as your model\n",
        "model = RandomForestRegressor(random_state=random_seed)\n",
        "\n",
        "# Define your scoring metrics\n",
        "# Remove ROC AUC score if your task is regression\n",
        "scoring_metrics = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='macro'),\n",
        "    'recall': make_scorer(recall_score, average='macro'),\n",
        "    'f1': make_scorer(f1_score, average='macro')\n",
        "}\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "k_folds = 10  # Number of folds\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    scores = cross_val_score(model, X_p1, y_p1.fillna(-999), scoring=scorer, cv=k_folds)\n",
        "    cv_results[metric_name] = scores\n",
        "    logger.info(f\"{metric_name} scores for each fold: {scores}\")\n",
        "    logger.info(f\"Average {metric_name} over {k_folds} folds: {scores.mean()}\")\n",
        "\n",
        "# Additional metrics analysis and error/bias exploration\n",
        "# ... Add your code for detailed analysis of errors, biases, etc. ...\n",
        "logger.info(\"Cross-validation and additional metrics analysis completed.\")\n",
        "\n",
        "# Feature Importance Analysis using SHAP\n",
        "# Assuming 'final_prediction_model' is your trained RandomForestRegressor model\n",
        "import shap\n",
        "\n",
        "# Load the trained model (if not already loaded)\n",
        "final_prediction_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(final_prediction_model)\n",
        "shap_values = explainer.shap_values(X_train_p1)\n",
        "\n",
        "# Plot summary plot using SHAP values\n",
        "shap.summary_plot(shap_values, X_train_p1)\n",
        "\n",
        "logger.info(\"Feature importance analysis using SHAP completed.\")"
      ],
      "metadata": {
        "id": "YupgSJuhz3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6. DETAILED ERROR AND BIAS ANALYSIS\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive to access the predictions_df.csv file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in Google Drive where the predictions DataFrame is saved\n",
        "predictions_df_path = '/content/drive/My Drive/predictions_df.csv'\n",
        "\n",
        "# Load the DataFrame from the CSV file if it exists, otherwise create it\n",
        "if os.path.exists(predictions_df_path):\n",
        "    predictions_df = pd.read_csv(predictions_df_path)\n",
        "    logger.info(\"predictions_df loaded from Google Drive successfully.\")\n",
        "else:\n",
        "    # Make sure final_model is loaded\n",
        "    final_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred_final = final_model.predict(X_test_p1)\n",
        "\n",
        "    # Create the predictions DataFrame\n",
        "    predictions_df = pd.DataFrame({'Actual': y_test_p1, 'Predicted': y_pred_final})\n",
        "\n",
        "    # Save the new predictions_df to Google Drive for future use\n",
        "    predictions_df.to_csv(predictions_df_path, index=False)\n",
        "    logger.info(\"predictions_df saved to Google Drive successfully.\")\n",
        "\n",
        "# Merge 'Prev_Week' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Week']], left_index=True, right_index=True)\n",
        "# Merge 'Prev_Entry' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Entry']], left_index=True, right_index=True)\n",
        "\n",
        "# Proceed with error analysis only if predictions_df is loaded or created\n",
        "if 'predictions_df' in locals():\n",
        "    # Analyze error distribution\n",
        "    predictions_df['Error'] = predictions_df['Predicted'] - predictions_df['Actual']\n",
        "    predictions_df['Absolute_Error'] = predictions_df['Error'].abs()\n",
        "\n",
        "    # Plotting error distribution\n",
        "    plt.hist(predictions_df['Error'], bins=30)\n",
        "    plt.title('Error Distribution')\n",
        "    plt.xlabel('Prediction Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Week'\n",
        "prev_week_performance = predictions_df.groupby('Prev_Week').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_week_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Week')\n",
        "plt.xlabel('Previous Week Draw')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Entry'\n",
        "prev_entry_performance = predictions_df.groupby('Prev_Entry').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_entry_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Entry')\n",
        "plt.xlabel('Previous Entry')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Document findings\n",
        "error_bias_report = \"\"\"\n",
        "Detailed Error Analysis:\n",
        "- Error Distribution Insights: {'Describe your findings from the error distribution here'}\n",
        "- Largest Errors: {'Describe characteristics of instances with largest errors here'}\n",
        "\n",
        "Bias Exploration:\n",
        "- Performance by Previous Week: {'Describe performance variations based on the previous week here'}\n",
        "- Performance by Previous Entry: {'Describe performance variations based on the previous entry here'}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Error and bias analysis completed.\")\n",
        "logger.info(error_bias_report)\n"
      ],
      "metadata": {
        "id": "mr21I98826sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7. FINAL REVIEW, DEPLOYMENT PREPARATION, AND DOCUMENTATION\n",
        "\n",
        "# Final Model Review and Refinement\n",
        "# ... Code/comments for any last adjustments to the model ...\n",
        "\n",
        "# Deployment Preparation\n",
        "# Serialize the final model\n",
        "joblib.dump(final_prediction_model, '/content/final_prediction_model_for_deployment.pkl')\n",
        "\n",
        "# Comprehensive Documentation Update\n",
        "# ... Update your comprehensive report with all final findings and methodologies ...\n",
        "\n",
        "# Final Checks and Tests\n",
        "# ... Code/comments for final tests and checks ...\n",
        "\n",
        "# Planning for Future Improvements\n",
        "future_improvement_plan = \"\"\"\n",
        "Future Improvement Plans:\n",
        "- Areas for further research: {describe areas for future research}\n",
        "- Methodologies to explore: {describe potential methodologies for future iterations}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Final review and deployment preparation completed.\")\n",
        "logger.info(future_improvement_plan)\n"
      ],
      "metadata": {
        "id": "qssa3uxO9tru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "u8Y77oV7koS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "WsQJRnWtkpPO"
      }
    }
  ]
}