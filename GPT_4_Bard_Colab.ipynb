{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGdFiByQULMldjpp1mgQ1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Project2023_v3/blob/main/GPT_4_Bard_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r_a3x9wp-p1",
        "outputId": "ffb0aa1a-b4c3-4601-97bc-ba64b7eeefee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# STEP 1. INSTALL PACKAGES AND IMPORT DATA\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas gdown numpy matplotlib scikit-learn xgboost shap\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import KNNImputer\n",
        "import shap\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive (root directory of Google Drive)\n",
        "drive_dataset_directory = '/content/drive/My Drive/'\n",
        "\n",
        "# Define the local file path in Google Drive for the CSV\n",
        "drive_csv_path = os.path.join(drive_dataset_directory, 'Training_Testing_Hybrid_MA.csv')\n",
        "\n",
        "# Log the start of the dataset loading\n",
        "logger.info(\"Reading the dataset from Google Drive...\")\n",
        "\n",
        "# Read the dataset from Google Drive\n",
        "data = pd.read_csv(drive_csv_path)\n",
        "logger.info(\"Dataset loaded successfully from Google Drive.\")\n",
        "\n",
        "# Basic Data Exploration\n",
        "logger.info(\"Performing basic data exploration...\")\n",
        "logger.info(f\"Dataset Size: {data.shape}\")\n",
        "logger.info(f\"First 5 Rows:\\n{data.head()}\")\n",
        "logger.info(f\"Missing Values:\\n{data.isnull().sum()}\")\n",
        "\n",
        "# Proceed with further data processing...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2.1 PROCESS DATE FEATURES AND SET UP PREDICTION1 COLUMN\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Processing date features...\")\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# Convert 'Date' to datetime and extract 'Year', 'Month', and 'Day'\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "data['Day'] = data['Date'].dt.day\n",
        "\n",
        "# Display data types and check for missing values\n",
        "logger.info(\"Data types:\\n%s\", data.dtypes)\n",
        "logger.info(\"Missing values:\\n%s\", data.isnull().sum())\n",
        "\n",
        "# Calculate Moving Averages for specified columns\n",
        "window_size = 3\n",
        "columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "# Initialize moving average columns with default values (e.g., 0)\n",
        "for col in target_columns:\n",
        "    data[col] = 0\n",
        "\n",
        "try:\n",
        "    for col, target_col in zip(columns_to_average, target_columns):\n",
        "        data[target_col] = data[col].rolling(window=window_size, min_periods=1).mean()\n",
        "except Exception as e:\n",
        "    logger.error(\"Error in moving average calculation: %s\", e)\n",
        "\n",
        "# Adjust entries to use previous day's data\n",
        "data['Prev_Morning'] = data['Morning'].shift(1)\n",
        "data['Prev_Afternoon'] = data['Afternoon'].shift(1)\n",
        "data['Prev_Evening'] = data['Evening'].shift(1)\n",
        "\n",
        "# Keep only relevant columns for Prediction1\n",
        "selected_columns = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "X = data[selected_columns]\n",
        "y = data['Morning']  # Using 'Morning' as the target variable\n",
        "\n",
        "logger.info(\"Saving the Prediction1 data to CSV...\")\n",
        "X.to_csv('/content/train_data_features.csv', index=False)\n",
        "y.to_csv('/content/train_data_target.csv', index=False)\n",
        "logger.info(\"Date features processed and saved successfully.\")\n",
        "logger.info(\"First few rows of feature data:\\n%s\", X.head())\n",
        "logger.info(\"First few rows of target data:\\n%s\", y.head())\n"
      ],
      "metadata": {
        "id": "VshK68czukkS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2.2 - ADDITIONAL DATA INSIGHTS\n",
        "\n",
        "# Print the number of missing values in each column\n",
        "print(\"Missing values in each column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Print the percentage of missing values in each column\n",
        "print(\"\\nPercentage of missing values in each column:\")\n",
        "print(data.isnull().mean() * 100)\n",
        "\n",
        "# Print summary statistics of the features (X)\n",
        "print(\"\\nSummary statistics of features (X):\")\n",
        "print(X.describe())\n",
        "\n",
        "# Print summary statistics of the target variable (y)\n",
        "print(\"\\nSummary statistics of target variable (y):\")\n",
        "print(y.describe())\n",
        "\n",
        "# Print first few rows of the features (X)\n",
        "print(\"\\nFirst few rows of feature data (X):\")\n",
        "print(X.head())\n",
        "\n",
        "# Print first few rows of the target variable (y)\n",
        "print(\"\\nFirst few rows of target data (y):\")\n",
        "print(y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMiB868RwRYx",
        "outputId": "4a9db48f-c27a-43c6-f603-6e39f48be859"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Date                0\n",
            "Day of the Week     0\n",
            "Morning             0\n",
            "Prev_Week           0\n",
            "Rep_Prev_Week       0\n",
            "Prev_Entry          0\n",
            "Rep_Prev_Entry      0\n",
            "Mov_Avg_Mor         0\n",
            "Afternoon           0\n",
            "Prev_Week.1         0\n",
            "Rep_Prev_Week.1     0\n",
            "Prev_Entry.1        0\n",
            "Rep_Prev_Entry.1    0\n",
            "Mov_Avg_Aft         0\n",
            "Evening             0\n",
            "Prev_Week.2         0\n",
            "Rep_Prev_Week.2     0\n",
            "Prev_Entry.2        0\n",
            "Rep_Prev_Entry.2    0\n",
            "Mov_Avg_Eve         0\n",
            "Night               0\n",
            "Prev_Week.3         0\n",
            "Rep_Prev_Week.3     0\n",
            "Prev_Entry.3        0\n",
            "Rep_Prev_Entry.3    0\n",
            "Mov_Avg_Nig         0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        1\n",
            "Prev_Afternoon      1\n",
            "Prev_Evening        1\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Date                0.000000\n",
            "Day of the Week     0.000000\n",
            "Morning             0.000000\n",
            "Prev_Week           0.000000\n",
            "Rep_Prev_Week       0.000000\n",
            "Prev_Entry          0.000000\n",
            "Rep_Prev_Entry      0.000000\n",
            "Mov_Avg_Mor         0.000000\n",
            "Afternoon           0.000000\n",
            "Prev_Week.1         0.000000\n",
            "Rep_Prev_Week.1     0.000000\n",
            "Prev_Entry.1        0.000000\n",
            "Rep_Prev_Entry.1    0.000000\n",
            "Mov_Avg_Aft         0.000000\n",
            "Evening             0.000000\n",
            "Prev_Week.2         0.000000\n",
            "Rep_Prev_Week.2     0.000000\n",
            "Prev_Entry.2        0.000000\n",
            "Rep_Prev_Entry.2    0.000000\n",
            "Mov_Avg_Eve         0.000000\n",
            "Night               0.000000\n",
            "Prev_Week.3         0.000000\n",
            "Rep_Prev_Week.3     0.000000\n",
            "Prev_Entry.3        0.000000\n",
            "Rep_Prev_Entry.3    0.000000\n",
            "Mov_Avg_Nig         0.000000\n",
            "Year                0.000000\n",
            "Month               0.000000\n",
            "Day                 0.000000\n",
            "Prev_Morning        0.070972\n",
            "Prev_Afternoon      0.070972\n",
            "Prev_Evening        0.070972\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics of features (X):\n",
            "              Year        Month          Day    Prev_Week   Prev_Entry  \\\n",
            "count  1409.000000  1409.000000  1409.000000  1409.000000  1409.000000   \n",
            "mean   2020.575586     6.630234    15.609652    18.191625    18.122072   \n",
            "std       1.556739     3.563382     8.759058    10.576569    10.370515   \n",
            "min    2018.000000     1.000000     1.000000     0.000000     1.000000   \n",
            "25%    2019.000000     3.000000     8.000000     9.000000     9.000000   \n",
            "50%    2021.000000     7.000000    16.000000    18.000000    18.000000   \n",
            "75%    2022.000000    10.000000    23.000000    27.000000    27.000000   \n",
            "max    2023.000000    12.000000    31.000000    36.000000    36.000000   \n",
            "\n",
            "       Mov_Avg_Mor  Prev_Morning  Prev_Afternoon  Prev_Evening  \n",
            "count  1409.000000   1408.000000     1408.000000   1408.000000  \n",
            "mean     18.762716     18.762074       18.620739     18.535511  \n",
            "std       5.742935     10.278541       10.356869     10.239569  \n",
            "min       4.000000      1.000000        1.000000      1.000000  \n",
            "25%      14.666667     10.000000        9.000000     10.000000  \n",
            "50%      18.666667     19.000000       19.000000     19.000000  \n",
            "75%      22.666667     28.000000       27.000000     27.000000  \n",
            "max      35.333333     36.000000       36.000000     36.000000  \n",
            "\n",
            "Summary statistics of target variable (y):\n",
            "count    1409.000000\n",
            "mean       18.766501\n",
            "std        10.276234\n",
            "min         1.000000\n",
            "25%        10.000000\n",
            "50%        19.000000\n",
            "75%        28.000000\n",
            "max        36.000000\n",
            "Name: Morning, dtype: float64\n",
            "\n",
            "First few rows of feature data (X):\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    1          7          23    19.000000           NaN   \n",
            "1  2018      8    2         11           9    25.000000          19.0   \n",
            "2  2018      8    3         19          12    21.666667          31.0   \n",
            "3  2018      8    4         35          35    25.666667          15.0   \n",
            "4  2018      8    6         18          16    25.666667          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0             NaN           NaN  \n",
            "1            14.0          33.0  \n",
            "2             3.0          35.0  \n",
            "3             9.0          23.0  \n",
            "4            21.0          29.0  \n",
            "\n",
            "First few rows of target data (y):\n",
            "0    19\n",
            "1    31\n",
            "2    15\n",
            "3    31\n",
            "4    31\n",
            "Name: Morning, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2.3 - SETTING BOUNDS FOR NUMERICAL RANGE\n",
        "\n",
        "# Define the columns that should have values in the range of 1 to 36\n",
        "columns_to_check = ['Morning', 'Afternoon', 'Evening', 'Night', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "\n",
        "# Loop through these columns and enforce the range\n",
        "for col in columns_to_check:\n",
        "    # Find values outside the range\n",
        "    outliers = data[(data[col] < 1) | (data[col] > 36)]\n",
        "\n",
        "    # Report if any outliers are found\n",
        "    if not outliers.empty:\n",
        "        print(f\"Outliers found in {col}:\")\n",
        "        print(outliers)\n",
        "\n",
        "    # Enforce the range by clipping values\n",
        "    data[col] = data[col].clip(lower=1, upper=36)\n",
        "\n",
        "# Ensure changes are reflected\n",
        "print(data[columns_to_check].describe())\n",
        "\n",
        "# Prepare the current data with NaNs in 'Prediction1' for testing\n",
        "current_data = data[selected_columns].copy()  # Use .copy() to create an independent copy\n",
        "current_data['Prediction1'] = np.nan  # Initialize 'Prediction1' with NaN\n",
        "\n",
        "# Save 'current_data' as a CSV file for loading in Step 3.1\n",
        "current_data.to_csv('/content/current_data.csv', index=False)\n",
        "logger.info(\"Current data with 'Prediction1' as NaN saved as 'current_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm8OBUPe0MN6",
        "outputId": "4b7c5242-6e73-40a1-dbd9-41f6d97d3d84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Morning    Afternoon      Evening        Night  Prev_Morning  \\\n",
            "count  1409.000000  1409.000000  1409.000000  1409.000000   1408.000000   \n",
            "mean     18.766501    18.613911    18.527324    18.109297     18.762074   \n",
            "std      10.276234    10.356362    10.240544    10.375588     10.278541   \n",
            "min       1.000000     1.000000     1.000000     1.000000      1.000000   \n",
            "25%      10.000000     9.000000    10.000000     9.000000     10.000000   \n",
            "50%      19.000000    19.000000    19.000000    18.000000     19.000000   \n",
            "75%      28.000000    27.000000    27.000000    27.000000     28.000000   \n",
            "max      36.000000    36.000000    36.000000    36.000000     36.000000   \n",
            "\n",
            "       Prev_Afternoon  Prev_Evening  \n",
            "count     1408.000000   1408.000000  \n",
            "mean        18.620739     18.535511  \n",
            "std         10.356869     10.239569  \n",
            "min          1.000000      1.000000  \n",
            "25%          9.000000     10.000000  \n",
            "50%         19.000000     19.000000  \n",
            "75%         27.000000     27.000000  \n",
            "max         36.000000     36.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3 TRAIN RANDOM FOREST MODEL ON HISTORICAL DATA AND APPLY TO CURRENT DATA\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the historical dataset\n",
        "historical_data = pd.read_csv('/content/train_data_features.csv')\n",
        "y_hist = pd.read_csv('/content/train_data_target.csv')\n",
        "logger.info(\"Historical dataset loaded successfully.\")\n",
        "\n",
        "# Split historical data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(historical_data, y_hist, test_size=0.2, random_state=42)\n",
        "logger.info(f\"Data split into training and validation sets. Training set size: {X_train.shape}, Validation set size: {X_val.shape}\")\n",
        "\n",
        "# Fill the first row's NaN values in 'Prev_Morning', 'Prev_Afternoon', and 'Prev_Evening' columns\n",
        "X_train['Prev_Morning'].fillna(18, inplace=True)\n",
        "X_train['Prev_Afternoon'].fillna(18, inplace=True)\n",
        "X_train['Prev_Evening'].fillna(18, inplace=True)\n",
        "\n",
        "# Exclude the first row from both X_train and y_train\n",
        "X_train = X_train.iloc[1:].reset_index(drop=True)\n",
        "y_train = y_train.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "# Remove row 518 from X_train and y_train\n",
        "X_train = X_train.drop(index=518).reset_index(drop=True)\n",
        "y_train = y_train.drop(index=518).reset_index(drop=True)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "random_forest_model = RandomForestRegressor(random_state=42)\n",
        "random_forest_model.fit(X_train, y_train.values.ravel())\n",
        "logger.info(\"Random Forest model trained on historical data.\")\n",
        "\n",
        "# Evaluation on the validation set\n",
        "y_val_pred = random_forest_model.predict(X_val)\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "r2_score_val = r2_score(y_val, y_val_pred)\n",
        "logger.info(f\"Validation MSE: {mse_val}, R2 Score: {r2_score_val}\")\n",
        "\n",
        "# Save the trained model\n",
        "model_path = '/content/random_forest_prediction_model_hist.pkl'\n",
        "joblib.dump(random_forest_model, model_path)\n",
        "logger.info(f\"RandomForest model trained on historical data saved to {model_path}.\")\n",
        "\n",
        "# Load and prepare the current dataset for prediction\n",
        "current_data = pd.read_csv('/content/current_data.csv')\n",
        "X_current = current_data.drop('Prediction1', axis=1)\n",
        "\n",
        "# Fill NaN values in the first row of 'Prev_Morning', 'Prev_Afternoon', and 'Prev_Evening'\n",
        "X_current.at[0, 'Prev_Morning'] = 18\n",
        "X_current.at[0, 'Prev_Afternoon'] = 18\n",
        "X_current.at[0, 'Prev_Evening'] = 18\n",
        "\n",
        "# Save the updated current_data with imputed values\n",
        "current_data.loc[X_current.index, ['Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']] = X_current[['Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']]\n",
        "current_data.to_csv('/content/current_data.csv', index=False)\n",
        "logger.info(\"Current data with imputed values saved as 'current_data.csv'\")\n",
        "\n",
        "# Generate predictions for the current data\n",
        "y_pred_current = random_forest_model.predict(X_current)\n",
        "current_data['Prediction1'] = y_pred_current\n",
        "\n",
        "# Save the current data with predictions\n",
        "current_data.to_csv('/content/current_data_with_predictions.csv', index=False)\n",
        "logger.info(\"Current data with predictions saved.\")\n",
        "\n",
        "logger.info(\"Model training and application to current data completed.\")\n",
        "\n",
        "# Additional Checks (if applicable)\n",
        "print(\"First few rows of X_train after adjustments:\")\n",
        "print(X_train.head())\n",
        "print(\"\\nFirst few rows of y_train:\")\n",
        "print(y_train.head())\n",
        "print(\"\\nFirst row in X_current after filling NaNs:\")\n",
        "print(X_current.head(1))\n"
      ],
      "metadata": {
        "id": "0CNfE2mY1nB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd83f949-a2f3-4376-cf74-d143d2148fc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of X_train after adjustments:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2020      8   18          2          35    10.333333           5.0   \n",
            "1  2022      9   13         16          22    29.333333          26.0   \n",
            "2  2018      9   26          2           9    20.333333          22.0   \n",
            "3  2019      1   31         22           4    27.333333          32.0   \n",
            "4  2019      8    5         20          18    26.333333          25.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0             4.0          19.0  \n",
            "1            16.0          30.0  \n",
            "2            32.0          35.0  \n",
            "3             8.0          11.0  \n",
            "4            29.0           3.0  \n",
            "\n",
            "First few rows of y_train:\n",
            "   Morning\n",
            "0        8\n",
            "1       35\n",
            "2        8\n",
            "3       19\n",
            "4       22\n",
            "\n",
            "First row in X_current after filling NaNs:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    1          7          23         19.0          18.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            18.0          18.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Check for NaN Values in All Axes\n",
        "\n",
        "# Check in X_train\n",
        "print(\"NaN values in X_train:\")\n",
        "print(X_train.isnull().sum())\n",
        "\n",
        "# Check in y_train\n",
        "print(\"\\nNaN values in y_train:\")\n",
        "print(y_train.isnull().sum())\n",
        "\n",
        "# Check in X_current\n",
        "print(\"\\nNaN values in X_current:\")\n",
        "print(X_current.isnull().sum())\n",
        "\n",
        "# Check in current_data\n",
        "print(\"\\nNaN values in current_data:\")\n",
        "print(current_data.isnull().sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDssvACI2Aul",
        "outputId": "fcb18741-dfa2-4d89-a5bc-70d23faf1559"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in X_train:\n",
            "Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n",
            "\n",
            "NaN values in y_train:\n",
            "Morning    0\n",
            "dtype: int64\n",
            "\n",
            "NaN values in X_current:\n",
            "Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n",
            "\n",
            "NaN values in current_data:\n",
            "Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "Prediction1       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4.1. MODEL INTERPRETATION\n",
        "\n",
        "# Check for NaN values in y_test_p1\n",
        "nan_count = y_test_p1.isnull().sum()\n",
        "logger.info(f\"Number of NaN values in y_test_p1: {nan_count}\")\n",
        "\n",
        "# If NaN values exist, print some examples\n",
        "if nan_count > 0:\n",
        "    logger.info(\"Examples of NaN values in y_test_p1:\")\n",
        "    logger.info(y_test_p1[y_test_p1.isnull()])\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the finalized model\n",
        "final_model = joblib.load('/content/random_forest_prediction_model_imputed.pkl')\n",
        "\n",
        "# Apply the same imputation and feature modification to X_test_p1\n",
        "imputed_test = imputer.transform(X_test_p1)\n",
        "X_test_p1_imputed, missing_indicator_test = imputed_test[:, :-1], imputed_test[:, -1]\n",
        "X_test_p1_imputed['Target_Missing'] = missing_indicator_test\n",
        "\n",
        "# Using SHAP to interpret the model\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_test_p1_imputed)\n",
        "shap.summary_plot(shap_values, X_test_p1_imputed, plot_type=\"bar\")\n",
        "\n",
        "# STEP 4.2. FINAL MODEL SELECTION AND REPORTING\n",
        "\n",
        "# Report the chosen model's evaluation metrics\n",
        "y_pred_final = final_model.predict(X_test_p1_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_p1, y_pred_final)\n",
        "precision = precision_score(y_test_p1, y_pred_final, average='macro')\n",
        "recall = recall_score(y_test_p1, y_pred_final, average='macro')\n",
        "f1 = f1_score(y_test_p1, y_pred_final, average='macro')\n",
        "\n",
        "logger.info(f\"Model Performance Metrics:\\n Accuracy: {accuracy}\\n Precision: {precision}\\n Recall: {recall}\\n F1 Score: {f1}\")\n",
        "\n",
        "# Add cross-validation implementation here if applicable\n",
        "# ...code for cross-validation...\n",
        "\n",
        "# Analysis of model performance\n",
        "# ...code/logic for detailed analysis of errors, biases, etc...\n",
        "\n",
        "# STEP 4.3. PREPARATION FOR DEPLOYMENT\n",
        "\n",
        "# ...code for deployment preparation...\n",
        "# Assuming the model will be deployed in a specific environment\n",
        "# Include any necessary steps for preparing the model for deployment\n",
        "# This might include serialization, testing the model in a deployment-like environment, etc.\n",
        "\n",
        "# STEP 4.4. DOCUMENTATION AND REPORTING\n",
        "\n",
        "# Prepare a comprehensive report\n",
        "report = f\"\"\"\n",
        "Model Selection Rationale:\n",
        "- The chosen model (e.g., Random Forest) was selected due to its superior performance in terms of accuracy, precision, and recall.\n",
        "\n",
        "Model Performance:\n",
        "- Accuracy: {accuracy}\n",
        "- Precision: {precision}\n",
        "- Recall: {recall}\n",
        "- F1-Score: {f1}\n",
        "\n",
        "Additional Model Analysis:\n",
        "- Detailed error analysis, biases, etc.\n",
        "- Results from cross-validation (if performed).\n",
        "\n",
        "Limitations and Recommendations:\n",
        "- The model may have limitations in terms of scalability or real-time prediction.\n",
        "- Future work could explore more advanced models or feature engineering techniques.\n",
        "\n",
        "Deployment Steps:\n",
        "- The model will be deployed in a cloud-based environment.\n",
        "- Necessary steps for deployment include serialization and environment setup.\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Model documentation and reporting completed.\")\n",
        "\n",
        "# Final Checks and Tests (if applicable)\n",
        "# Include any additional code for final testing or checks before deployment\n",
        "\n",
        "logger.info(\"Final checks and tests completed.\")\n",
        "\n",
        "logger.info(\"Cell 4 tasks completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "V28FJ39EHF8y",
        "outputId": "2960ed02-4db7-4c26-ead8-6fad9f78cfa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-30684bf1f5ec>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check for NaN values in y_test_p1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnan_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_p1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of NaN values in y_test_p1: {nan_count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test_p1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5. CROSS-VALIDATION AND ADDITIONAL METRICS ANALYSIS\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming you are using RandomForestRegressor as your model\n",
        "model = RandomForestRegressor(random_state=random_seed)\n",
        "\n",
        "# Define your scoring metrics\n",
        "# Remove ROC AUC score if your task is regression\n",
        "scoring_metrics = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='macro'),\n",
        "    'recall': make_scorer(recall_score, average='macro'),\n",
        "    'f1': make_scorer(f1_score, average='macro')\n",
        "}\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "k_folds = 10  # Number of folds\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    scores = cross_val_score(model, X_p1, y_p1.fillna(-999), scoring=scorer, cv=k_folds)\n",
        "    cv_results[metric_name] = scores\n",
        "    logger.info(f\"{metric_name} scores for each fold: {scores}\")\n",
        "    logger.info(f\"Average {metric_name} over {k_folds} folds: {scores.mean()}\")\n",
        "\n",
        "# Additional metrics analysis and error/bias exploration\n",
        "# ... Add your code for detailed analysis of errors, biases, etc. ...\n",
        "logger.info(\"Cross-validation and additional metrics analysis completed.\")\n",
        "\n",
        "# Feature Importance Analysis using SHAP\n",
        "# Assuming 'final_prediction_model' is your trained RandomForestRegressor model\n",
        "import shap\n",
        "\n",
        "# Load the trained model (if not already loaded)\n",
        "final_prediction_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(final_prediction_model)\n",
        "shap_values = explainer.shap_values(X_train_p1)\n",
        "\n",
        "# Plot summary plot using SHAP values\n",
        "shap.summary_plot(shap_values, X_train_p1)\n",
        "\n",
        "logger.info(\"Feature importance analysis using SHAP completed.\")"
      ],
      "metadata": {
        "id": "YupgSJuhz3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6. DETAILED ERROR AND BIAS ANALYSIS\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive to access the predictions_df.csv file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in Google Drive where the predictions DataFrame is saved\n",
        "predictions_df_path = '/content/drive/My Drive/predictions_df.csv'\n",
        "\n",
        "# Load the DataFrame from the CSV file if it exists, otherwise create it\n",
        "if os.path.exists(predictions_df_path):\n",
        "    predictions_df = pd.read_csv(predictions_df_path)\n",
        "    logger.info(\"predictions_df loaded from Google Drive successfully.\")\n",
        "else:\n",
        "    # Make sure final_model is loaded\n",
        "    final_model = joblib.load('/content/final_prediction_model.pkl')\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred_final = final_model.predict(X_test_p1)\n",
        "\n",
        "    # Create the predictions DataFrame\n",
        "    predictions_df = pd.DataFrame({'Actual': y_test_p1, 'Predicted': y_pred_final})\n",
        "\n",
        "    # Save the new predictions_df to Google Drive for future use\n",
        "    predictions_df.to_csv(predictions_df_path, index=False)\n",
        "    logger.info(\"predictions_df saved to Google Drive successfully.\")\n",
        "\n",
        "# Merge 'Prev_Week' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Week']], left_index=True, right_index=True)\n",
        "# Merge 'Prev_Entry' into predictions_df\n",
        "predictions_df = predictions_df.merge(data[['Prev_Entry']], left_index=True, right_index=True)\n",
        "\n",
        "# Proceed with error analysis only if predictions_df is loaded or created\n",
        "if 'predictions_df' in locals():\n",
        "    # Analyze error distribution\n",
        "    predictions_df['Error'] = predictions_df['Predicted'] - predictions_df['Actual']\n",
        "    predictions_df['Absolute_Error'] = predictions_df['Error'].abs()\n",
        "\n",
        "    # Plotting error distribution\n",
        "    plt.hist(predictions_df['Error'], bins=30)\n",
        "    plt.title('Error Distribution')\n",
        "    plt.xlabel('Prediction Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Week'\n",
        "prev_week_performance = predictions_df.groupby('Prev_Week').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_week_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Week')\n",
        "plt.xlabel('Previous Week Draw')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Subgroup analysis based on 'Prev_Entry'\n",
        "prev_entry_performance = predictions_df.groupby('Prev_Entry').mean()['Absolute_Error']\n",
        "plt.figure(figsize=(10, 6))\n",
        "prev_entry_performance.plot(kind='bar')\n",
        "plt.title('Performance by Previous Entry')\n",
        "plt.xlabel('Previous Entry')\n",
        "plt.ylabel('Average Absolute Error')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Document findings\n",
        "error_bias_report = \"\"\"\n",
        "Detailed Error Analysis:\n",
        "- Error Distribution Insights: {'Describe your findings from the error distribution here'}\n",
        "- Largest Errors: {'Describe characteristics of instances with largest errors here'}\n",
        "\n",
        "Bias Exploration:\n",
        "- Performance by Previous Week: {'Describe performance variations based on the previous week here'}\n",
        "- Performance by Previous Entry: {'Describe performance variations based on the previous entry here'}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Error and bias analysis completed.\")\n",
        "logger.info(error_bias_report)\n"
      ],
      "metadata": {
        "id": "mr21I98826sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7. FINAL REVIEW, DEPLOYMENT PREPARATION, AND DOCUMENTATION\n",
        "\n",
        "# Final Model Review and Refinement\n",
        "# ... Code/comments for any last adjustments to the model ...\n",
        "\n",
        "# Deployment Preparation\n",
        "# Serialize the final model\n",
        "joblib.dump(final_prediction_model, '/content/final_prediction_model_for_deployment.pkl')\n",
        "\n",
        "# Comprehensive Documentation Update\n",
        "# ... Update your comprehensive report with all final findings and methodologies ...\n",
        "\n",
        "# Final Checks and Tests\n",
        "# ... Code/comments for final tests and checks ...\n",
        "\n",
        "# Planning for Future Improvements\n",
        "future_improvement_plan = \"\"\"\n",
        "Future Improvement Plans:\n",
        "- Areas for further research: {describe areas for future research}\n",
        "- Methodologies to explore: {describe potential methodologies for future iterations}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Final review and deployment preparation completed.\")\n",
        "logger.info(future_improvement_plan)\n"
      ],
      "metadata": {
        "id": "qssa3uxO9tru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "u8Y77oV7koS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "WsQJRnWtkpPO"
      }
    }
  ]
}