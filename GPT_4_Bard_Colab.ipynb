{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3V/HufzPKpAC+HADBdL+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Project2023_v3/blob/main/GPT_4_Bard_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s9bpuQSY3t6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1.1: Package Installation\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas gdown numpy matplotlib scikit-learn xgboost shap\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install black  # Install Black for code formatting\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plnc-ffhAUCk",
        "outputId": "5e1652a8-0a5b-4dc7-bafc-9acf2f4531f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (23.12.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (23.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.5.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1.2: Import Libraries and Set Up Logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import KNNImputer\n",
        "import shap\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "fP_Q74gUBGQi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.3: Load, Split, and Preprocess Data from Google Drive\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive\n",
        "drive_dataset_directory = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/'\n",
        "\n",
        "# Define the path to the comprehensive CSV file for training and testing\n",
        "csv_filename_train_test = '1_Model_Train_Test_Data.csv'\n",
        "drive_csv_path_train_test = os.path.join(drive_dataset_directory, csv_filename_train_test)\n",
        "\n",
        "# Define the path to the CSV file for unseen data\n",
        "csv_filename_unseen = '2_Model_Unseen_Data.csv'\n",
        "drive_csv_path_unseen = os.path.join(drive_dataset_directory, csv_filename_unseen)\n",
        "\n",
        "# Function to preprocess data\n",
        "def preprocess_data(data, is_training=True):\n",
        "    logger.info(\"Starting preprocessing for dataset\")\n",
        "\n",
        "    # Convert 'Date' to datetime and extract 'Year', 'Month', and 'Day'\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data['Year'] = data['Date'].dt.year\n",
        "    data['Month'] = data['Date'].dt.month\n",
        "    data['Day'] = data['Date'].dt.day\n",
        "    logger.info(\"Extracted year, month, and day from 'Date' column\")\n",
        "\n",
        "    # Drop the 'Date' column\n",
        "    data.drop(columns=['Date'], inplace=True)\n",
        "    logger.info(\"'Date' column dropped after extraction of year, month, and day\")\n",
        "\n",
        "    # Apply the moving averages calculation only for training/testing data\n",
        "    if is_training:\n",
        "        # Function to calculate moving averages with dynamic window size\n",
        "        def calculate_moving_averages(data, window_size, columns_to_average, target_columns):\n",
        "            try:\n",
        "                for col, target_col in zip(columns_to_average, target_columns):\n",
        "                    data[target_col] = data[col].rolling(window=window_size, min_periods=1).mean()\n",
        "                logger.info(f\"Calculated moving averages for specified columns with window size: {window_size}\")\n",
        "            except Exception as e:\n",
        "                logger.error(\"Error in moving average calculation: %s\", e)\n",
        "\n",
        "        # Apply the function with an initial window size\n",
        "        initial_window_size = 2  # Adjust as needed\n",
        "        columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "        target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "        # Initialize moving average columns with default values (e.g., 0)\n",
        "        for col in target_columns:\n",
        "            data[col] = 0\n",
        "\n",
        "        calculate_moving_averages(data, initial_window_size, columns_to_average, target_columns)\n",
        "\n",
        "    # Adjust entries to use previous day's data\n",
        "    data['Prev_Morning'] = data['Morning'].shift(1)\n",
        "    data['Prev_Afternoon'] = data['Afternoon'].shift(1)\n",
        "    data['Prev_Evening'] = data['Evening'].shift(1)\n",
        "    logger.info(\"Created previous day columns\")\n",
        "\n",
        "    # Create 'Prediction1' column\n",
        "    # Initially, set it to the values from 'Morning' column\n",
        "    data['Prediction1'] = data['Morning']\n",
        "\n",
        "    # Handle NaN values for new columns\n",
        "    data['Prev_Morning'].fillna(18, inplace=True)  # Adjust default values as needed\n",
        "    data['Prev_Afternoon'].fillna(18, inplace=True)\n",
        "    data['Prev_Evening'].fillna(18, inplace=True)\n",
        "\n",
        "    # Apply specific operations for training/testing data\n",
        "    if is_training:\n",
        "        # Drop the first row\n",
        "        data = data.iloc[1:]\n",
        "\n",
        "        # Conditionally drop row with index 518 if it exists\n",
        "        if 518 in data.index:\n",
        "            data = data.drop(index=518)\n",
        "\n",
        "        # Reset index after row exclusions\n",
        "        data = data.reset_index(drop=True)\n",
        "        logger.info(\"Specific rows dropped and index reset for training/testing data\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Check and load the datasets\n",
        "def load_dataset(file_path):\n",
        "    if os.path.isfile(file_path):\n",
        "        print(\"File found. Proceeding to load the dataset.\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"File not found. Check the file path or the Google Drive mount.\")\n",
        "        return None\n",
        "\n",
        "train_test_data = load_dataset(drive_csv_path_train_test)\n",
        "unseen_data = load_dataset(drive_csv_path_unseen)\n",
        "\n",
        "# Logging the start of dataset loading\n",
        "logger.info(\"Reading the training/testing dataset from Google Drive\")\n",
        "logger.info(\"Reading the unseen dataset from Google Drive\")\n",
        "\n",
        "# Apply preprocessing to training/testing and unseen datasets\n",
        "train_test_data = preprocess_data(train_test_data, is_training=True) if train_test_data is not None else None\n",
        "unseen_data = preprocess_data(unseen_data, is_training=False) if unseen_data is not None else None\n",
        "logger.info(\"Preprocessing applied to both training/testing and unseen datasets\")\n",
        "\n",
        "# Define paths to save the preprocessed data\n",
        "preprocessed_train_test_path = os.path.join(drive_dataset_directory, '7_preprocessed_train_test_data.csv')\n",
        "preprocessed_unseen_path = os.path.join(drive_dataset_directory, '8_preprocessed_unseen_data.csv')\n",
        "\n",
        "# Saving preprocessed data\n",
        "if train_test_data is not None:\n",
        "    train_test_data.to_csv(preprocessed_train_test_path, index=False)\n",
        "    logger.info(\"Preprocessed training/testing data saved to Google Drive.\")\n",
        "\n",
        "if unseen_data is not None:\n",
        "    unseen_data.to_csv(preprocessed_unseen_path, index=False)\n",
        "    logger.info(\"Preprocessed unseen data saved to Google Drive.\")\n",
        "\n",
        "# [Continue with further data processing...]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbrz72E39KQY",
        "outputId": "7d127054-df63-4618-ed96-ba465d9e179c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found. Proceeding to load the dataset.\n",
            "File found. Proceeding to load the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2.1: Saving Preprocessed Data\n",
        "\n",
        "# Define the paths to save the preprocessed data\n",
        "preprocessed_train_test_path = os.path.join(drive_dataset_directory, '3_train_test_features.csv')\n",
        "preprocessed_train_test_target_path = os.path.join(drive_dataset_directory, '4_train_test_target.csv')\n",
        "preprocessed_unseen_features_path = os.path.join(drive_dataset_directory, '5_unseen_features.csv')\n",
        "preprocessed_unseen_target_path = os.path.join(drive_dataset_directory, '6_unseen_target.csv')\n",
        "\n",
        "# Save preprocessed training/testing data\n",
        "train_test_data.to_csv(preprocessed_train_test_path, index=False)\n",
        "logger.info(\"Preprocessed training/testing features saved to Google Drive.\")\n",
        "\n",
        "# Assuming 'Prediction1' is the target for training/testing data\n",
        "train_test_target = train_test_data[['Prediction1']]\n",
        "train_test_data.drop(columns=['Prediction1'], inplace=True)\n",
        "train_test_target.to_csv(preprocessed_train_test_target_path, index=False)\n",
        "logger.info(\"Preprocessed training/testing target saved to Google Drive.\")\n",
        "\n",
        "# Save preprocessed unseen data\n",
        "# Assuming we split features and target as we did with train_test_data\n",
        "unseen_target = unseen_data[['Prediction1']]\n",
        "unseen_data.drop(columns=['Prediction1'], inplace=True)\n",
        "unseen_data.to_csv(preprocessed_unseen_features_path, index=False)\n",
        "unseen_target.to_csv(preprocessed_unseen_target_path, index=False)\n",
        "logger.info(\"Preprocessed unseen features and target saved to Google Drive.\")\n",
        "\n",
        "# Print the first few rows of the preprocessed training/testing data for visual confirmation\n",
        "print(\"First few rows of the preprocessed training/testing features:\")\n",
        "print(train_test_data.head())\n",
        "print(\"\\nFirst few rows of the preprocessed training/testing target:\")\n",
        "print(train_test_target.head())\n",
        "\n",
        "# Print the first few rows of the preprocessed unseen data for visual confirmation\n",
        "print(\"\\nFirst few rows of the preprocessed unseen features:\")\n",
        "print(unseen_data.head())\n",
        "print(\"\\nFirst few rows of the preprocessed unseen target:\")\n",
        "print(unseen_target.head())\n",
        "\n",
        "# Optionally, print the shape and column names for further confirmation\n",
        "print(\"\\nShape of the training/testing features DataFrame:\", train_test_data.shape)\n",
        "print(\"Column names:\", train_test_data.columns)\n",
        "print(\"Shape of the training/testing target DataFrame:\", train_test_target.shape)\n",
        "print(\"Column names:\", train_test_target.columns)\n",
        "print(\"Shape of the unseen features DataFrame:\", unseen_data.shape)\n",
        "print(\"Column names:\", unseen_data.columns)\n",
        "print(\"Shape of the unseen target DataFrame:\", unseen_target.shape)\n",
        "print(\"Column names:\", unseen_target.columns)\n",
        "\n",
        "# [Continue with further data processing...]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2J6MXxcnlH5",
        "outputId": "d3aa2f55-b083-424a-8acb-3cecda99d509"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the preprocessed training/testing features:\n",
            "   Row Number Data_Type  Morning  Prev_Week  Rep_Prev_Week  Prev_Entry  \\\n",
            "0           2  Training       31         11              0           9   \n",
            "1           3  Training       15         19              0          12   \n",
            "2           4  Training       31         35              0          35   \n",
            "3           5  Training       31         18              0          16   \n",
            "4           6  Training       21         13              0          18   \n",
            "\n",
            "   Rep_Prev_Entry  Mov_Avg_Mor  Afternoon  Prev_Week.1  ...  Rep_Prev_Week.3  \\\n",
            "0               0         25.0          3           21  ...                0   \n",
            "1               0         23.0          9           19  ...                0   \n",
            "2               0         23.0         21           20  ...                0   \n",
            "3               0         31.0         31           30  ...                1   \n",
            "4               0         26.0         17           34  ...                0   \n",
            "\n",
            "   Prev_Entry.3  Rep_Prev_Entry.3  Mov_Avg_Nig  Year  Month  Day  \\\n",
            "0            35                 0         10.5  2018      8    2   \n",
            "1            23                 0         23.5  2018      8    3   \n",
            "2            29                 0         25.5  2018      8    4   \n",
            "3            15                 0         17.0  2018      8    6   \n",
            "4             8                 0         23.0  2018      8    7   \n",
            "\n",
            "   Prev_Morning  Prev_Afternoon  Prev_Evening  \n",
            "0          19.0            14.0          33.0  \n",
            "1          31.0             3.0          35.0  \n",
            "2          15.0             9.0          23.0  \n",
            "3          31.0            21.0          29.0  \n",
            "4          31.0            31.0          15.0  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "First few rows of the preprocessed training/testing target:\n",
            "   Prediction1\n",
            "0           31\n",
            "1           15\n",
            "2           31\n",
            "3           31\n",
            "4           21\n",
            "\n",
            "First few rows of the preprocessed unseen features:\n",
            "   Row Number Data_Type  Morning  Prev_Week  Rep_Prev_Week  Prev_Entry  \\\n",
            "0        1410    Unseen      NaN         27           26.0           5   \n",
            "1        1411    Unseen      NaN         33           22.5          18   \n",
            "2        1412    Unseen      NaN         27           15.0          28   \n",
            "3        1413    Unseen      NaN         20           15.5           2   \n",
            "4        1414    Unseen      NaN         29           21.5          12   \n",
            "\n",
            "   2WeeksM  Mov_Avg_Mor  Afternoon  Prev_Week.1  ...  Rep_Prev_Week.3  \\\n",
            "0      NaN         26.0         20            7  ...              NaN   \n",
            "1     12.0         22.5         31           18  ...              NaN   \n",
            "2      3.0         15.0          5           22  ...              NaN   \n",
            "3     11.0         15.5         28           29  ...              NaN   \n",
            "4     14.0         21.5         35            7  ...              NaN   \n",
            "\n",
            "   Prev_Entry.3  2WeeksN  Mov_Avg_Nig  Year  Month  Day  Prev_Morning  \\\n",
            "0            26      NaN         14.5  2023      8    1          18.0   \n",
            "1             7      5.0          6.5  2023      8    2          18.0   \n",
            "2             2      6.0         18.0  2023      8    3          18.0   \n",
            "3            22      7.0          4.5  2023      8    4          18.0   \n",
            "4            31     18.0         10.5  2023      8    5          18.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            18.0          18.0  \n",
            "1            20.0          26.0  \n",
            "2            31.0           7.0  \n",
            "3             5.0           2.0  \n",
            "4            28.0          22.0  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "First few rows of the preprocessed unseen target:\n",
            "   Prediction1\n",
            "0          NaN\n",
            "1          NaN\n",
            "2          NaN\n",
            "3          NaN\n",
            "4          NaN\n",
            "\n",
            "Shape of the training/testing features DataFrame: (1407, 32)\n",
            "Column names: Index(['Row Number', 'Data_Type', 'Morning', 'Prev_Week', 'Rep_Prev_Week',\n",
            "       'Prev_Entry', 'Rep_Prev_Entry', 'Mov_Avg_Mor', 'Afternoon',\n",
            "       'Prev_Week.1', 'Rep_Prev_Week.1', 'Prev_Entry.1', 'Rep_Prev_Entry.1',\n",
            "       'Mov_Avg_Aft', 'Evening', 'Prev_Week.2', 'Rep_Prev_Week.2',\n",
            "       'Prev_Entry.2', 'Rep_Prev_Entry.2', 'Mov_Avg_Eve', 'Night',\n",
            "       'Prev_Week.3', 'Rep_Prev_Week.3', 'Prev_Entry.3', 'Rep_Prev_Entry.3',\n",
            "       'Mov_Avg_Nig', 'Year', 'Month', 'Day', 'Prev_Morning', 'Prev_Afternoon',\n",
            "       'Prev_Evening'],\n",
            "      dtype='object')\n",
            "Shape of the training/testing target DataFrame: (1407, 1)\n",
            "Column names: Index(['Prediction1'], dtype='object')\n",
            "Shape of the unseen features DataFrame: (6, 32)\n",
            "Column names: Index(['Row Number', 'Data_Type', 'Morning', 'Prev_Week', 'Rep_Prev_Week',\n",
            "       'Prev_Entry', '2WeeksM', 'Mov_Avg_Mor', 'Afternoon', 'Prev_Week.1',\n",
            "       'Rep_Prev_Week.1', 'Prev_Entry.1', '2WeeksA', 'Mov_Avg_Aft', 'Evening',\n",
            "       'Prev_Week.2', 'Rep_Prev_Week.2', 'Prev_Entry.2', '2WeeksE',\n",
            "       'Mov_Avg_Eve', 'Night', 'Prev_Week.3', 'Rep_Prev_Week.3',\n",
            "       'Prev_Entry.3', '2WeeksN', 'Mov_Avg_Nig', 'Year', 'Month', 'Day',\n",
            "       'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening'],\n",
            "      dtype='object')\n",
            "Shape of the unseen target DataFrame: (6, 1)\n",
            "Column names: Index(['Prediction1'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.2: Feature Finalization and Saving for Model Input\n",
        "\n",
        "def finalize_and_save_features(data, filename_suffix, file_number):\n",
        "    logger.info(f\"Finalizing features for model input and saving processed data for {filename_suffix}\")\n",
        "\n",
        "    selected_columns = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "    X = data[selected_columns]\n",
        "\n",
        "    # Save the processed data features to CSV with the updated naming convention\n",
        "    file_name = f'{file_number}_{filename_suffix}_features.csv'\n",
        "    file_path = os.path.join(drive_dataset_directory, file_name)\n",
        "    X.to_csv(file_path, index=False)\n",
        "    logger.info(f\"Processed data features for {filename_suffix} saved to Google Drive as {file_name}.\")\n",
        "\n",
        "    # Print the first few rows for verification\n",
        "    print(f\"First few rows of {filename_suffix} features:\")\n",
        "    print(X.head())\n",
        "\n",
        "# Apply to training/testing data\n",
        "finalize_and_save_features(train_test_data, 'train_test', '9')\n",
        "\n",
        "# Apply to unseen data\n",
        "finalize_and_save_features(unseen_data, 'unseen', '10')\n"
      ],
      "metadata": {
        "id": "GGRPjzS2ILM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9985aa-5701-45b7-bf82-d26a8dcdda76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of train_test features:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    2         11           9         25.0          19.0   \n",
            "1  2018      8    3         19          12         23.0          31.0   \n",
            "2  2018      8    4         35          35         23.0          15.0   \n",
            "3  2018      8    6         18          16         31.0          31.0   \n",
            "4  2018      8    7         13          18         26.0          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            14.0          33.0  \n",
            "1             3.0          35.0  \n",
            "2             9.0          23.0  \n",
            "3            21.0          29.0  \n",
            "4            31.0          15.0  \n",
            "First few rows of unseen features:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2023      8    1         27           5         26.0          18.0   \n",
            "1  2023      8    2         33          18         22.5          18.0   \n",
            "2  2023      8    3         27          28         15.0          18.0   \n",
            "3  2023      8    4         20           2         15.5          18.0   \n",
            "4  2023      8    5         29          12         21.5          18.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            18.0          18.0  \n",
            "1            20.0          26.0  \n",
            "2            31.0           7.0  \n",
            "3             5.0           2.0  \n",
            "4            28.0          22.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3.1: Additional Data Insights for CSVs\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Exploring additional data insights for CSVs...\")\n",
        "\n",
        "# Define the directory for datasets in Google Drive\n",
        "drive_dataset_directory = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/'\n",
        "\n",
        "# Read the CSV files with updated paths\n",
        "train_test_features = pd.read_csv(drive_dataset_directory + '9_train_test_features.csv')\n",
        "train_test_target = pd.read_csv(drive_dataset_directory + '4_train_test_target.csv')\n",
        "unseen_features = pd.read_csv(drive_dataset_directory + '10_unseen_features.csv')\n",
        "unseen_target = pd.read_csv(drive_dataset_directory + '6_unseen_target.csv')\n",
        "\n",
        "# Function to print data insights\n",
        "def print_data_insights(df, df_name):\n",
        "    print(f\"Data Insights for {df_name}:\\n\")\n",
        "    print(\"Missing values in each column:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    print(\"\\nPercentage of missing values in each column:\")\n",
        "    print(df.isnull().mean() * 100)\n",
        "\n",
        "    print(\"\\nSummary statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    print(\"\\nFirst few rows of data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\n-----------------------------------------\\n\")\n",
        "\n",
        "# Print insights for each DataFrame\n",
        "print_data_insights(train_test_features, \"Train/Test Features\")\n",
        "print_data_insights(train_test_target, \"Train/Test Target\")\n",
        "print_data_insights(unseen_features, \"Unseen Features\")\n",
        "print_data_insights(unseen_target, \"Unseen Target\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4mPofrAFRsH",
        "outputId": "857a8575-3908-4f96-ec2f-56b9c640d04d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Insights for Train/Test Features:\n",
            "\n",
            "Missing values in each column:\n",
            "Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Year              0.0\n",
            "Month             0.0\n",
            "Day               0.0\n",
            "Prev_Week         0.0\n",
            "Prev_Entry        0.0\n",
            "Mov_Avg_Mor       0.0\n",
            "Prev_Morning      0.0\n",
            "Prev_Afternoon    0.0\n",
            "Prev_Evening      0.0\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics:\n",
            "              Year        Month          Day    Prev_Week   Prev_Entry  \\\n",
            "count  1407.000000  1407.000000  1407.000000  1407.000000  1407.000000   \n",
            "mean   2020.577825     6.629709    15.626866    18.212509    18.130064   \n",
            "std       1.556254     3.565689     8.752854    10.568730    10.368159   \n",
            "min    2018.000000     1.000000     1.000000     0.000000     1.000000   \n",
            "25%    2019.000000     3.000000     8.000000     9.000000     9.000000   \n",
            "50%    2021.000000     7.000000    16.000000    18.000000    18.000000   \n",
            "75%    2022.000000    10.000000    23.000000    27.000000    27.000000   \n",
            "max    2023.000000    12.000000    31.000000    36.000000    36.000000   \n",
            "\n",
            "       Mov_Avg_Mor  Prev_Morning  Prev_Afternoon  Prev_Evening  \n",
            "count  1407.000000   1407.000000     1407.000000   1407.000000  \n",
            "mean     18.773632     18.770434       18.623312     18.528785  \n",
            "std       7.076781     10.277406       10.360101     10.240097  \n",
            "min       1.500000      1.000000        1.000000      1.000000  \n",
            "25%      13.500000     10.000000        9.000000     10.000000  \n",
            "50%      19.000000     19.000000       19.000000     19.000000  \n",
            "75%      24.000000     28.000000       27.000000     27.000000  \n",
            "max      35.500000     36.000000       36.000000     36.000000  \n",
            "\n",
            "First few rows of data:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    2         11           9         25.0          19.0   \n",
            "1  2018      8    3         19          12         23.0          31.0   \n",
            "2  2018      8    4         35          35         23.0          15.0   \n",
            "3  2018      8    6         18          16         31.0          31.0   \n",
            "4  2018      8    7         13          18         26.0          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            14.0          33.0  \n",
            "1             3.0          35.0  \n",
            "2             9.0          23.0  \n",
            "3            21.0          29.0  \n",
            "4            31.0          15.0  \n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Data Insights for Train/Test Target:\n",
            "\n",
            "Missing values in each column:\n",
            "Prediction1    0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Prediction1    0.0\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics:\n",
            "       Prediction1\n",
            "count   1407.00000\n",
            "mean      18.77683\n",
            "std       10.27599\n",
            "min        1.00000\n",
            "25%       10.00000\n",
            "50%       19.00000\n",
            "75%       28.00000\n",
            "max       36.00000\n",
            "\n",
            "First few rows of data:\n",
            "   Prediction1\n",
            "0           31\n",
            "1           15\n",
            "2           31\n",
            "3           31\n",
            "4           21\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Data Insights for Unseen Features:\n",
            "\n",
            "Missing values in each column:\n",
            "Year              0\n",
            "Month             0\n",
            "Day               0\n",
            "Prev_Week         0\n",
            "Prev_Entry        0\n",
            "Mov_Avg_Mor       0\n",
            "Prev_Morning      0\n",
            "Prev_Afternoon    0\n",
            "Prev_Evening      0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Year              0.0\n",
            "Month             0.0\n",
            "Day               0.0\n",
            "Prev_Week         0.0\n",
            "Prev_Entry        0.0\n",
            "Mov_Avg_Mor       0.0\n",
            "Prev_Morning      0.0\n",
            "Prev_Afternoon    0.0\n",
            "Prev_Evening      0.0\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics:\n",
            "         Year  Month       Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  \\\n",
            "count     6.0    6.0  6.000000   6.000000    6.000000      6.00000   \n",
            "mean   2023.0    8.0  3.666667  26.833333   12.666667     19.25000   \n",
            "std       0.0    0.0  2.160247   4.308906    9.373722      4.71964   \n",
            "min    2023.0    8.0  1.000000  20.000000    2.000000     15.00000   \n",
            "25%    2023.0    8.0  2.250000  25.500000    6.500000     15.12500   \n",
            "50%    2023.0    8.0  3.500000  27.000000   11.500000     18.50000   \n",
            "75%    2023.0    8.0  4.750000  28.500000   16.500000     22.25000   \n",
            "max    2023.0    8.0  7.000000  33.000000   28.000000     26.00000   \n",
            "\n",
            "       Prev_Morning  Prev_Afternoon  Prev_Evening  \n",
            "count           6.0        6.000000      6.000000  \n",
            "mean           18.0       22.833333     17.666667  \n",
            "std             0.0       10.870449     11.183321  \n",
            "min            18.0        5.000000      2.000000  \n",
            "25%            18.0       18.500000      9.750000  \n",
            "50%            18.0       24.000000     20.000000  \n",
            "75%            18.0       30.250000     25.000000  \n",
            "max            18.0       35.000000     31.000000  \n",
            "\n",
            "First few rows of data:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2023      8    1         27           5         26.0          18.0   \n",
            "1  2023      8    2         33          18         22.5          18.0   \n",
            "2  2023      8    3         27          28         15.0          18.0   \n",
            "3  2023      8    4         20           2         15.5          18.0   \n",
            "4  2023      8    5         29          12         21.5          18.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  \n",
            "0            18.0          18.0  \n",
            "1            20.0          26.0  \n",
            "2            31.0           7.0  \n",
            "3             5.0           2.0  \n",
            "4            28.0          22.0  \n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Data Insights for Unseen Target:\n",
            "\n",
            "Missing values in each column:\n",
            "Prediction1    6\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Prediction1    100.0\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics:\n",
            "       Prediction1\n",
            "count          0.0\n",
            "mean           NaN\n",
            "std            NaN\n",
            "min            NaN\n",
            "25%            NaN\n",
            "50%            NaN\n",
            "75%            NaN\n",
            "max            NaN\n",
            "\n",
            "First few rows of data:\n",
            "   Prediction1\n",
            "0          NaN\n",
            "1          NaN\n",
            "2          NaN\n",
            "3          NaN\n",
            "4          NaN\n",
            "\n",
            "-----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3.2: Setting Bounds for Numerical Range and Preparing Data for Prediction\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    # Corrected path for loading the dataset\n",
        "    data_path = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/7_preprocessed_train_test_data.csv'\n",
        "    data = pd.read_csv(data_path)\n",
        "    logger.info(\"Dataset loaded successfully.\")\n",
        "\n",
        "    logger.info(\"Enforcing numerical bounds...\")\n",
        "\n",
        "    # Define all columns that should have values in the range of 1 to 36\n",
        "    columns_to_check = [\n",
        "        'Morning', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Afternoon',\n",
        "        'Prev_Week.1', 'Prev_Entry.1', 'Mov_Avg_Aft', 'Evening', 'Prev_Week.2',\n",
        "        'Prev_Entry.2', 'Mov_Avg_Eve', 'Night', 'Prev_Week.3', 'Prev_Entry.3',\n",
        "        'Mov_Avg_Nig', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prediction1'\n",
        "    ]\n",
        "\n",
        "    # Loop through these columns and enforce the range if they exist in the DataFrame\n",
        "    for col in columns_to_check:\n",
        "        if col in data.columns:\n",
        "            # Find values outside the range and enforce the range by clipping values\n",
        "            data[col] = data[col].clip(lower=1, upper=36)\n",
        "\n",
        "    # Ensure changes are reflected\n",
        "    print(data.describe())\n",
        "\n",
        "    # Prepare the current data with NaNs in 'Prediction1' for testing\n",
        "    selected_columns = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "    current_data = data[selected_columns].copy()  # Use .copy() to create an independent copy\n",
        "    current_data['Prediction1'] = np.nan  # Initialize 'Prediction1' with NaN\n",
        "\n",
        "    # After initializing 'Prediction1' with NaN\n",
        "    print(\"\\nFirst few rows of current data with 'Prediction1' initialized as NaN:\")\n",
        "    print(current_data.head())\n",
        "\n",
        "    # Display Row 1406 (End of Testing Data)\n",
        "    print(\"Row 1406 (End of Testing Data):\")\n",
        "    print(data.iloc[1406])  # Remember, Python indexing starts at 0\n",
        "\n",
        "    # Corrected filename to avoid conflict and match naming convention\n",
        "    prepared_current_data_path = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/11_prepared_current_data_for_prediction1.csv'\n",
        "    current_data.to_csv(prepared_current_data_path, index=False)\n",
        "    logger.info(\"Prepared current data saved to Google Drive.\")\n",
        "\n",
        "    # Confirming the file saving by reading and displaying the first few rows\n",
        "    logger.info(\"Verifying saved prepared current data:\")\n",
        "    saved_current_data = pd.read_csv(prepared_current_data_path)\n",
        "    print(saved_current_data.head())\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi7kZT26PrzA",
        "outputId": "0e8c5a4f-e8fa-4e80-afc7-8fb466e234c4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Row Number     Morning    Prev_Week  Rep_Prev_Week   Prev_Entry  \\\n",
            "count  1407.000000  1407.00000  1407.000000    1407.000000  1407.000000   \n",
            "mean    705.632552    18.77683    18.240938       0.027008    18.130064   \n",
            "std     406.713037    10.27599    10.520904       0.162164    10.368159   \n",
            "min       2.000000     1.00000     1.000000       0.000000     1.000000   \n",
            "25%     353.500000    10.00000     9.000000       0.000000     9.000000   \n",
            "50%     706.000000    19.00000    18.000000       0.000000    18.000000   \n",
            "75%    1057.500000    28.00000    27.000000       0.000000    27.000000   \n",
            "max    1409.000000    36.00000    36.000000       1.000000    36.000000   \n",
            "\n",
            "       Rep_Prev_Entry  Mov_Avg_Mor    Afternoon  Prev_Week.1  Rep_Prev_Week.1  \\\n",
            "count     1407.000000  1407.000000  1407.000000  1407.000000      1407.000000   \n",
            "mean         0.014925    18.773632    18.613362    18.156361         0.022033   \n",
            "std          0.121297     7.076781    10.361999    10.601390         0.146842   \n",
            "min          0.000000     1.500000     1.000000     1.000000         0.000000   \n",
            "25%          0.000000    13.500000     9.000000     9.000000         0.000000   \n",
            "50%          0.000000    19.000000    19.000000    19.000000         0.000000   \n",
            "75%          0.000000    24.000000    27.000000    27.000000         0.000000   \n",
            "max          1.000000    35.500000    36.000000    36.000000         1.000000   \n",
            "\n",
            "       ...  Prev_Entry.3  Rep_Prev_Entry.3  Mov_Avg_Nig         Year  \\\n",
            "count  ...   1407.000000       1407.000000  1407.000000  1407.000000   \n",
            "mean   ...     18.506041          0.026297    18.119048  2020.577825   \n",
            "std    ...     10.232215          0.160074     7.420336     1.556254   \n",
            "min    ...      1.000000          0.000000     1.000000  2018.000000   \n",
            "25%    ...     10.000000          0.000000    12.500000  2019.000000   \n",
            "50%    ...     19.000000          0.000000    18.000000  2021.000000   \n",
            "75%    ...     27.000000          0.000000    23.250000  2022.000000   \n",
            "max    ...     36.000000          1.000000    35.000000  2023.000000   \n",
            "\n",
            "             Month          Day  Prev_Morning  Prev_Afternoon  Prev_Evening  \\\n",
            "count  1407.000000  1407.000000   1407.000000     1407.000000   1407.000000   \n",
            "mean      6.629709    15.626866     18.770434       18.623312     18.528785   \n",
            "std       3.565689     8.752854     10.277406       10.360101     10.240097   \n",
            "min       1.000000     1.000000      1.000000        1.000000      1.000000   \n",
            "25%       3.000000     8.000000     10.000000        9.000000     10.000000   \n",
            "50%       7.000000    16.000000     19.000000       19.000000     19.000000   \n",
            "75%      10.000000    23.000000     28.000000       27.000000     27.000000   \n",
            "max      12.000000    31.000000     36.000000       36.000000     36.000000   \n",
            "\n",
            "       Prediction1  \n",
            "count   1407.00000  \n",
            "mean      18.77683  \n",
            "std       10.27599  \n",
            "min        1.00000  \n",
            "25%       10.00000  \n",
            "50%       19.00000  \n",
            "75%       28.00000  \n",
            "max       36.00000  \n",
            "\n",
            "[8 rows x 32 columns]\n",
            "\n",
            "First few rows of current data with 'Prediction1' initialized as NaN:\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    2         11           9         25.0          19.0   \n",
            "1  2018      8    3         19          12         23.0          31.0   \n",
            "2  2018      8    4         35          35         23.0          15.0   \n",
            "3  2018      8    6         18          16         31.0          31.0   \n",
            "4  2018      8    7         13          18         26.0          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  Prediction1  \n",
            "0            14.0          33.0          NaN  \n",
            "1             3.0          35.0          NaN  \n",
            "2             9.0          23.0          NaN  \n",
            "3            21.0          29.0          NaN  \n",
            "4            31.0          15.0          NaN  \n",
            "Row 1406 (End of Testing Data):\n",
            "Row Number             1409\n",
            "Data_Type           Testing\n",
            "Morning                  25\n",
            "Prev_Week                 5\n",
            "Rep_Prev_Week             0\n",
            "Prev_Entry                3\n",
            "Rep_Prev_Entry            0\n",
            "Mov_Avg_Mor            27.0\n",
            "Afternoon                 9\n",
            "Prev_Week.1              14\n",
            "Rep_Prev_Week.1           0\n",
            "Prev_Entry.1             25\n",
            "Rep_Prev_Entry.1          0\n",
            "Mov_Avg_Aft             8.0\n",
            "Evening                   7\n",
            "Prev_Week.2              30\n",
            "Rep_Prev_Week.2           0\n",
            "Prev_Entry.2              9\n",
            "Rep_Prev_Entry.2          0\n",
            "Mov_Avg_Eve             6.0\n",
            "Night                     5\n",
            "Prev_Week.3              22\n",
            "Rep_Prev_Week.3           0\n",
            "Prev_Entry.3              7\n",
            "Rep_Prev_Entry.3          0\n",
            "Mov_Avg_Nig             4.0\n",
            "Year                   2023\n",
            "Month                     7\n",
            "Day                      31\n",
            "Prev_Morning           29.0\n",
            "Prev_Afternoon          7.0\n",
            "Prev_Evening            5.0\n",
            "Prediction1              25\n",
            "Name: 1406, dtype: object\n",
            "   Year  Month  Day  Prev_Week  Prev_Entry  Mov_Avg_Mor  Prev_Morning  \\\n",
            "0  2018      8    2         11           9         25.0          19.0   \n",
            "1  2018      8    3         19          12         23.0          31.0   \n",
            "2  2018      8    4         35          35         23.0          15.0   \n",
            "3  2018      8    6         18          16         31.0          31.0   \n",
            "4  2018      8    7         13          18         26.0          31.0   \n",
            "\n",
            "   Prev_Afternoon  Prev_Evening  Prediction1  \n",
            "0            14.0          33.0          NaN  \n",
            "1             3.0          35.0          NaN  \n",
            "2             9.0          23.0          NaN  \n",
            "3            21.0          29.0          NaN  \n",
            "4            31.0          15.0          NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4.1: Data Preparation for Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define the preprocessing function for feature datasets\n",
        "def preprocess_features(data):\n",
        "    \"\"\"Preprocess feature data.\"\"\"\n",
        "    logger.info(\"Starting preprocessing for feature dataset\")\n",
        "\n",
        "    # Check if 'Date' column exists and convert it to datetime, then extract 'Year', 'Month', 'Day'\n",
        "    if 'Date' in data.columns:\n",
        "        data['Date'] = pd.to_datetime(data['Date'])\n",
        "        data['Year'] = data['Date'].dt.year\n",
        "        data['Month'] = data['Date'].dt.month\n",
        "        data['Day'] = data['Date'].dt.day\n",
        "        data.drop('Date', axis=1, inplace=True)  # Drop the 'Date' column after extraction\n",
        "\n",
        "    # Select relevant columns\n",
        "    selected_columns = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening']\n",
        "    data = data[selected_columns]\n",
        "\n",
        "    return data\n",
        "\n",
        "# Paths to CSV files\n",
        "paths = {\n",
        "    \"train_test_features\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/3_train_test_features.csv',\n",
        "    \"train_test_target\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/4_train_test_target.csv',\n",
        "    \"unseen_features\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/5_unseen_features.csv',\n",
        "    \"unseen_target\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/6_unseen_target.csv'\n",
        "}\n",
        "\n",
        "# Load and preprocess feature datasets\n",
        "X_train_test = preprocess_features(pd.read_csv(paths[\"train_test_features\"]))\n",
        "X_unseen = preprocess_features(pd.read_csv(paths[\"unseen_features\"]))\n",
        "\n",
        "# Load target datasets without preprocessing\n",
        "y_train_test = pd.read_csv(paths[\"train_test_target\"])\n",
        "y_unseen = pd.read_csv(paths[\"unseen_target\"])\n",
        "\n",
        "# Quick check of data structures\n",
        "logger.info(\"Train/Test Features Shape: %s, Unseen Features Shape: %s\", X_train_test.shape, X_unseen.shape)\n",
        "logger.info(\"Train/Test Target Shape: %s, Unseen Target Shape: %s\", y_train_test.shape, y_unseen.shape)\n",
        "\n",
        "# Further processing (if necessary)\n",
        "# Example: Feature scaling, handling categorical variables, etc.\n",
        "\n",
        "# Prepare for unseen data processing (to be done at a later stage)\n"
      ],
      "metadata": {
        "id": "VshK68czukkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4.1: Data Preparation for Prediction\n",
        "# ... [previous setup and preprocessing code] ...\n",
        "\n",
        "# Load and preprocess feature datasets\n",
        "X_train_test = preprocess_features(pd.read_csv(paths[\"train_test_features\"]))\n",
        "X_unseen = preprocess_features(pd.read_csv(paths[\"unseen_features\"]))\n",
        "\n",
        "# Load target datasets without preprocessing\n",
        "y_train_test = pd.read_csv(paths[\"train_test_target\"])\n",
        "y_unseen = pd.read_csv(paths[\"unseen_target\"])\n",
        "\n",
        "# Quick integrity check at the end\n",
        "print(\"\\nIntegrity Check for Train/Test Features\")\n",
        "print(\"Shape of Features:\", X_train_test.shape)\n",
        "print(\"First few rows of Features:\")\n",
        "print(X_train_test.head())\n",
        "\n",
        "print(\"\\nIntegrity Check for Train/Test Target\")\n",
        "print(\"Shape of Target:\", y_train_test.shape)\n",
        "print(\"First few rows of Target:\")\n",
        "print(y_train_test.head())\n",
        "\n",
        "print(\"\\nIntegrity Check for Unseen Features\")\n",
        "print(\"Shape of Features:\", X_unseen.shape)\n",
        "print(\"First few rows of Features:\")\n",
        "print(X_unseen.head())\n",
        "\n",
        "print(\"\\nIntegrity Check for Unseen Target\")\n",
        "print(\"Shape of Target:\", y_unseen.shape)\n",
        "print(\"First few rows of Target:\")\n",
        "print(y_unseen.head())\n"
      ],
      "metadata": {
        "id": "oZM73PaRUGKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4.2: Model Training, Prediction, and Unseen Data Loading\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train_and_evaluate_model(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train Random Forest model and evaluate it on validation data.\"\"\"\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    logger.info(\"Random Forest model trained.\")\n",
        "\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    logger.info(f\"Validation MSE: {mean_squared_error(y_val, y_val_pred)}, R2 Score: {r2_score(y_val, y_val_pred)}\")\n",
        "    return model\n",
        "\n",
        "# Paths to data\n",
        "data_paths = {\n",
        "    \"X_train\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/3_train_test_features.csv',\n",
        "    \"y_train\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/4_train_test_target.csv',\n",
        "    \"X_val\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/3_train_test_features.csv',\n",
        "    \"y_val\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/4_train_test_target.csv',\n",
        "    \"current_data\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/7_prepared_current_data_for_prediction1.csv',\n",
        "    \"original_unseen_data\": '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/5_unseen_features.csv'  # Adjust to original unseen data file\n",
        "}\n",
        "\n",
        "# Load and process data\n",
        "X_train = pd.read_csv(data_paths[\"X_train\"])\n",
        "y_train = pd.read_csv(data_paths[\"y_train\"])\n",
        "X_val = pd.read_csv(data_paths[\"X_val\"])\n",
        "y_val = pd.read_csv(data_paths[\"y_val\"])\n",
        "current_data = pd.read_csv(data_paths[\"current_data\"])\n",
        "original_unseen_data = pd.read_csv(data_paths[\"original_unseen_data\"])\n",
        "\n",
        "# Train and save model\n",
        "model = train_and_evaluate_model(X_train, y_train, X_val, y_val)\n",
        "joblib.dump(model, '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/random_forest_model.pkl')\n",
        "logger.info(\"Model saved to Google Drive.\")\n",
        "\n",
        "# Predict and save current data\n",
        "current_data['Prediction1'] = np.round(model.predict(current_data.drop('Prediction1', axis=1)))\n",
        "current_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/current_data_with_prediction1.csv', index=False)\n",
        "logger.info(\"Current data predictions saved.\")\n",
        "\n",
        "# Predict unseen data\n",
        "original_unseen_data['Prediction1'] = model.predict(original_unseen_data[selected_columns])\n",
        "\n",
        "# Save unseen data with predictions\n",
        "original_unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/Model_Unseen_Data_with_Predictions.csv', index=False)\n",
        "logger.info(\"Unseen data predictions saved.\")\n"
      ],
      "metadata": {
        "id": "oMiB868RwRYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and display the first few rows of '8_current_data_with_prediction1.csv'\n",
        "current_data_with_predictions_path = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/current_data_with_prediction1.csv'\n",
        "current_data_with_predictions = pd.read_csv(current_data_with_predictions_path)\n",
        "\n",
        "print(\"First few rows of current data with predictions ('8_current_data_with_prediction1.csv'):\")\n",
        "print(current_data_with_predictions.head())\n",
        "\n",
        "# Load and display the first few rows of '9_Model_Unseen_Data_with_Predictions.csv'\n",
        "unseen_data_with_predictions_path = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Model_Unseen_Data_with_Predictions.csv'\n",
        "unseen_data_with_predictions = pd.read_csv(unseen_data_with_predictions_path)\n",
        "\n",
        "print(\"\\nFirst few rows of unseen data with predictions ('9_Model_Unseen_Data_with_Predictions.csv'):\")\n",
        "print(unseen_data_with_predictions.head())\n"
      ],
      "metadata": {
        "id": "hvkKukSx7rVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.3: Filter Unseen Data to Match Previous Dataset Structure\n",
        "\n",
        "# Define the columns to keep\n",
        "columns_to_keep = ['Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry',\n",
        "                   'Mov_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon',\n",
        "                   'Prev_Evening', 'Prediction1']\n",
        "\n",
        "# Drop irrelevant columns\n",
        "unseen_data_filtered = unseen_data[columns_to_keep]\n",
        "\n",
        "# Print the first few rows of the filtered unseen data\n",
        "print(\"First few rows of filtered unseen data:\")\n",
        "print(unseen_data_filtered.head())\n"
      ],
      "metadata": {
        "id": "c-RtF3NUOrH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.3: Enhanced NaN Check and Handling in 'Prediction1'\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Loading current data with predictions for enhanced NaN handling...\")\n",
        "\n",
        "# Load the current data with predictions\n",
        "current_data_with_predictions = pd.read_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/current_data_with_prediction1.csv')\n",
        "\n",
        "# Forward fill NaNs in the first row\n",
        "current_data_with_predictions.iloc[0] = current_data_with_predictions.iloc[0].fillna(method='ffill')\n",
        "\n",
        "# Print the first few rows to verify NaN handling\n",
        "print(\"First few rows of current data after NaN handling:\\n\", current_data_with_predictions.head())\n",
        "\n",
        "# Check for NaN values in 'Prediction1'\n",
        "nan_count_prediction1 = current_data_with_predictions['Prediction1'].isnull().sum()\n",
        "logger.info(f\"Number of NaN values in 'Prediction1': {nan_count_prediction1}\")\n",
        "\n",
        "if nan_count_prediction1 > 0:\n",
        "    logger.warning(\"NaNs detected in 'Prediction1'. Here are the details:\")\n",
        "    nan_rows = current_data_with_predictions[current_data_with_predictions['Prediction1'].isnull()]\n",
        "    print(\"Rows with NaN in 'Prediction1':\\n\", nan_rows)\n",
        "else:\n",
        "    print(\"No NaN values found in 'Prediction1'.\")\n",
        "\n",
        "# Additional analysis or handling of NaNs can be added here if needed\n"
      ],
      "metadata": {
        "id": "lm8OBUPe0MN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4.1. Model Interpretation\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the finalized model\n",
        "final_model = joblib.load('/content/drive/My Drive/Predictive_Modeling_Four_Draws/random_forest_prediction_model.pkl')\n",
        "\n",
        "logger.info(\"Interpreting the model with SHAP values...\")\n",
        "\n",
        "# Assuming X_val is already prepared in previous cells\n",
        "# Using SHAP to interpret the model\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_val)\n",
        "shap.summary_plot(shap_values, X_val, plot_type=\"bar\")\n",
        "\n",
        "# STEP 4.2. FINAL MODEL SELECTION AND REPORTING\n",
        "\n",
        "logger.info(\"Evaluating final model performance with regression metrics...\")\n",
        "\n",
        "# Flatten y_val to ensure it's 1-dimensional\n",
        "y_val_flat = y_val.values.ravel() if isinstance(y_val, pd.DataFrame) else y_val\n",
        "\n",
        "# Generate predictions for the validation set\n",
        "y_pred_val = final_model.predict(X_val)\n",
        "y_pred_val_rounded = np.round(y_pred_val)\n",
        "\n",
        "# Flatten y_pred_val_rounded to ensure it's 1-dimensional\n",
        "y_pred_val_flat = y_pred_val_rounded.ravel() if isinstance(y_pred_val_rounded, pd.DataFrame) else y_pred_val_rounded\n",
        "\n",
        "# Create the DataFrame\n",
        "predictions_df = pd.DataFrame({'Actual': y_val_flat, 'Predicted': y_pred_val_flat})\n",
        "\n",
        "# Save the DataFrame as CSV\n",
        "predictions_df.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/predictions_df.csv', index=False)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) and R2 Score for the validation set\n",
        "mse_val = mean_squared_error(y_val_flat, y_pred_val_flat)\n",
        "r2_val = r2_score(y_val_flat, y_pred_val_flat)\n",
        "\n",
        "logger.info(f\"Validation MSE: {mse_val}, R2 Score: {r2_val}\")\n",
        "\n",
        "logger.info(f\"Regression Metrics:\\nMSE: {mse_val}\\nR2 Score: {r2_val}\")\n",
        "\n",
        "# STEP 4.3. PREPARATION FOR DEPLOYMENT\n",
        "\n",
        "# ...[Include steps for preparing the model for deployment]...\n",
        "\n",
        "# STEP 4.4. DOCUMENTATION AND REPORTING\n",
        "\n",
        "# ...[Prepare a comprehensive report on the model's performance, limitations, and deployment steps]...\n",
        "\n",
        "logger.info(\"Model documentation and reporting completed.\")\n",
        "\n",
        "# Final Checks and Tests (if applicable)\n",
        "# ...[Include any final testing or checks before deployment]...\n",
        "\n",
        "logger.info(\"Final checks and tests completed.\")\n",
        "logger.info(\"Cell 4 tasks completed successfully.\")\n"
      ],
      "metadata": {
        "id": "V28FJ39EHF8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5. Cross-Validation and  additional metrics analysis\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import joblib\n",
        "import shap\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Assuming you are using RandomForestRegressor as your model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define your scoring metrics for regression\n",
        "scoring_metrics = {\n",
        "    'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'R2': make_scorer(r2_score)\n",
        "}\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming 'data' is sorted by date and 'model' is your trained model\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)  # Train your model\n",
        "    predictions = model.predict(X_test)  # Make predictions\n",
        "\n",
        "    mse = mean_squared_error(y_test, predictions)  # Calculate MSE\n",
        "    print(f\"MSE for the current fold: {mse}\")\n",
        "\n",
        "# Given the list of MSE scores from each fold\n",
        "mse_scores = [3.4702196581196585, 1.9403478632478632, 1.051690170940171, 0.7291717948717948, 0.4926722222222223]\n",
        "\n",
        "# Calculate the average MSE\n",
        "average_mse = sum(mse_scores) / len(mse_scores)\n",
        "print(f\"Average MSE across all folds: {average_mse}\")\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "k_folds = 10  # Number of folds\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    scores = cross_val_score(model, X, y.fillna(y.mean()), scoring=scorer, cv=k_folds)\n",
        "    cv_results[metric_name] = scores\n",
        "    logger.info(f\"{metric_name} scores for each fold: {scores}\")\n",
        "    logger.info(f\"Average {metric_name} over {k_folds} folds: {np.mean(scores)}\")\n",
        "\n",
        "# Additional metrics analysis and error/bias exploration\n",
        "# ... Add your code for detailed analysis of errors, biases, etc. ...\n",
        "logger.info(\"Cross-validation and additional metrics analysis completed.\")\n",
        "\n",
        "# Feature Importance Analysis using SHAP\n",
        "# Assuming 'final_model' is your trained RandomForestRegressor model\n",
        "\n",
        "# Load the trained model (if not already loaded)\n",
        "final_model = joblib.load('/content/drive/My Drive/Predictive_Modeling_Four_Draws/random_forest_prediction_model.pkl')\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# Plot summary plot using SHAP values\n",
        "shap.summary_plot(shap_values, X)\n",
        "\n",
        "logger.info(\"Feature importance analysis using SHAP completed.\")\n"
      ],
      "metadata": {
        "id": "YupgSJuhz3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6. Detailed error and bias analysis\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ensure Google Drive is mounted\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define the path in Google Drive where the predictions DataFrame is saved\n",
        "predictions_df_path = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/predictions_df.csv'\n",
        "\n",
        "# Load or create the predictions DataFrame\n",
        "if os.path.exists(predictions_df_path):\n",
        "    predictions_df = pd.read_csv(predictions_df_path)\n",
        "else:\n",
        "    # Assuming predictions were made in a previous step and saved as 'predictions_df.csv'\n",
        "    raise FileNotFoundError(\"predictions_df.csv not found. Ensure it's created in previous steps.\")\n",
        "\n",
        "logger.info(\"Predictions DataFrame loaded successfully for error and bias analysis.\")\n",
        "print(predictions_df.columns)\n",
        "\n",
        "# Error Analysis\n",
        "predictions_df['Error'] = predictions_df['Predicted'] - predictions_df['Actual']\n",
        "predictions_df['Absolute_Error'] = predictions_df['Error'].abs()\n",
        "\n",
        "# Plotting error distribution\n",
        "plt.hist(predictions_df['Error'], bins=30)\n",
        "plt.title('Error Distribution')\n",
        "plt.xlabel('Prediction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Subgroup Analysis\n",
        "# Ensure that 'Prev_Week' and 'Prev_Entry' are in predictions_df\n",
        "if 'Prev_Week' in predictions_df and 'Prev_Entry' in predictions_df:\n",
        "    # Subgroup analysis based on 'Prev_Week'\n",
        "    prev_week_performance = predictions_df.groupby('Prev_Week').mean()['Absolute_Error']\n",
        "    prev_week_performance.plot(kind='bar', figsize=(10, 6))\n",
        "    plt.title('Performance by Previous Week')\n",
        "    plt.xlabel('Previous Week')\n",
        "    plt.ylabel('Average Absolute Error')\n",
        "    plt.show()\n",
        "\n",
        "    # Subgroup analysis based on 'Prev_Entry'\n",
        "    prev_entry_performance = predictions_df.groupby('Prev_Entry').mean()['Absolute_Error']\n",
        "    prev_entry_performance.plot(kind='bar', figsize=(10, 6))\n",
        "    plt.title('Performance by Previous Entry')\n",
        "    plt.xlabel('Previous Entry')\n",
        "    plt.ylabel('Average Absolute Error')\n",
        "    plt.show()\n",
        "\n",
        "# Document findings\n",
        "error_bias_report = \"\"\"\n",
        "Detailed Error Analysis:\n",
        "- Error Distribution Insights: {'Describe your findings from the error distribution here'}\n",
        "- Largest Errors: {'Describe characteristics of instances with largest errors here'}\n",
        "\n",
        "Bias Exploration:\n",
        "- Performance by Previous Week: {'Describe performance variations based on the previous week here'}\n",
        "- Performance by Previous Entry: {'Describe performance variations based on the previous entry here'}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Error and bias analysis completed.\")\n",
        "logger.info(error_bias_report)\n"
      ],
      "metadata": {
        "id": "mr21I98826sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7. Final review, deployment preparation, and documentation\n",
        "\n",
        "import joblib\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Final Model Review and Refinement\n",
        "# ... Code/comments for any last adjustments to the model ...\n",
        "\n",
        "# Deployment Preparation\n",
        "# Serialize the final model\n",
        "# Ensure that 'final_model' is the variable name for your trained model to be deployed\n",
        "final_model_path = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/random_forest_prediction_model.pkl'\n",
        "joblib.dump(final_model, final_model_path)\n",
        "logger.info(f\"Final model serialized and saved for deployment at: {final_model_path}\")\n",
        "\n",
        "# Comprehensive Documentation Update\n",
        "# ... Update your comprehensive report with all final findings and methodologies ...\n",
        "# Include details on model performance, SHAP interpretation, error analysis, etc.\n",
        "\n",
        "# Final Checks and Tests\n",
        "# ... Code/comments for final tests and checks to ensure model is ready for deployment ...\n",
        "\n",
        "# Planning for Future Improvements\n",
        "# Describe areas where further research could be beneficial, and methodologies to explore in future iterations of the project\n",
        "future_improvement_plan = \"\"\"\n",
        "Future Improvement Plans:\n",
        "- Areas for further research: {describe areas where additional data, feature engineering, or alternative modeling techniques could be explored}\n",
        "- Methodologies to explore: {describe potential methodologies, like deep learning or ensemble methods, for future iterations}\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Final review and deployment preparation completed.\")\n",
        "logger.info(future_improvement_plan)\n"
      ],
      "metadata": {
        "id": "qssa3uxO9tru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "u8Y77oV7koS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "WsQJRnWtkpPO"
      }
    }
  ]
}