{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME8E8kq3l0uXM8qjBIgUzJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Project2023_v3/blob/main/Data_Prep_GPT_4_Bard_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1.1: Package Installation and Library Import\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas numpy\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plnc-ffhAUCk",
        "outputId": "b2f465cd-7bf7-4eb9-b6dd-5fd56a5b204e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tzdata, numpy, pandas\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.17.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.3 pandas-2.1.4 tzdata-2023.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.2: Data Loading from Google Drive and preprocessing Training / Testing dataset\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive\n",
        "drive_dataset_directory = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/'\n",
        "\n",
        "# Define the path to the comprehensive CSV file for training and testing\n",
        "csv_filename_train_test = 'Model_Train_Test_Data.csv'\n",
        "drive_csv_path_train_test = os.path.join(drive_dataset_directory, csv_filename_train_test)\n",
        "\n",
        "# Check and load the datasets\n",
        "def load_dataset(file_path):\n",
        "    if os.path.isfile(file_path):\n",
        "        print(\"File found. Proceeding to load the dataset.\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"File not found. Check the file path or the Google Drive mount.\")\n",
        "        return None\n",
        "\n",
        "train_test_data = load_dataset(drive_csv_path_train_test)\n",
        "\n",
        "# Function to preprocess training/testing data\n",
        "def preprocess_train_test_data(data):\n",
        "    print(\"Initial data columns:\", data.columns)\n",
        "\n",
        "    # Convert 'Date' to datetime and extract 'Year', 'Month', and 'Day'\n",
        "    if 'Date' in data.columns:\n",
        "        print(\"Converting 'Date' to datetime...\")\n",
        "        data['Date'] = pd.to_datetime(data['Date'])\n",
        "        data['Year'] = data['Date'].dt.year\n",
        "        data['Month'] = data['Date'].dt.month\n",
        "        data['Day'] = data['Date'].dt.day\n",
        "        print(\"After extracting Year, Month, Day:\", data.columns)\n",
        "        data.drop(columns=['Date'], inplace=True)\n",
        "        print(\"After dropping 'Date':\", data.columns)\n",
        "    else:\n",
        "        print(\"Date column not found in the given dataset.\")\n",
        "\n",
        "    # Initialize 'Prediction1' column with 'Morning' values\n",
        "    data['Prediction1'] = data['Morning']\n",
        "\n",
        "    # Create shifted columns for previous day's data\n",
        "    data['Prev_Morning'] = data['Morning'].shift(1)\n",
        "    data['Prev_Afternoon'] = data['Afternoon'].shift(1)\n",
        "    data['Prev_Evening'] = data['Evening'].shift(1)\n",
        "\n",
        "    # Calculate moving averages excluding current row\n",
        "    initial_window_size = 3  # Increased by 1 to exclude the current row\n",
        "    columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "    target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "    for col, target_col in zip(columns_to_average, target_columns):\n",
        "    # Roll over an additional row and then shift to exclude the current row\n",
        "        data[target_col] = data[col].rolling(window=initial_window_size, min_periods=1).mean().shift(1)\n",
        "\n",
        "    # Calculate vertical average for 'Morning' excluding the current row\n",
        "    vertical_avg = data['Morning'].rolling(window=3, min_periods=1).mean().shift(1)\n",
        "    data['Vert_Avg_Mor'] = vertical_avg\n",
        "\n",
        "    # Handle NaN values\n",
        "    data['Prev_Morning'].fillna(18, inplace=True)\n",
        "    data['Prev_Afternoon'].fillna(18, inplace=True)\n",
        "    data['Prev_Evening'].fillna(18, inplace=True)\n",
        "\n",
        "    # Select relevant columns, including 'Prediction1'\n",
        "    selected_columns = ['Row Number', 'Data_Type', 'Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Vert_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prediction1']\n",
        "    data[selected_columns]\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the training/testing dataset\n",
        "train_test_data = preprocess_train_test_data(train_test_data)\n",
        "\n",
        "# Display the preprocessed data\n",
        "print(\"First few rows of preprocessed training/testing data:\")\n",
        "print(train_test_data.head())\n"
      ],
      "metadata": {
        "id": "fP_Q74gUBGQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970b2831-bd0c-4725-dbbb-1665f5b2789d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found. Proceeding to load the dataset.\n",
            "Initial data columns: Index(['Date', 'Row Number', 'Data_Type', 'Morning', 'Prev_Week',\n",
            "       'Rep_Prev_Week', 'Prev_Entry', 'Rep_Prev_Entry', 'Mov_Avg_Mor',\n",
            "       'Afternoon', 'Prev_Week.1', 'Rep_Prev_Week.1', 'Prev_Entry.1',\n",
            "       'Rep_Prev_Entry.1', 'Mov_Avg_Aft', 'Evening', 'Prev_Week.2',\n",
            "       'Rep_Prev_Week.2', 'Prev_Entry.2', 'Rep_Prev_Entry.2', 'Mov_Avg_Eve',\n",
            "       'Night', 'Prev_Week.3', 'Rep_Prev_Week.3', 'Prev_Entry.3',\n",
            "       'Rep_Prev_Entry.3', 'Mov_Avg_Nig'],\n",
            "      dtype='object')\n",
            "Converting 'Date' to datetime...\n",
            "After extracting Year, Month, Day: Index(['Date', 'Row Number', 'Data_Type', 'Morning', 'Prev_Week',\n",
            "       'Rep_Prev_Week', 'Prev_Entry', 'Rep_Prev_Entry', 'Mov_Avg_Mor',\n",
            "       'Afternoon', 'Prev_Week.1', 'Rep_Prev_Week.1', 'Prev_Entry.1',\n",
            "       'Rep_Prev_Entry.1', 'Mov_Avg_Aft', 'Evening', 'Prev_Week.2',\n",
            "       'Rep_Prev_Week.2', 'Prev_Entry.2', 'Rep_Prev_Entry.2', 'Mov_Avg_Eve',\n",
            "       'Night', 'Prev_Week.3', 'Rep_Prev_Week.3', 'Prev_Entry.3',\n",
            "       'Rep_Prev_Entry.3', 'Mov_Avg_Nig', 'Year', 'Month', 'Day'],\n",
            "      dtype='object')\n",
            "After dropping 'Date': Index(['Row Number', 'Data_Type', 'Morning', 'Prev_Week', 'Rep_Prev_Week',\n",
            "       'Prev_Entry', 'Rep_Prev_Entry', 'Mov_Avg_Mor', 'Afternoon',\n",
            "       'Prev_Week.1', 'Rep_Prev_Week.1', 'Prev_Entry.1', 'Rep_Prev_Entry.1',\n",
            "       'Mov_Avg_Aft', 'Evening', 'Prev_Week.2', 'Rep_Prev_Week.2',\n",
            "       'Prev_Entry.2', 'Rep_Prev_Entry.2', 'Mov_Avg_Eve', 'Night',\n",
            "       'Prev_Week.3', 'Rep_Prev_Week.3', 'Prev_Entry.3', 'Rep_Prev_Entry.3',\n",
            "       'Mov_Avg_Nig', 'Year', 'Month', 'Day'],\n",
            "      dtype='object')\n",
            "First few rows of preprocessed training/testing data:\n",
            "   Row Number Data_Type  Morning  Prev_Week  Rep_Prev_Week  Prev_Entry  \\\n",
            "0           1  Training       19          7              0          23   \n",
            "1           2  Training       31         11              0           9   \n",
            "2           3  Training       15         19              0          12   \n",
            "3           4  Training       31         35              0          35   \n",
            "4           5  Training       31         18              0          16   \n",
            "\n",
            "   Rep_Prev_Entry  Mov_Avg_Mor  Afternoon  Prev_Week.1  ...  Rep_Prev_Entry.3  \\\n",
            "0               0          NaN         14           13  ...                 0   \n",
            "1               0    19.000000          3           21  ...                 0   \n",
            "2               0    25.000000          9           19  ...                 0   \n",
            "3               0    21.666667         21           20  ...                 0   \n",
            "4               0    25.666667         31           30  ...                 0   \n",
            "\n",
            "   Mov_Avg_Nig  Year  Month  Day  Prediction1  Prev_Morning  Prev_Afternoon  \\\n",
            "0          NaN  2018      8    1           19          18.0            18.0   \n",
            "1     9.000000  2018      8    2           31          19.0            14.0   \n",
            "2    10.500000  2018      8    3           15          31.0             3.0   \n",
            "3    18.666667  2018      8    4           31          15.0             9.0   \n",
            "4    21.000000  2018      8    6           31          31.0            21.0   \n",
            "\n",
            "   Prev_Evening  Vert_Avg_Mor  \n",
            "0          18.0           NaN  \n",
            "1          33.0     19.000000  \n",
            "2          35.0     25.000000  \n",
            "3          23.0     21.666667  \n",
            "4          29.0     25.666667  \n",
            "\n",
            "[5 rows x 34 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.3: Data Loading from Google Drive and Preprocessing Unseen Dataset\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive\n",
        "drive_dataset_directory = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model_Docs/'\n",
        "\n",
        "# Define the path to the CSV file for unseen data\n",
        "csv_filename_unseen = 'Model_Unseen_Data.csv'\n",
        "drive_csv_path_unseen = os.path.join(drive_dataset_directory, csv_filename_unseen)\n",
        "\n",
        "# Check and load the dataset\n",
        "def load_dataset(file_path):\n",
        "    if os.path.isfile(file_path):\n",
        "        print(\"File found. Proceeding to load the dataset.\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"File not found. Check the file path or the Google Drive mount.\")\n",
        "        return None\n",
        "\n",
        "unseen_data = load_dataset(drive_csv_path_unseen)\n",
        "\n",
        "# Function to preprocess unseen data\n",
        "def preprocess_unseen_data(data):\n",
        "    print(\"Initial data columns:\", data.columns)\n",
        "\n",
        "    # Convert 'Date' to datetime and extract 'Year', 'Month', and 'Day'\n",
        "    if 'Date' in data.columns:\n",
        "        print(\"Converting 'Date' to datetime...\")\n",
        "        data['Date'] = pd.to_datetime(data['Date'])\n",
        "        data['Year'] = data['Date'].dt.year\n",
        "        data['Month'] = data['Date'].dt.month\n",
        "        data['Day'] = data['Date'].dt.day\n",
        "        print(\"After extracting Year, Month, Day:\", data.columns)\n",
        "        data.drop(columns=['Date'], inplace=True)\n",
        "        print(\"After dropping 'Date':\", data.columns)\n",
        "    else:\n",
        "        print(\"Date column not found in the given dataset.\")\n",
        "\n",
        "    # Initialize 'Prediction1' column with NaNs for unseen data\n",
        "    data['Prediction1'] = np.nan\n",
        "\n",
        "    # Create shifted columns for previous day's data\n",
        "    data['Prev_Morning'] = data['Morning'].shift(1)\n",
        "    data['Prev_Afternoon'] = data['Afternoon'].shift(1)\n",
        "    data['Prev_Evening'] = data['Evening'].shift(1)\n",
        "\n",
        "    # Calculate moving averages excluding current row\n",
        "    initial_window_size = 3  # Increased by 1 to exclude the current row\n",
        "    columns_to_average = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "    target_columns = ['Mov_Avg_Mor', 'Mov_Avg_Aft', 'Mov_Avg_Eve', 'Mov_Avg_Nig']\n",
        "\n",
        "    for col, target_col in zip(columns_to_average, target_columns):\n",
        "    # Roll over an additional row and then shift to exclude the current row\n",
        "        data[target_col] = data[col].rolling(window=initial_window_size, min_periods=1).mean().shift(1)\n",
        "    # Manually set the value for the first row\n",
        "    unseen_data.at[0, 'Mov_Avg_Mor'] = 6\n",
        "    unseen_data.at[1, 'Mov_Avg_Mor'] = 22\n",
        "    unseen_data.at[2, 'Mov_Avg_Mor'] = 17.5\n",
        "    unseen_data.at[3, 'Mov_Avg_Mor'] = 2\n",
        "    unseen_data.at[4, 'Mov_Avg_Mor'] = 17\n",
        "\n",
        "    # Calculate vertical average for 'Morning' excluding the current row\n",
        "    vertical_avg = data['Morning'].rolling(window=3, min_periods=1).mean().shift(1)\n",
        "    data['Vert_Avg_Mor'] = vertical_avg\n",
        "\n",
        "    # Handle NaN values\n",
        "    data['Prev_Morning'].fillna(25, inplace=True)\n",
        "    data['Prev_Afternoon'].fillna(9, inplace=True)\n",
        "    data['Prev_Evening'].fillna(7, inplace=True)\n",
        "\n",
        "    # Select relevant columns, including 'Prediction1'\n",
        "    selected_columns = ['Row Number', 'Data_Type', 'Year', 'Month', 'Day', 'Prev_Week', 'Prev_Entry', 'Mov_Avg_Mor', 'Vert_Avg_Mor', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prediction1']\n",
        "    data[selected_columns]\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the unseen dataset\n",
        "unseen_data = preprocess_unseen_data(unseen_data)\n",
        "\n",
        "# Display the preprocessed unseen data\n",
        "print(\"First few rows of preprocessed unseen data:\")\n",
        "print(unseen_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsY7Mvh7Jp1z",
        "outputId": "bbeb76a4-327c-4bc5-9808-098e5ed2c504"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found. Proceeding to load the dataset.\n",
            "Initial data columns: Index(['Date', 'Row Number', 'Data_Type', 'Morning', 'Prev_Week',\n",
            "       'Rep_Prev_Week', 'Prev_Entry', '2WeeksM', 'Mov_Avg_Mor', 'Afternoon',\n",
            "       'Prev_Week.1', 'Rep_Prev_Week.1', 'Prev_Entry.1', '2WeeksA',\n",
            "       'Mov_Avg_Aft', 'Evening', 'Prev_Week.2', 'Rep_Prev_Week.2',\n",
            "       'Prev_Entry.2', '2WeeksE', 'Mov_Avg_Eve', 'Night', 'Prev_Week.3',\n",
            "       'Rep_Prev_Week.3', 'Prev_Entry.3', '2WeeksN', 'Mov_Avg_Nig'],\n",
            "      dtype='object')\n",
            "Converting 'Date' to datetime...\n",
            "After extracting Year, Month, Day: Index(['Date', 'Row Number', 'Data_Type', 'Morning', 'Prev_Week',\n",
            "       'Rep_Prev_Week', 'Prev_Entry', '2WeeksM', 'Mov_Avg_Mor', 'Afternoon',\n",
            "       'Prev_Week.1', 'Rep_Prev_Week.1', 'Prev_Entry.1', '2WeeksA',\n",
            "       'Mov_Avg_Aft', 'Evening', 'Prev_Week.2', 'Rep_Prev_Week.2',\n",
            "       'Prev_Entry.2', '2WeeksE', 'Mov_Avg_Eve', 'Night', 'Prev_Week.3',\n",
            "       'Rep_Prev_Week.3', 'Prev_Entry.3', '2WeeksN', 'Mov_Avg_Nig', 'Year',\n",
            "       'Month', 'Day'],\n",
            "      dtype='object')\n",
            "After dropping 'Date': Index(['Row Number', 'Data_Type', 'Morning', 'Prev_Week', 'Rep_Prev_Week',\n",
            "       'Prev_Entry', '2WeeksM', 'Mov_Avg_Mor', 'Afternoon', 'Prev_Week.1',\n",
            "       'Rep_Prev_Week.1', 'Prev_Entry.1', '2WeeksA', 'Mov_Avg_Aft', 'Evening',\n",
            "       'Prev_Week.2', 'Rep_Prev_Week.2', 'Prev_Entry.2', '2WeeksE',\n",
            "       'Mov_Avg_Eve', 'Night', 'Prev_Week.3', 'Rep_Prev_Week.3',\n",
            "       'Prev_Entry.3', '2WeeksN', 'Mov_Avg_Nig', 'Year', 'Month', 'Day'],\n",
            "      dtype='object')\n",
            "First few rows of preprocessed unseen data:\n",
            "   Row Number Data_Type  Morning  Prev_Week  Rep_Prev_Week  Prev_Entry  \\\n",
            "0        1410    Unseen      NaN         27           26.0           5   \n",
            "1        1411    Unseen      NaN         33           22.5          18   \n",
            "2        1412    Unseen      NaN         27           15.0          28   \n",
            "3        1413    Unseen      NaN         20           15.5           2   \n",
            "4        1414    Unseen      NaN         29           21.5          12   \n",
            "\n",
            "   2WeeksM  Mov_Avg_Mor  Afternoon  Prev_Week.1  ...  2WeeksN  Mov_Avg_Nig  \\\n",
            "0      NaN          6.0         20            7  ...      NaN          NaN   \n",
            "1     12.0         22.0         31           18  ...      5.0         18.0   \n",
            "2      3.0         17.5          5           22  ...      6.0         23.0   \n",
            "3     11.0          2.0         28           29  ...      7.0         16.0   \n",
            "4     14.0         17.0         35            7  ...     18.0         14.0   \n",
            "\n",
            "   Year  Month  Day  Prediction1  Prev_Morning  Prev_Afternoon  Prev_Evening  \\\n",
            "0  2023      8    1          NaN          25.0             9.0           7.0   \n",
            "1  2023      8    2          NaN          25.0            20.0          26.0   \n",
            "2  2023      8    3          NaN          25.0            31.0           7.0   \n",
            "3  2023      8    4          NaN          25.0             5.0           2.0   \n",
            "4  2023      8    5          NaN          25.0            28.0          22.0   \n",
            "\n",
            "   Vert_Avg_Mor  \n",
            "0           NaN  \n",
            "1           NaN  \n",
            "2           NaN  \n",
            "3           NaN  \n",
            "4           NaN  \n",
            "\n",
            "[5 rows x 34 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the preprocessed training/testing dataset\n",
        "preprocessed_train_test_path = os.path.join(drive_dataset_directory, '15_preprocessed_train_test_data.csv')\n",
        "train_test_data.to_csv(preprocessed_train_test_path, index=False)\n",
        "print(\"Preprocessed training/testing data saved to Google Drive.\")\n",
        "\n",
        "# Save the preprocessed unseen dataset\n",
        "preprocessed_unseen_path = os.path.join(drive_dataset_directory, '16_preprocessed_unseen_data.csv')\n",
        "unseen_data.to_csv(preprocessed_unseen_path, index=False)\n",
        "print(\"Preprocessed unseen data saved to Google Drive.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncZjXqqlHgvm",
        "outputId": "5c1f5247-1cee-4bcc-92e9-7864df892004"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed training/testing data saved to Google Drive.\n",
            "Preprocessed unseen data saved to Google Drive.\n"
          ]
        }
      ]
    }
  ]
}