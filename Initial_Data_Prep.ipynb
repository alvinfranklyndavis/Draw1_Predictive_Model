{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5Q9j4Ybi//VsRhxbqxSXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Draw1_Predictive_Model/blob/main/Initial_Data_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1.1: Package Installation and Library Import\n",
        "\n",
        "# Check for existing libraries\n",
        "!pip show pandas numpy\n",
        "\n",
        "# Install or upgrade required packages\n",
        "!pip install -U --upgrade-strategy eager pip\n",
        "!pip install -U --upgrade-strategy eager pandas==<desired_version> numpy==<desired_version>\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Set up logging to save logs in a file\n",
        "log_file = 'project.log'\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set up virtual environment (optional but recommended)\n",
        "# You can create a virtual environment with: !python -m venv myenv\n",
        "# And activate it with: source myenv/bin/activate (Linux/macOS) or myenv\\Scripts\\activate (Windows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plnc-ffhAUCk",
        "outputId": "dc1f5def-2ad4-4fc4-c94f-bf80728483a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pandas\n",
            "Version: 1.5.3\n",
            "Summary: Powerful data structures for data analysis, time series, and statistics\n",
            "Home-page: https://pandas.pydata.org\n",
            "Author: The Pandas Development Team\n",
            "Author-email: pandas-dev@python.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, python-dateutil, pytz\n",
            "Required-by: altair, arviz, bigframes, bokeh, bqplot, cmdstanpy, cufflinks, datascience, db-dtypes, dopamine-rl, fastai, geemap, geopandas, google-colab, gspread-dataframe, holoviews, ibis-framework, mizani, mlxtend, pandas-datareader, pandas-gbq, panel, plotnine, prophet, pymc, seaborn, sklearn-pandas, statsmodels, vega-datasets, xarray, yfinance\n",
            "---\n",
            "Name: numpy\n",
            "Version: 1.25.2\n",
            "Summary: Fundamental package for array computing in Python\n",
            "Home-page: https://www.numpy.org\n",
            "Author: Travis E. Oliphant et al.\n",
            "Author-email: \n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: albumentations, altair, arviz, astropy, autograd, blis, bokeh, bqplot, chex, cmdstanpy, contourpy, cufflinks, cupy-cuda12x, cvxpy, datascience, db-dtypes, dopamine-rl, ecos, flax, folium, geemap, gensim, gym, h5py, holoviews, hyperopt, ibis-framework, imageio, imbalanced-learn, imgaug, jax, jaxlib, librosa, lightgbm, matplotlib, matplotlib-venn, missingno, mizani, ml-dtypes, mlxtend, moviepy, music21, nibabel, numba, numexpr, opencv-contrib-python, opencv-python, opencv-python-headless, opt-einsum, optax, orbax-checkpoint, osqp, pandas, pandas-gbq, patsy, plotnine, prophet, pyarrow, pycocotools, pyerfa, pymc, pytensor, python-louvain, PyWavelets, qdldl, qudida, scikit-image, scikit-learn, scipy, scs, seaborn, shapely, sklearn-pandas, soxr, spacy, stanio, statsmodels, tables, tensorboard, tensorflow, tensorflow-datasets, tensorflow-hub, tensorflow-probability, tensorstore, thinc, tifffile, torchtext, torchvision, transformers, wordcloud, xarray, xarray-einstats, xgboost, yellowbrick, yfinance\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 1: `pip install -U --upgrade-strategy eager pandas==<desired_version> numpy==<desired_version>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.2: Data Loading from Google Drive Training / Testing  and Unseen datasets\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory for datasets in Google Drive\n",
        "drive_dataset_directory = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Define the paths to the CSV files\n",
        "csv_filename_train_test = 'A_Initial_Train_Test_Data.csv'\n",
        "csv_filename_unseen = 'B_Initial_Unseen_Data.csv'\n",
        "\n",
        "drive_csv_path_train_test = os.path.join(drive_dataset_directory, csv_filename_train_test)\n",
        "drive_csv_path_unseen = os.path.join(drive_dataset_directory, csv_filename_unseen)\n",
        "\n",
        "# Check and load the datasets\n",
        "def load_dataset(file_path):\n",
        "    if os.path.isfile(file_path):\n",
        "        print(\"File found. Proceeding to load the dataset.\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"File not found. Check the file path or the Google Drive mount.\")\n",
        "        return None\n",
        "\n",
        "# Load training/testing data\n",
        "train_test_data = load_dataset(drive_csv_path_train_test)\n",
        "\n",
        "# Load unseen data\n",
        "unseen_data = load_dataset(drive_csv_path_unseen)\n",
        "\n",
        "# Print the first few rows of both datasets for inspection\n",
        "print(\"First few rows of training/testing data:\")\n",
        "print(train_test_data.head())\n",
        "\n",
        "print(\"\\nFirst few rows of unseen data:\")\n",
        "print(unseen_data.head())\n"
      ],
      "metadata": {
        "id": "fP_Q74gUBGQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b74dfc7-2340-44d8-cab5-7a8e936ff0e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found. Proceeding to load the dataset.\n",
            "File found. Proceeding to load the dataset.\n",
            "First few rows of training/testing data:\n",
            "       Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  01-08-18           1  Training   19.0            7.0        27.0   \n",
            "1  02-08-18           2  Training   31.0           11.0         1.0   \n",
            "2  03-08-18           3  Training   15.0           19.0        21.0   \n",
            "3  04-08-18           4  Training   31.0           35.0        18.0   \n",
            "4  05-08-18           5       NaN    NaN            NaN         NaN   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0            23.0              32.0         27.5          17.0  ...   \n",
            "1             9.0              33.0         21.0           6.0  ...   \n",
            "2            12.0              35.0         23.5          20.0  ...   \n",
            "3            35.0              23.0         29.0          26.5  ...   \n",
            "4             NaN               NaN          NaN           NaN  ...   \n",
            "\n",
            "   DR3_Prev_Entry-2  DR3_Mov_Avg  DR3_Vert_Avg  Draw4  DR4_Prev_Week  \\\n",
            "0              19.0         16.5          15.0    9.0            2.0   \n",
            "1              31.0         17.0          17.0   12.0           35.0   \n",
            "2              15.0         12.0          10.0   35.0           11.0   \n",
            "3              31.0         26.0          23.5   16.0           13.0   \n",
            "4               NaN          NaN           NaN    NaN            NaN   \n",
            "\n",
            "   DR4_2Weeks  DR4_Prev_Entry  DR4_Prev_Entry-2  DR4_Mov_Avg  DR4_Vert_Avg  \n",
            "0        24.0            33.0              14.0         23.5          13.0  \n",
            "1        26.0            35.0               3.0         19.0          30.5  \n",
            "2        29.0            23.0               9.0         16.0          20.0  \n",
            "3        17.0            29.0              21.0         25.0          15.0  \n",
            "4         NaN             NaN               NaN          NaN           NaN  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "First few rows of unseen data:\n",
            "       Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  1/8/2023        1673    Unseen   13.0           27.0        25.0   \n",
            "1  2/8/2023        1674    Unseen   21.0           33.0        12.0   \n",
            "2  3/8/2023        1675    Unseen   15.0           27.0         3.0   \n",
            "3  4/8/2023        1676    Unseen   13.0           20.0        11.0   \n",
            "4  5/8/2023        1677    Unseen   12.0           29.0        14.0   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0             5.0               7.0          6.0          26.0  ...   \n",
            "1            18.0              26.0         22.0          22.5  ...   \n",
            "2            28.0               7.0         17.5          15.0  ...   \n",
            "3             2.0               2.0          2.0          15.5  ...   \n",
            "4            12.0              22.0         17.0          21.5  ...   \n",
            "\n",
            "   DR3_Prev_Entry-2  DR3_Mov_Avg  DR3_Vert_Avg  Draw4  DR4_Prev_Week  \\\n",
            "0              13.0         16.5          25.0   18.0           26.0   \n",
            "1              21.0         26.0           6.0   28.0            8.0   \n",
            "2              15.0         10.0          18.0    2.0           30.0   \n",
            "3              13.0         20.5          26.0   12.0            2.0   \n",
            "4              12.0         23.5          18.5   11.0            3.0   \n",
            "\n",
            "   DR4_2Weeks  DR4_Prev_Entry  DR4_Prev_Entry-2  DR4_Mov_Avg  DR4_Vert_Avg  \n",
            "0         3.0            26.0              20.0         23.0          14.5  \n",
            "1         5.0             7.0              31.0         19.0           6.5  \n",
            "2         6.0             2.0               5.0          3.5          18.0  \n",
            "3         7.0            22.0              28.0         25.0           4.5  \n",
            "4        18.0            31.0              35.0         33.0          10.5  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.3: Surveillance Check for NaNs within both datasets\n",
        "\n",
        "# Check for NaN values in training/testing data\n",
        "print(\"NaN check for training/testing data:\")\n",
        "print(train_test_data.isna().sum())\n",
        "\n",
        "# Check for NaN values in unseen data\n",
        "print(\"\\nNaN check for unseen data:\")\n",
        "print(unseen_data.isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2THl1IOP57A7",
        "outputId": "25103cbc-7026-4846-fd0e-b55ee6f68710"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN check for training/testing data:\n",
            "Date                  0\n",
            "Row Number            0\n",
            "Data_Type           239\n",
            "Draw1               239\n",
            "DR1_Prev_Week       239\n",
            "DR1_2Weeks          239\n",
            "DR1_Prev_Entry      239\n",
            "DR1_Prev_Entry-2    239\n",
            "DR1_Mov_Avg         239\n",
            "DR1_Vert_Avg        239\n",
            "Draw2               239\n",
            "DR2_Prev_Week       239\n",
            "DR2_2Weeks          239\n",
            "DR2_Prev_Entry      239\n",
            "DR2_Prev_Entry-2    239\n",
            "DR2_Mov_Avg         239\n",
            "DR2_Vert_Avg        239\n",
            "Draw3               239\n",
            "DR3_Prev_Week       239\n",
            "DR3_2Weeks          239\n",
            "DR3_Prev_Entry      239\n",
            "DR3_Prev_Entry-2    239\n",
            "DR3_Mov_Avg         239\n",
            "DR3_Vert_Avg        239\n",
            "Draw4               239\n",
            "DR4_Prev_Week       239\n",
            "DR4_2Weeks          239\n",
            "DR4_Prev_Entry      239\n",
            "DR4_Prev_Entry-2    239\n",
            "DR4_Mov_Avg         239\n",
            "DR4_Vert_Avg        239\n",
            "dtype: int64\n",
            "\n",
            "NaN check for unseen data:\n",
            "Date                 0\n",
            "Row Number           0\n",
            "Data_Type           17\n",
            "Draw1               17\n",
            "DR1_Prev_Week       17\n",
            "DR1_2Weeks          17\n",
            "DR1_Prev_Entry      17\n",
            "DR1_Prev_Entry-2    17\n",
            "DR1_Mov_Avg         17\n",
            "DR1_Vert_Avg        17\n",
            "Draw2               17\n",
            "DR2_Prev_Week       17\n",
            "DR2_2Weeks          17\n",
            "DR2_Prev_Entry      17\n",
            "DR2_Prev_Entry-2    17\n",
            "DR2_Mov_Avg         17\n",
            "DR2_Vert_Avg        17\n",
            "Draw3               17\n",
            "DR3_Prev_Week       17\n",
            "DR3_2Weeks          17\n",
            "DR3_Prev_Entry      17\n",
            "DR3_Prev_Entry-2    17\n",
            "DR3_Mov_Avg         17\n",
            "DR3_Vert_Avg        17\n",
            "Draw4               17\n",
            "DR4_Prev_Week       17\n",
            "DR4_2Weeks          17\n",
            "DR4_Prev_Entry      17\n",
            "DR4_Prev_Entry-2    17\n",
            "DR4_Mov_Avg         17\n",
            "DR4_Vert_Avg        17\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.1: NaN handling and new CSV saving for Training / Testing  and Unseen datasets\n",
        "\n",
        "\n",
        "# Impute NaN values with zeros in training/testing data\n",
        "train_test_data = train_test_data.fillna(0)\n",
        "\n",
        "# Impute NaN values with zeros in unseen data\n",
        "unseen_data = unseen_data.fillna(0)\n",
        "\n",
        "# Define new CSV file names\n",
        "new_csv_filename_train_test = 'C_NaN_Handled_Train_Test_Data.csv'\n",
        "new_csv_filename_unseen = 'C_NaN_Handled_Unseen_Data.csv'\n",
        "\n",
        "# Define the paths for saving the new CSV files\n",
        "new_csv_path_train_test = os.path.join(drive_dataset_directory, new_csv_filename_train_test)\n",
        "new_csv_path_unseen = os.path.join(drive_dataset_directory, new_csv_filename_unseen)\n",
        "\n",
        "# Save the preprocessed training/testing data as a new CSV file\n",
        "train_test_data.to_csv(new_csv_path_train_test, index=False)\n",
        "\n",
        "# Save the preprocessed unseen data as a new CSV file\n",
        "unseen_data.to_csv(new_csv_path_unseen, index=False)\n",
        "\n",
        "# Print a message to confirm that the preprocessing and saving is complete\n",
        "print(\"Preprocessing and saving of datasets is complete.\")\n",
        "\n",
        "# Check for NaN values in the preprocessed training/testing data\n",
        "print(\"\\nNaN check for preprocessed training/testing data:\")\n",
        "print(train_test_data.isna().sum())\n",
        "\n",
        "# Check for NaN values in the preprocessed unseen data\n",
        "print(\"\\nNaN check for preprocessed unseen data:\")\n",
        "print(unseen_data.isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7QU7JyA8B8b",
        "outputId": "3fc18361-1641-4e60-de7f-5d4b89dee8c5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and saving of datasets is complete.\n",
            "\n",
            "NaN check for preprocessed training/testing data:\n",
            "Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "dtype: int64\n",
            "\n",
            "NaN check for preprocessed unseen data:\n",
            "Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.2: Extract Y/M/D from Date and new CSV saving for Training / Testing  and Unseen datasets\n",
        "\n",
        "# Load the NaN-handled training/testing data\n",
        "nan_handled_train_test_data = load_dataset(new_csv_path_train_test)\n",
        "\n",
        "# Load the NaN-handled unseen data\n",
        "nan_handled_unseen_data = load_dataset(new_csv_path_unseen)\n",
        "\n",
        "# Check and load the datasets\n",
        "def load_dataset(file_path):\n",
        "    if os.path.isfile(file_path):\n",
        "        print(\"File found. Proceeding to load the dataset.\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"File not found. Check the file path.\")\n",
        "        return None\n",
        "\n",
        "# Function to extract 'Year', 'Month', and 'Day' from the 'Date' column\n",
        "def extract_date_features(data):\n",
        "    if 'Date' in data.columns:\n",
        "        print(\"Converting 'Date' to datetime and extracting Year, Month, and Day...\")\n",
        "        date_formats = ['%d-%m-%y', '%d/%m/%Y']\n",
        "        for date_format in date_formats:\n",
        "            try:\n",
        "                data['Date'] = pd.to_datetime(data['Date'], format=date_format)\n",
        "                data['Year'] = data['Date'].dt.year.fillna(0).astype(int)\n",
        "                data['Month'] = data['Date'].dt.month.fillna(0).astype(int)\n",
        "                data['Day'] = data['Date'].dt.day.fillna(0).astype(int)\n",
        "                print(\"After extracting Year, Month, and Day:\", data.columns)\n",
        "                break  # Break the loop if successful date conversion\n",
        "            except ValueError:\n",
        "                print(f\"Failed to convert 'Date' with format: {date_format}\")\n",
        "    else:\n",
        "        print(\"'Date' column not found in the dataset.\")\n",
        "\n",
        "# Extract 'Year', 'Month', and 'Day' from the 'Date' column in training/testing data\n",
        "extract_date_features(nan_handled_train_test_data)\n",
        "\n",
        "# Extract 'Year', 'Month', and 'Day' from the 'Date' column in unseen data\n",
        "extract_date_features(nan_handled_unseen_data)\n",
        "\n",
        "# Define new CSV file names\n",
        "new_csv_filename_train_test_date = 'D_Date_Extracted_Train_Test_Data.csv'\n",
        "new_csv_filename_unseen_date = 'D_Date_Extracted_Unseen_Data.csv'\n",
        "\n",
        "# Define the paths for saving the new CSV files\n",
        "new_csv_path_train_test_date = os.path.join(drive_dataset_directory, new_csv_filename_train_test_date)\n",
        "new_csv_path_unseen_date = os.path.join(drive_dataset_directory, new_csv_filename_unseen_date)\n",
        "\n",
        "# Save the datasets with extracted date features as new CSV files\n",
        "nan_handled_train_test_data.to_csv(new_csv_path_train_test_date, index=False)\n",
        "nan_handled_unseen_data.to_csv(new_csv_path_unseen_date, index=False)\n",
        "\n",
        "# Print a message to confirm that the date extraction and saving is complete\n",
        "print(\"Date extraction and saving of datasets is complete.\")\n",
        "\n",
        "# Check for NaN values in the datasets with extracted date features\n",
        "print(\"\\nNaN check for training/testing data with extracted date features:\")\n",
        "print(nan_handled_train_test_data.isna().sum())\n",
        "\n",
        "print(\"\\nNaN check for unseen data with extracted date features:\")\n",
        "print(nan_handled_unseen_data.isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEbg8-qg8NiZ",
        "outputId": "e3a2c102-1695-4234-ce15-9f89ea08aab6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found. Proceeding to load the dataset.\n",
            "File found. Proceeding to load the dataset.\n",
            "Converting 'Date' to datetime and extracting Year, Month, and Day...\n",
            "After extracting Year, Month, and Day: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Vert_Avg', 'Draw2', 'DR2_Prev_Week', 'DR2_2Weeks',\n",
            "       'DR2_Prev_Entry', 'DR2_Prev_Entry-2', 'DR2_Mov_Avg', 'DR2_Vert_Avg',\n",
            "       'Draw3', 'DR3_Prev_Week', 'DR3_2Weeks', 'DR3_Prev_Entry',\n",
            "       'DR3_Prev_Entry-2', 'DR3_Mov_Avg', 'DR3_Vert_Avg', 'Draw4',\n",
            "       'DR4_Prev_Week', 'DR4_2Weeks', 'DR4_Prev_Entry', 'DR4_Prev_Entry-2',\n",
            "       'DR4_Mov_Avg', 'DR4_Vert_Avg', 'Year', 'Month', 'Day'],\n",
            "      dtype='object')\n",
            "Converting 'Date' to datetime and extracting Year, Month, and Day...\n",
            "Failed to convert 'Date' with format: %d-%m-%y\n",
            "After extracting Year, Month, and Day: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Vert_Avg', 'Draw2', 'DR2_Prev_Week', 'DR2_2Weeks',\n",
            "       'DR2_Prev_Entry', 'DR2_Prev_Entry-2', 'DR2_Mov_Avg', 'DR2_Vert_Avg',\n",
            "       'Draw3', 'DR3_Prev_Week', 'DR3_2Weeks', 'DR3_Prev_Entry',\n",
            "       'DR3_Prev_Entry-2', 'DR3_Mov_Avg', 'DR3_Vert_Avg', 'Draw4',\n",
            "       'DR4_Prev_Week', 'DR4_2Weeks', 'DR4_Prev_Entry', 'DR4_Prev_Entry-2',\n",
            "       'DR4_Mov_Avg', 'DR4_Vert_Avg', 'Year', 'Month', 'Day'],\n",
            "      dtype='object')\n",
            "Date extraction and saving of datasets is complete.\n",
            "\n",
            "NaN check for training/testing data with extracted date features:\n",
            "Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "dtype: int64\n",
            "\n",
            "NaN check for unseen data with extracted date features:\n",
            "Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.3: Create shifted columns for previous day's data\n",
        "\n",
        "# Function to create shifted columns for previous day's data\n",
        "def create_shifted_columns(data):\n",
        "    data['Prev_Morning'] = data['Draw1'].shift(1)\n",
        "    data['Prev_Afternoon'] = data['Draw2'].shift(1)\n",
        "    data['Prev_Evening'] = data['Draw3'].shift(1)\n",
        "    data['Prev_Night'] = data['Draw4'].shift(1)\n",
        "    data[['Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night']] = data[['Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night']].fillna(0).astype(int)\n",
        "\n",
        "# Load the date extracted training/testing data\n",
        "date_extracted_train_test_data = load_dataset('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/D_Date_Extracted_Train_Test_Data.csv')\n",
        "\n",
        "# Apply the function to create shifted columns\n",
        "create_shifted_columns(date_extracted_train_test_data)\n",
        "\n",
        "# Save the updated training/testing data with shifted columns\n",
        "date_extracted_train_test_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/E_Shifted_Train_Test_Data.csv', index=False)\n",
        "\n",
        "# Load the date extracted unseen data\n",
        "date_extracted_unseen_data = load_dataset('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/D_Date_Extracted_Unseen_Data.csv')\n",
        "\n",
        "# Apply the function to create shifted columns\n",
        "create_shifted_columns(date_extracted_unseen_data)\n",
        "\n",
        "# Save the updated unseen data with shifted columns\n",
        "date_extracted_unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/E_Shifted_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Print the first few rows of both datasets for inspection\n",
        "print(\"First few rows of date extracted training/testing data:\")\n",
        "print(date_extracted_train_test_data.head())\n",
        "\n",
        "print(\"\\nFirst few rows of date extracted unseen data:\")\n",
        "print(date_extracted_unseen_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YbHzP7wBUY1",
        "outputId": "d1ee972e-7bed-4a76-b16b-09fe5eb2abb4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found. Proceeding to load the dataset.\n",
            "File found. Proceeding to load the dataset.\n",
            "First few rows of date extracted training/testing data:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2018-08-01           1  Training   19.0            7.0        27.0   \n",
            "1  2018-08-02           2  Training   31.0           11.0         1.0   \n",
            "2  2018-08-03           3  Training   15.0           19.0        21.0   \n",
            "3  2018-08-04           4  Training   31.0           35.0        18.0   \n",
            "4  2018-08-05           5         0    0.0            0.0         0.0   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0            23.0              32.0         27.5          17.0  ...   \n",
            "1             9.0              33.0         21.0           6.0  ...   \n",
            "2            12.0              35.0         23.5          20.0  ...   \n",
            "3            35.0              23.0         29.0          26.5  ...   \n",
            "4             0.0               0.0          0.0           0.0  ...   \n",
            "\n",
            "   DR4_Prev_Entry-2  DR4_Mov_Avg  DR4_Vert_Avg  Year  Month  Day  \\\n",
            "0              14.0         23.5          13.0  2018      8    1   \n",
            "1               3.0         19.0          30.5  2018      8    2   \n",
            "2               9.0         16.0          20.0  2018      8    3   \n",
            "3              21.0         25.0          15.0  2018      8    4   \n",
            "4               0.0          0.0           0.0  2018      8    5   \n",
            "\n",
            "   Prev_Morning  Prev_Afternoon  Prev_Evening  Prev_Night  \n",
            "0             0               0             0           0  \n",
            "1            19              14            33           9  \n",
            "2            31               3            35          12  \n",
            "3            15               9            23          35  \n",
            "4            31              21            29          16  \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "\n",
            "First few rows of date extracted unseen data:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2023-08-01        1673    Unseen   13.0           27.0        25.0   \n",
            "1  2023-08-02        1674    Unseen   21.0           33.0        12.0   \n",
            "2  2023-08-03        1675    Unseen   15.0           27.0         3.0   \n",
            "3  2023-08-04        1676    Unseen   13.0           20.0        11.0   \n",
            "4  2023-08-05        1677    Unseen   12.0           29.0        14.0   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0             5.0               7.0          6.0          26.0  ...   \n",
            "1            18.0              26.0         22.0          22.5  ...   \n",
            "2            28.0               7.0         17.5          15.0  ...   \n",
            "3             2.0               2.0          2.0          15.5  ...   \n",
            "4            12.0              22.0         17.0          21.5  ...   \n",
            "\n",
            "   DR4_Prev_Entry-2  DR4_Mov_Avg  DR4_Vert_Avg  Year  Month  Day  \\\n",
            "0              20.0         23.0          14.5  2023      8    1   \n",
            "1              31.0         19.0           6.5  2023      8    2   \n",
            "2               5.0          3.5          18.0  2023      8    3   \n",
            "3              28.0         25.0           4.5  2023      8    4   \n",
            "4              35.0         33.0          10.5  2023      8    5   \n",
            "\n",
            "   Prev_Morning  Prev_Afternoon  Prev_Evening  Prev_Night  \n",
            "0             0               0             0           0  \n",
            "1            13              20            26          18  \n",
            "2            21              31             7          28  \n",
            "3            15               5             2           2  \n",
            "4            13              28            22          12  \n",
            "\n",
            "[5 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.4: Handle NaN values for previous day's data\n",
        "\n",
        "# Load the shifted training/testing data\n",
        "shifted_train_test_data = load_dataset('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/E_Shifted_Train_Test_Data.csv')\n",
        "\n",
        "# Manually set values for the first row of training/testing set\n",
        "shifted_train_test_data.at[0, 'Prev_Morning'] = 13\n",
        "shifted_train_test_data.at[0, 'Prev_Afternoon'] = 34\n",
        "shifted_train_test_data.at[0, 'Prev_Evening'] = 32\n",
        "shifted_train_test_data.at[0, 'Prev_Night'] = 23\n",
        "\n",
        "# Save the updated training/testing data\n",
        "shifted_train_test_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/F_NaN_Handled_Shifted_Train_Test_Data.csv', index=False)\n",
        "\n",
        "# Load the shifted unseen data\n",
        "shifted_unseen_data = load_dataset('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/E_Shifted_Unseen_Data.csv')\n",
        "\n",
        "# Manually set values for the first row of unseen set\n",
        "shifted_unseen_data.at[0, 'Prev_Morning'] = 25\n",
        "shifted_unseen_data.at[0, 'Prev_Afternoon'] = 9\n",
        "shifted_unseen_data.at[0, 'Prev_Evening'] = 7\n",
        "shifted_unseen_data.at[0, 'Prev_Night'] = 5\n",
        "\n",
        "# Save the updated unseen data\n",
        "shifted_unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/F_NaN_Handled_Shifted_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Print the first few rows of both datasets for inspection\n",
        "print(\"First few rows of handled shifted training/testing data:\")\n",
        "print(shifted_train_test_data.head())\n",
        "\n",
        "print(\"\\nFirst few rows of handled shifted unseen data:\")\n",
        "print(shifted_unseen_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8U-4sqFGQXj",
        "outputId": "8a5af329-f3b4-4874-e880-a17783cc887a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found. Proceeding to load the dataset.\n",
            "File found. Proceeding to load the dataset.\n",
            "First few rows of handled shifted training/testing data:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2018-08-01           1  Training   19.0            7.0        27.0   \n",
            "1  2018-08-02           2  Training   31.0           11.0         1.0   \n",
            "2  2018-08-03           3  Training   15.0           19.0        21.0   \n",
            "3  2018-08-04           4  Training   31.0           35.0        18.0   \n",
            "4  2018-08-05           5         0    0.0            0.0         0.0   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0            23.0              32.0         27.5          17.0  ...   \n",
            "1             9.0              33.0         21.0           6.0  ...   \n",
            "2            12.0              35.0         23.5          20.0  ...   \n",
            "3            35.0              23.0         29.0          26.5  ...   \n",
            "4             0.0               0.0          0.0           0.0  ...   \n",
            "\n",
            "   DR4_Prev_Entry-2  DR4_Mov_Avg  DR4_Vert_Avg  Year  Month  Day  \\\n",
            "0              14.0         23.5          13.0  2018      8    1   \n",
            "1               3.0         19.0          30.5  2018      8    2   \n",
            "2               9.0         16.0          20.0  2018      8    3   \n",
            "3              21.0         25.0          15.0  2018      8    4   \n",
            "4               0.0          0.0           0.0  2018      8    5   \n",
            "\n",
            "   Prev_Morning  Prev_Afternoon  Prev_Evening  Prev_Night  \n",
            "0            13              34            32          23  \n",
            "1            19              14            33           9  \n",
            "2            31               3            35          12  \n",
            "3            15               9            23          35  \n",
            "4            31              21            29          16  \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "\n",
            "First few rows of handled shifted unseen data:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2023-08-01        1673    Unseen   13.0           27.0        25.0   \n",
            "1  2023-08-02        1674    Unseen   21.0           33.0        12.0   \n",
            "2  2023-08-03        1675    Unseen   15.0           27.0         3.0   \n",
            "3  2023-08-04        1676    Unseen   13.0           20.0        11.0   \n",
            "4  2023-08-05        1677    Unseen   12.0           29.0        14.0   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0             5.0               7.0          6.0          26.0  ...   \n",
            "1            18.0              26.0         22.0          22.5  ...   \n",
            "2            28.0               7.0         17.5          15.0  ...   \n",
            "3             2.0               2.0          2.0          15.5  ...   \n",
            "4            12.0              22.0         17.0          21.5  ...   \n",
            "\n",
            "   DR4_Prev_Entry-2  DR4_Mov_Avg  DR4_Vert_Avg  Year  Month  Day  \\\n",
            "0              20.0         23.0          14.5  2023      8    1   \n",
            "1              31.0         19.0           6.5  2023      8    2   \n",
            "2               5.0          3.5          18.0  2023      8    3   \n",
            "3              28.0         25.0           4.5  2023      8    4   \n",
            "4              35.0         33.0          10.5  2023      8    5   \n",
            "\n",
            "   Prev_Morning  Prev_Afternoon  Prev_Evening  Prev_Night  \n",
            "0            25               9             7           5  \n",
            "1            13              20            26          18  \n",
            "2            21              31             7          28  \n",
            "3            15               5             2           2  \n",
            "4            13              28            22          12  \n",
            "\n",
            "[5 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.1: Converting the columns to integer in both datasets (excluding 'Date')\n",
        "\n",
        "# Load the most recent CSVs\n",
        "train_test_data = pd.read_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/F_NaN_Handled_Shifted_Train_Test_Data.csv')\n",
        "unseen_data = pd.read_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/F_NaN_Handled_Shifted_Unseen_Data.csv')\n",
        "\n",
        "# List of columns to convert to integer (excluding 'Data_Type' and 'Date')\n",
        "columns_to_convert_train_test = [col for col in train_test_data.columns if col not in ['Data_Type', 'Date']]\n",
        "columns_to_convert_unseen = [col for col in unseen_data.columns if col not in ['Data_Type', 'Date']]\n",
        "\n",
        "# Convert columns to integer\n",
        "train_test_data[columns_to_convert_train_test] = train_test_data[columns_to_convert_train_test].astype(int)\n",
        "unseen_data[columns_to_convert_unseen] = unseen_data[columns_to_convert_unseen].astype(int)\n",
        "\n",
        "# Save the updated datasets\n",
        "train_test_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/G_Handled_DataType_Train_Test_Data.csv', index=False)\n",
        "unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/G_Handled_DataType_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Display the data types of the columns after conversion\n",
        "print(\"Data types of columns in train/test data after conversion:\")\n",
        "print(train_test_data.dtypes)\n",
        "\n",
        "print(\"\\nData types of columns in unseen data after conversion:\")\n",
        "print(unseen_data.dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adGQ6YMGOsYv",
        "outputId": "bff91ebe-5a23-4df8-89d3-39e73313708d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types of columns in train/test data after conversion:\n",
            "Date                object\n",
            "Row Number           int64\n",
            "Data_Type           object\n",
            "Draw1                int64\n",
            "DR1_Prev_Week        int64\n",
            "DR1_2Weeks           int64\n",
            "DR1_Prev_Entry       int64\n",
            "DR1_Prev_Entry-2     int64\n",
            "DR1_Mov_Avg          int64\n",
            "DR1_Vert_Avg         int64\n",
            "Draw2                int64\n",
            "DR2_Prev_Week        int64\n",
            "DR2_2Weeks           int64\n",
            "DR2_Prev_Entry       int64\n",
            "DR2_Prev_Entry-2     int64\n",
            "DR2_Mov_Avg          int64\n",
            "DR2_Vert_Avg         int64\n",
            "Draw3                int64\n",
            "DR3_Prev_Week        int64\n",
            "DR3_2Weeks           int64\n",
            "DR3_Prev_Entry       int64\n",
            "DR3_Prev_Entry-2     int64\n",
            "DR3_Mov_Avg          int64\n",
            "DR3_Vert_Avg         int64\n",
            "Draw4                int64\n",
            "DR4_Prev_Week        int64\n",
            "DR4_2Weeks           int64\n",
            "DR4_Prev_Entry       int64\n",
            "DR4_Prev_Entry-2     int64\n",
            "DR4_Mov_Avg          int64\n",
            "DR4_Vert_Avg         int64\n",
            "Year                 int64\n",
            "Month                int64\n",
            "Day                  int64\n",
            "Prev_Morning         int64\n",
            "Prev_Afternoon       int64\n",
            "Prev_Evening         int64\n",
            "Prev_Night           int64\n",
            "dtype: object\n",
            "\n",
            "Data types of columns in unseen data after conversion:\n",
            "Date                object\n",
            "Row Number           int64\n",
            "Data_Type           object\n",
            "Draw1                int64\n",
            "DR1_Prev_Week        int64\n",
            "DR1_2Weeks           int64\n",
            "DR1_Prev_Entry       int64\n",
            "DR1_Prev_Entry-2     int64\n",
            "DR1_Mov_Avg          int64\n",
            "DR1_Vert_Avg         int64\n",
            "Draw2                int64\n",
            "DR2_Prev_Week        int64\n",
            "DR2_2Weeks           int64\n",
            "DR2_Prev_Entry       int64\n",
            "DR2_Prev_Entry-2     int64\n",
            "DR2_Mov_Avg          int64\n",
            "DR2_Vert_Avg         int64\n",
            "Draw3                int64\n",
            "DR3_Prev_Week        int64\n",
            "DR3_2Weeks           int64\n",
            "DR3_Prev_Entry       int64\n",
            "DR3_Prev_Entry-2     int64\n",
            "DR3_Mov_Avg          int64\n",
            "DR3_Vert_Avg         int64\n",
            "Draw4                int64\n",
            "DR4_Prev_Week        int64\n",
            "DR4_2Weeks           int64\n",
            "DR4_Prev_Entry       int64\n",
            "DR4_Prev_Entry-2     int64\n",
            "DR4_Mov_Avg          int64\n",
            "DR4_Vert_Avg         int64\n",
            "Year                 int64\n",
            "Month                int64\n",
            "Day                  int64\n",
            "Prev_Morning         int64\n",
            "Prev_Afternoon       int64\n",
            "Prev_Evening         int64\n",
            "Prev_Night           int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3.2: Flagging and Exemption of Sundays and Public Holidays\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save datasets function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the most recent datasets\n",
        "train_test_data = load_dataset('G_Handled_DataType_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('G_Handled_DataType_Unseen_Data.csv')\n",
        "\n",
        "# Ensure Date Column is in DateTime Format for both datasets\n",
        "train_test_data['Date'] = pd.to_datetime(train_test_data['Date'])\n",
        "unseen_data['Date'] = pd.to_datetime(unseen_data['Date'])\n",
        "\n",
        "# Updated public holidays list including 2025\n",
        "public_holidays_list = [\n",
        "    \"2018-03-30\", \"2018-05-31\", \"2018-06-15\", \"2018-11-06\", \"2018-12-25\",\n",
        "    \"2019-03-30\", \"2019-04-19\", \"2019-06-05\", \"2019-06-20\", \"2019-10-27\", \"2019-12-25\",\n",
        "    \"2020-03-30\", \"2020-04-10\", \"2020-05-24\", \"2020-06-11\", \"2020-11-14\", \"2020-12-25\",\n",
        "    \"2021-03-30\", \"2021-04-03\", \"2021-05-13\", \"2021-06-03\", \"2021-11-04\", \"2021-12-25\",\n",
        "    \"2022-03-30\", \"2022-04-15\", \"2022-05-02\", \"2022-06-16\", \"2022-10-24\", \"2022-12-26\",\n",
        "    \"2023-03-30\", \"2023-04-07\", \"2023-04-22\", \"2023-06-08\", \"2023-11-12\", \"2023-12-25\",\n",
        "    \"2024-03-29\", \"2024-03-30\", \"2024-04-10\", \"2024-05-30\", \"2024-10-31\", \"2024-12-25\",\n",
        "    # Including 2025 holidays\n",
        "    \"2025-01-01\", \"2025-03-30\", \"2025-03-31\", \"2025-04-18\", \"2025-04-21\",\n",
        "    \"2025-05-30\", \"2025-06-19\", \"2025-08-01\", \"2025-08-31\", \"2025-09-01\",\n",
        "    \"2025-09-24\", \"2025-10-20\", \"2025-12-25\", \"2025-12-26\"\n",
        "]\n",
        "\n",
        "# Convert public holidays list to datetime for comparison\n",
        "public_holidays = pd.to_datetime(public_holidays_list)\n",
        "\n",
        "# Flag public holidays in the datasets\n",
        "train_test_data['Is_Holiday'] = train_test_data['Date'].isin(public_holidays)\n",
        "unseen_data['Is_Holiday'] = unseen_data['Date'].isin(public_holidays)\n",
        "\n",
        "# Flag Sundays as special days alongside public holidays\n",
        "train_test_data['Is_Sunday'] = train_test_data['Date'].dt.dayofweek == 6\n",
        "unseen_data['Is_Sunday'] = unseen_data['Date'].dt.dayofweek == 6\n",
        "\n",
        "# Combine flags to identify any special day\n",
        "train_test_data['Is_Special_Day'] = train_test_data['Is_Holiday'] | train_test_data['Is_Sunday']\n",
        "unseen_data['Is_Special_Day'] = unseen_data['Is_Holiday'] | unseen_data['Is_Sunday']\n",
        "\n",
        "# Debugging - Print to verify flags for a subset\n",
        "print(train_test_data[['Date', 'Is_Special_Day', 'Is_Holiday', 'Is_Sunday']].head(20))\n",
        "\n",
        "# Assuming 'Draw1', 'Draw2', 'Draw3', and 'Draw4' are the draw columns\n",
        "draw_columns = ['Draw1', 'Draw2', 'Draw3', 'Draw4']\n",
        "\n",
        "# Adding a temporary column to check if all draws are zero for a day\n",
        "train_test_data['All_Draws_Zero'] = train_test_data[draw_columns].eq(0).all(axis=1)\n",
        "unseen_data['All_Draws_Zero'] = unseen_data[draw_columns].eq(0).all(axis=1)\n",
        "\n",
        "# Remove rows where it's a special day AND all draws are zero\n",
        "train_test_data = train_test_data[~((train_test_data['Is_Special_Day']) & (train_test_data['All_Draws_Zero']))]\n",
        "unseen_data = unseen_data[~((unseen_data['Is_Special_Day']) & (unseen_data['All_Draws_Zero']))]\n",
        "\n",
        "# Drop the 'All_Draws_Zero' helper column as it's no longer needed\n",
        "train_test_data.drop(columns=['All_Draws_Zero'], inplace=True)\n",
        "unseen_data.drop(columns=['All_Draws_Zero'], inplace=True)\n",
        "\n",
        "# Now continue to save the datasets\n",
        "save_dataset(train_test_data, 'H_Train_Test_Data_Excluding_Special_Days.csv')\n",
        "save_dataset(unseen_data, 'H_Unseen_Data_Excluding_Special_Days.csv')\n",
        "print(\"Datasets excluding special days saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx8k8AJ0omdj",
        "outputId": "0a69d7d0-ce6e-47dd-808e-6919c607d6c7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date  Is_Special_Day  Is_Holiday  Is_Sunday\n",
            "0  2018-08-01           False       False      False\n",
            "1  2018-08-02           False       False      False\n",
            "2  2018-08-03           False       False      False\n",
            "3  2018-08-04           False       False      False\n",
            "4  2018-08-05            True       False       True\n",
            "5  2018-08-06           False       False      False\n",
            "6  2018-08-07           False       False      False\n",
            "7  2018-08-08           False       False      False\n",
            "8  2018-08-09           False       False      False\n",
            "9  2018-08-10           False       False      False\n",
            "10 2018-08-11           False       False      False\n",
            "11 2018-08-12            True       False       True\n",
            "12 2018-08-13           False       False      False\n",
            "13 2018-08-14           False       False      False\n",
            "14 2018-08-15           False       False      False\n",
            "15 2018-08-16           False       False      False\n",
            "16 2018-08-17           False       False      False\n",
            "17 2018-08-18           False       False      False\n",
            "18 2018-08-19            True       False       True\n",
            "19 2018-08-20           False       False      False\n",
            "Datasets excluding special days saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3.3: Adjusted Logic for Handling 'Prev_' Columns Without Future Data Leakage\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Function to load and save datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the datasets\n",
        "train_test_data = load_dataset('H_Train_Test_Data_Excluding_Special_Days.csv')\n",
        "unseen_data = load_dataset('H_Unseen_Data_Excluding_Special_Days.csv')\n",
        "\n",
        "# Assuming train_test_data is your DataFrame\n",
        "\n",
        "# Function to conditionally shift values backward by one day\n",
        "def conditional_shift(df, target_column, source_column):\n",
        "    for i in range(1, len(df)):  # Start from the second row\n",
        "        if df.at[i, target_column] == 0:  # Check if the current row's value is zero\n",
        "            # Replace with the previous day's source column value\n",
        "            df.at[i, target_column] = df.at[i-1, source_column]\n",
        "\n",
        "# Apply conditional shifting\n",
        "conditional_shift(train_test_data, 'DR1_Prev_Entry', 'Prev_Night')\n",
        "conditional_shift(train_test_data, 'DR2_Prev_Entry-2', 'Prev_Night')\n",
        "conditional_shift(train_test_data, 'DR1_Prev_Entry-2', 'Prev_Afternoon')\n",
        "\n",
        "# Repeat for unseen_data if necessary\n",
        "conditional_shift(unseen_data, 'DR1_Prev_Entry', 'Prev_Night')\n",
        "conditional_shift(unseen_data, 'DR2_Prev_Entry-2', 'Prev_Night')\n",
        "conditional_shift(unseen_data, 'DR1_Prev_Entry-2', 'Prev_Afternoon')\n",
        "\n",
        "# Columns for adjusted logic\n",
        "columns_to_adjust = ['Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night']\n",
        "\n",
        "# Assuming 'Draw1', 'Draw2', 'Draw3', and 'Draw4' are your original draw columns\n",
        "original_draw_columns = ['Draw1', 'Draw2', 'Draw3', 'Draw4']\n",
        "\n",
        "# Mapping of 'Prev_' columns to their corresponding 'Draw' columns\n",
        "columns_mapping = dict(zip(columns_to_adjust, original_draw_columns))\n",
        "\n",
        "# Apply the conditional shift for train_test_data\n",
        "for prev_col, draw_col in columns_mapping.items():\n",
        "    # Find indices where 'Prev_' column is 0 and shift the corresponding 'Draw' column value from the day before\n",
        "    indices_to_shift = train_test_data[train_test_data[prev_col] == 0].index\n",
        "    for idx in indices_to_shift:\n",
        "        if idx > 0:  # Ensure it's not the first row to avoid indexing issues\n",
        "            train_test_data.at[idx, prev_col] = train_test_data.at[idx-1, draw_col]\n",
        "\n",
        "# Apply the conditional shift for unseen_data\n",
        "for prev_col, draw_col in columns_mapping.items():\n",
        "    indices_to_shift = unseen_data[unseen_data[prev_col] == 0].index\n",
        "    for idx in indices_to_shift:\n",
        "        if idx > 0:\n",
        "            unseen_data.at[idx, prev_col] = unseen_data.at[idx-1, draw_col]\n",
        "\n",
        "# Note: This approach assumes the first row does not contain zeros in 'Prev_' columns.\n",
        "# If the first row can have zeros, additional logic is needed to handle those cases.\n",
        "# Rename 'DR__Vert_Avg' to 'Draw__Mov_Avg_2'\n",
        "train_test_data.rename(columns={'DR1_Vert_Avg': 'DR1_Mov_Avg_2'}, inplace=True)\n",
        "unseen_data.rename(columns={'DR1_Vert_Avg': 'DR1_Mov_Avg_2'}, inplace=True)\n",
        "train_test_data.rename(columns={'DR2_Vert_Avg': 'DR2_Mov_Avg_2'}, inplace=True)\n",
        "unseen_data.rename(columns={'DR2_Vert_Avg': 'DR2_Mov_Avg_2'}, inplace=True)\n",
        "train_test_data.rename(columns={'DR3_Vert_Avg': 'DR3_Mov_Avg_2'}, inplace=True)\n",
        "unseen_data.rename(columns={'DR3_Vert_Avg': 'DR3_Mov_Avg_2'}, inplace=True)\n",
        "train_test_data.rename(columns={'DR4_Vert_Avg': 'DR4_Mov_Avg_2'}, inplace=True)\n",
        "unseen_data.rename(columns={'DR4_Vert_Avg': 'DR4_Mov_Avg_2'}, inplace=True)\n",
        "\n",
        "# Debug: Confirm the renaming and the addition of new features\n",
        "print(\"Columns in Training/Testing Data after renaming and feature addition:\", train_test_data.columns)\n",
        "print(\"Columns in Unseen Data after renaming and feature addition:\", unseen_data.columns)\n",
        "\n",
        "# Save the datasets with adjusted 'Prev_' columns\n",
        "save_dataset(train_test_data, 'I_Train_Test_Data_adjusted_Prev_columns.csv')\n",
        "save_dataset(unseen_data, 'I_Unseen_Data_adjusted_Prev_columns.csv')\n",
        "\n",
        "print(\"Adjusted 'Prev_' columns completed and datasets saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8evcbXBvQ6A",
        "outputId": "f535aab5-b58b-44de-a76d-a310672357df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in Training/Testing Data after renaming and feature addition: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Mov_Avg_2', 'Draw2', 'DR2_Prev_Week', 'DR2_2Weeks',\n",
            "       'DR2_Prev_Entry', 'DR2_Prev_Entry-2', 'DR2_Mov_Avg', 'DR2_Mov_Avg_2',\n",
            "       'Draw3', 'DR3_Prev_Week', 'DR3_2Weeks', 'DR3_Prev_Entry',\n",
            "       'DR3_Prev_Entry-2', 'DR3_Mov_Avg', 'DR3_Mov_Avg_2', 'Draw4',\n",
            "       'DR4_Prev_Week', 'DR4_2Weeks', 'DR4_Prev_Entry', 'DR4_Prev_Entry-2',\n",
            "       'DR4_Mov_Avg', 'DR4_Mov_Avg_2', 'Year', 'Month', 'Day', 'Prev_Morning',\n",
            "       'Prev_Afternoon', 'Prev_Evening', 'Prev_Night', 'Is_Holiday',\n",
            "       'Is_Sunday', 'Is_Special_Day'],\n",
            "      dtype='object')\n",
            "Columns in Unseen Data after renaming and feature addition: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Mov_Avg_2', 'Draw2', 'DR2_Prev_Week', 'DR2_2Weeks',\n",
            "       'DR2_Prev_Entry', 'DR2_Prev_Entry-2', 'DR2_Mov_Avg', 'DR2_Mov_Avg_2',\n",
            "       'Draw3', 'DR3_Prev_Week', 'DR3_2Weeks', 'DR3_Prev_Entry',\n",
            "       'DR3_Prev_Entry-2', 'DR3_Mov_Avg', 'DR3_Mov_Avg_2', 'Draw4',\n",
            "       'DR4_Prev_Week', 'DR4_2Weeks', 'DR4_Prev_Entry', 'DR4_Prev_Entry-2',\n",
            "       'DR4_Mov_Avg', 'DR4_Mov_Avg_2', 'Year', 'Month', 'Day', 'Prev_Morning',\n",
            "       'Prev_Afternoon', 'Prev_Evening', 'Prev_Night', 'Is_Holiday',\n",
            "       'Is_Sunday', 'Is_Special_Day'],\n",
            "      dtype='object')\n",
            "Adjusted 'Prev_' columns completed and datasets saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4.1: Introducing \"Lines\" as a new feature in both datasets\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load the most recent CSVs\n",
        "train_test_data = pd.read_csv(base_dir + 'I_Train_Test_Data_adjusted_Prev_columns.csv')\n",
        "unseen_data = pd.read_csv(base_dir + 'I_Unseen_Data_adjusted_Prev_columns.csv')\n",
        "\n",
        "# Directory of lines with corresponding numbers\n",
        "lines_directory = {\n",
        "    1: [1, 10, 19, 28],\n",
        "    2: [2, 11, 20, 29],\n",
        "    3: [3, 12, 21, 30],\n",
        "    4: [4, 13, 22, 31],\n",
        "    5: [5, 14, 23, 32],\n",
        "    6: [6, 15, 24, 33],\n",
        "    7: [7, 16, 25, 34],\n",
        "    8: [8, 17, 26, 35],\n",
        "    9: [9, 18, 27, 36],\n",
        "}\n",
        "\n",
        "# Function to calculate the sum of digits and map to a 'Line'\n",
        "def sum_to_line(x):\n",
        "    try:\n",
        "        sum_of_digits = sum(int(digit) for digit in str(x))\n",
        "        # Ensure the sum is between 1 and 9\n",
        "        while sum_of_digits > 9:\n",
        "            sum_of_digits = sum(int(digit) for digit in str(sum_of_digits))\n",
        "        return sum_of_digits\n",
        "    except ValueError:\n",
        "        # Return 0 if the value cannot be converted to an integer (e.g., missing or non-numeric data)\n",
        "        return 0\n",
        "\n",
        "# Create the 'Line_Prev_Entry' column\n",
        "train_test_data['Line_Prev_Entry'] = train_test_data['Prev_Night'].apply(sum_to_line)\n",
        "unseen_data['Line_Prev_Entry'] = unseen_data['Prev_Night'].apply(sum_to_line)\n",
        "\n",
        "# Function to extract numbers from the line subset for the active line\n",
        "def extract_line_numbers(row, lines_dict):\n",
        "    active_line = row['Line_Prev_Entry']\n",
        "    # Check if it's a day with no draws or an invalid line number\n",
        "    if active_line == 0 or active_line not in lines_dict:\n",
        "        # Set all numbers for this row to zero or NaN\n",
        "        for i in range(1, 5):\n",
        "            row[f'Line_PE_Num_{i}'] = 0  # Or use NaN if that's preferred\n",
        "    else:\n",
        "        # Populate the row with numbers from the active line\n",
        "        for i, num in enumerate(lines_dict[active_line], start=1):\n",
        "            row[f'Line_PE_Num_{i}'] = num\n",
        "    return row\n",
        "\n",
        "# Apply the function to each row of the DataFrame\n",
        "train_test_data = train_test_data.apply(lambda row: extract_line_numbers(row, lines_directory), axis=1)\n",
        "unseen_data = unseen_data.apply(lambda row: extract_line_numbers(row, lines_directory), axis=1)\n",
        "\n",
        "# Save the updated datasets with 'Lines' as new features\n",
        "train_test_data.to_csv(base_dir + 'J_Lines_Train_Test_Data.csv', index=False)\n",
        "unseen_data.to_csv(base_dir + 'J_Lines_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of both datasets to verify the \"Lines\" assignment\n",
        "print(\"First few rows of train/test data with 'Lines' assigned:\")\n",
        "print(train_test_data.head())\n",
        "\n",
        "print(\"\\nFirst few rows of unseen data with 'Lines' assigned:\")\n",
        "print(unseen_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qBmKgOFOskq",
        "outputId": "b48fbf9c-bc68-426a-a815-eec2ea4bb82f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of train/test data with 'Lines' assigned:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2018-08-01           1  Training     19              7          27   \n",
            "1  2018-08-02           2  Training     31             11           1   \n",
            "2  2018-08-03           3  Training     15             19          21   \n",
            "3  2018-08-04           4  Training     31             35          18   \n",
            "4  2018-08-06           6  Training     31             18          22   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Mov_Avg_2  ...  \\\n",
            "0              23                32           27             17  ...   \n",
            "1               9                33           21              6  ...   \n",
            "2              12                35           23             20  ...   \n",
            "3              35                23           29             26  ...   \n",
            "4              16                29           22             20  ...   \n",
            "\n",
            "   Prev_Evening  Prev_Night  Is_Holiday  Is_Sunday  Is_Special_Day  \\\n",
            "0            32          23       False      False           False   \n",
            "1            33           9       False      False           False   \n",
            "2            35          12       False      False           False   \n",
            "3            23          35       False      False           False   \n",
            "4            29          16       False      False           False   \n",
            "\n",
            "   Line_Prev_Entry  Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  Line_PE_Num_4  \n",
            "0                5              5             14             23             32  \n",
            "1                9              9             18             27             36  \n",
            "2                3              3             12             21             30  \n",
            "3                8              8             17             26             35  \n",
            "4                7              7             16             25             34  \n",
            "\n",
            "[5 rows x 46 columns]\n",
            "\n",
            "First few rows of unseen data with 'Lines' assigned:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2023-08-01        1673    Unseen     13             27          25   \n",
            "1  2023-08-02        1674    Unseen     21             33          12   \n",
            "2  2023-08-03        1675    Unseen     15             27           3   \n",
            "3  2023-08-04        1676    Unseen     13             20          11   \n",
            "4  2023-08-05        1677    Unseen     12             29          14   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Mov_Avg_2  ...  \\\n",
            "0               5                 7            6             26  ...   \n",
            "1              18                26           22             22  ...   \n",
            "2              28                 7           17             15  ...   \n",
            "3               2                 2            2             15  ...   \n",
            "4              12                22           17             21  ...   \n",
            "\n",
            "   Prev_Evening  Prev_Night  Is_Holiday  Is_Sunday  Is_Special_Day  \\\n",
            "0             7           5       False      False           False   \n",
            "1            26          18       False      False           False   \n",
            "2             7          28       False      False           False   \n",
            "3             2           2       False      False           False   \n",
            "4            22          12       False      False           False   \n",
            "\n",
            "   Line_Prev_Entry  Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  Line_PE_Num_4  \n",
            "0                5              5             14             23             32  \n",
            "1                9              9             18             27             36  \n",
            "2                1              1             10             19             28  \n",
            "3                2              2             11             20             29  \n",
            "4                3              3             12             21             30  \n",
            "\n",
            "[5 rows x 46 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 4.2: # Introducing \"Special Groups\" as a new feature in both datasets\n",
        "\n",
        "# Load the most recent CSVs\n",
        "#train_test_data = pd.read_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/M_Lines_Train_Test_Data.csv')\n",
        "#unseen_data = pd.read_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/N_Lines_Unseen_Data.csv')\n",
        "\n",
        "# Define the mapping for \"Special Groups\"\n",
        "#special_groups_mapping = {\n",
        "#    2: 1, 15: 1, 16: 1, 24: 1, 31: 1,  # \"Ladies\"\n",
        "#    4: 2, 5: 2, 12: 2, 29: 2, 34: 2,  # \"Men\"\n",
        "#    11: 3, 17: 3, 26: 3,  # \"Birds\"\n",
        "#    7: 4, 9: 4, 19: 4, 20: 4, 22: 4, 30: 4, 36: 4,  # \"Domestic Animals\"\n",
        "#    8: 5, 10: 5, 13: 5, 25: 5,  # \"Wild Animals\"\n",
        "#    18: 6, 28: 6, 32: 6,  # \"Ocean\"\n",
        "#    1: 7, 27: 7, 33: 7, 35: 7,  # \"Snakes & Insects\"\n",
        "#    3: 8, 6: 8, 14: 8, 21: 8, 23: 8  # \"Home\"\n",
        "#}\n",
        "\n",
        "# Function to assign \"Special Groups\" based on the mapping\n",
        "#def assign_special_groups(data, column_name, special_groups_mapping):\n",
        "#    data[f'Special_Groups_{column_name}'] = data[column_name].map(special_groups_mapping).fillna(0).astype(int)\n",
        "\n",
        "# List of columns to assign \"Special Groups\"\n",
        "#columns_to_assign_special_groups = ['Draw1', 'DR1_Prev_Week', 'DR1_Prev_Entry']\n",
        "\n",
        "# Assign \"Special Groups\" for specified columns in train/test data\n",
        "#for column in columns_to_assign_special_groups:\n",
        "#    assign_special_groups(train_test_data, column, special_groups_mapping)\n",
        "\n",
        "# Assign \"Special Groups\" for specified columns in unseen data\n",
        "#for column in columns_to_assign_special_groups:\n",
        "#    assign_special_groups(unseen_data, column, special_groups_mapping)\n",
        "\n",
        "# Save the updated datasets\n",
        "#train_test_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/O_Special_Groups_Train_Test_Data.csv', index=False)\n",
        "#unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/P_Special_Groups_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of both datasets to verify the \"Special Groups\" assignment\n",
        "#print(\"First few rows of train/test data with 'Special Groups' assigned:\")\n",
        "#print(train_test_data.head())\n",
        "\n",
        "#print(\"\\nFirst few rows of unseen data with 'Special Groups' assigned:\")\n",
        "#print(unseen_data.head())\n"
      ],
      "metadata": {
        "id": "U8TauDfPOtKG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4.3: Introducing \"Spirits\" as a new feature in both datasets\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load the most recent CSVs\n",
        "train_test_data = pd.read_csv(base_dir + 'J_Lines_Train_Test_Data.csv')\n",
        "unseen_data = pd.read_csv(base_dir + 'J_Lines_Unseen_Data.csv')\n",
        "\n",
        "# Define a mapping dictionary for \"Spirits\" pairs\n",
        "spirits_mapping = {\n",
        "    1: 5,\n",
        "    2: 24,\n",
        "    3: 19,\n",
        "    4: 35,\n",
        "    5: 1,\n",
        "    6: 15,\n",
        "    7: 13,\n",
        "    8: 29,\n",
        "    9: 33,\n",
        "    10: 28,\n",
        "    11: 36,\n",
        "    12: 32,\n",
        "    13: 7,\n",
        "    14: 25,\n",
        "    15: 6,\n",
        "    16: 17,\n",
        "    17: 16,\n",
        "    18: 30,\n",
        "    19: 3,\n",
        "    20: 22,\n",
        "    21: 23,\n",
        "    22: 20,\n",
        "    23: 21,\n",
        "    24: 2,\n",
        "    25: 14,\n",
        "    26: 27,\n",
        "    27: 26,\n",
        "    28: 10,\n",
        "    29: 8,\n",
        "    30: 18,\n",
        "    31: 34,\n",
        "    32: 12,\n",
        "    33: 9,\n",
        "    34: 31,\n",
        "    35: 4,\n",
        "    36: 11\n",
        "}\n",
        "\n",
        "# Function to map \"Prev_Night\" to its spirit pair\n",
        "def map_to_spirit(x, spirits_dict):\n",
        "    # Return the corresponding spirit number or None if not found\n",
        "    return spirits_dict.get(x)\n",
        "\n",
        "# Create the 'Spirit_PE_Num' column\n",
        "train_test_data['Spirit_PE_Num'] = train_test_data['Prev_Night'].apply(lambda x: map_to_spirit(x, spirits_mapping))\n",
        "unseen_data['Spirit_PE_Num'] = unseen_data['Prev_Night'].apply(lambda x: map_to_spirit(x, spirits_mapping))\n",
        "\n",
        "# Replace NaNs with 0 and convert to int\n",
        "train_test_data['Spirit_PE_Num'] = train_test_data['Spirit_PE_Num'].fillna(0).astype(int)\n",
        "unseen_data['Spirit_PE_Num'] = unseen_data['Spirit_PE_Num'].fillna(0).astype(int)\n",
        "\n",
        "# Save the updated datasets with 'Spirits' as new features\n",
        "train_test_data.to_csv(base_dir + 'K_Spirits_Train_Test_Data.csv', index=False)\n",
        "unseen_data.to_csv(base_dir + 'K_Spirits_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of both datasets to verify the \"Spirits\" assignment\n",
        "print(\"First few rows of train/test data with 'Spirits' assigned:\")\n",
        "print(train_test_data.head())\n",
        "\n",
        "print(\"\\nFirst few rows of unseen data with 'Spirits' assigned:\")\n",
        "print(unseen_data.head())\n"
      ],
      "metadata": {
        "id": "_8T-Y4tK20PV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65979b5-6c96-4933-f66e-7c16a22cd9af"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of train/test data with 'Spirits' assigned:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2018-08-01           1  Training     19              7          27   \n",
            "1  2018-08-02           2  Training     31             11           1   \n",
            "2  2018-08-03           3  Training     15             19          21   \n",
            "3  2018-08-04           4  Training     31             35          18   \n",
            "4  2018-08-06           6  Training     31             18          22   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Mov_Avg_2  ...  \\\n",
            "0              23                32           27             17  ...   \n",
            "1               9                33           21              6  ...   \n",
            "2              12                35           23             20  ...   \n",
            "3              35                23           29             26  ...   \n",
            "4              16                29           22             20  ...   \n",
            "\n",
            "   Prev_Night  Is_Holiday  Is_Sunday  Is_Special_Day  Line_Prev_Entry  \\\n",
            "0          23       False      False           False                5   \n",
            "1           9       False      False           False                9   \n",
            "2          12       False      False           False                3   \n",
            "3          35       False      False           False                8   \n",
            "4          16       False      False           False                7   \n",
            "\n",
            "   Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  Line_PE_Num_4  Spirit_PE_Num  \n",
            "0              5             14             23             32             21  \n",
            "1              9             18             27             36             33  \n",
            "2              3             12             21             30             32  \n",
            "3              8             17             26             35              4  \n",
            "4              7             16             25             34             17  \n",
            "\n",
            "[5 rows x 47 columns]\n",
            "\n",
            "First few rows of unseen data with 'Spirits' assigned:\n",
            "         Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2023-08-01        1673    Unseen     13             27          25   \n",
            "1  2023-08-02        1674    Unseen     21             33          12   \n",
            "2  2023-08-03        1675    Unseen     15             27           3   \n",
            "3  2023-08-04        1676    Unseen     13             20          11   \n",
            "4  2023-08-05        1677    Unseen     12             29          14   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Mov_Avg_2  ...  \\\n",
            "0               5                 7            6             26  ...   \n",
            "1              18                26           22             22  ...   \n",
            "2              28                 7           17             15  ...   \n",
            "3               2                 2            2             15  ...   \n",
            "4              12                22           17             21  ...   \n",
            "\n",
            "   Prev_Night  Is_Holiday  Is_Sunday  Is_Special_Day  Line_Prev_Entry  \\\n",
            "0           5       False      False           False                5   \n",
            "1          18       False      False           False                9   \n",
            "2          28       False      False           False                1   \n",
            "3           2       False      False           False                2   \n",
            "4          12       False      False           False                3   \n",
            "\n",
            "   Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  Line_PE_Num_4  Spirit_PE_Num  \n",
            "0              5             14             23             32              1  \n",
            "1              9             18             27             36             30  \n",
            "2              1             10             19             28             10  \n",
            "3              2             11             20             29             24  \n",
            "4              3             12             21             30             32  \n",
            "\n",
            "[5 rows x 47 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4.4: Introducing \"Rakes\" as a new feature in both datasets\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load the most recent CSVs\n",
        "train_test_data = pd.read_csv(base_dir + 'K_Spirits_Train_Test_Data.csv')\n",
        "unseen_data = pd.read_csv(base_dir + 'K_Spirits_Unseen_Data.csv')\n",
        "\n",
        "# Define a mapping dictionary for \"Rakes\" numbers\n",
        "rakes_mapping = {\n",
        "    1: [7, 12, 15, 36],\n",
        "    2: [4, 14, 16, 23],\n",
        "    3: [19, 22, 34, 35],\n",
        "    4: [7, 11, 14, 32],\n",
        "    5: [20, 23, 31, 33],\n",
        "    6: [14, 24, 30, 32],\n",
        "    7: [4, 11, 24, 29],\n",
        "    8: [12, 14, 33, 36],\n",
        "    9: [7, 11, 24, 32],\n",
        "    10: [7, 12, 15, 36],\n",
        "    11: [4, 7, 17, 22],\n",
        "    12: [1, 10, 28, 35],\n",
        "    13: [10, 25, 28, 29],\n",
        "    14: [16, 23, 31, 33],\n",
        "    15: [5, 6, 23, 36],\n",
        "    16: [14, 31, 35, 36],\n",
        "    17: [11, 16, 26, 29],\n",
        "    18: [5, 7, 28, 33],\n",
        "    19: [10, 27, 32, 36],\n",
        "    20: [4, 14, 24, 30],\n",
        "    21: [16, 22, 24, 29],\n",
        "    22: [11, 30, 32, 34],\n",
        "    23: [12, 14, 16, 20],\n",
        "    24: [2, 6, 12, 21],\n",
        "    25: [1, 3, 13, 18],\n",
        "    26: [1, 2, 11, 17],\n",
        "    27: [14, 16, 19, 35],\n",
        "    28: [12, 13, 18, 33],\n",
        "    29: [7, 13, 16, 28],\n",
        "    30: [6, 8, 20, 22],\n",
        "    31: [5, 14, 16, 36],\n",
        "    32: [4, 6, 28, 36],\n",
        "    33: [5, 10, 14, 20],\n",
        "    34: [1, 12, 20, 22],\n",
        "    35: [3, 12, 14, 16],\n",
        "    36: [1, 8, 16, 32],\n",
        "}\n",
        "\n",
        "# Function to assign \"Rakes\" based on the 'Prev_Night' value\n",
        "def assign_rakes(data, column_name, rakes_dict):\n",
        "    # Create new columns for Rakes numbers\n",
        "    for i in range(1, 5):\n",
        "        data[f'Rake_PE_Num_{i}'] = 0\n",
        "\n",
        "    # Populate Rakes numbers\n",
        "    for index, row in data.iterrows():\n",
        "        rakes_numbers = rakes_dict.get(row[column_name], [0, 0, 0, 0])\n",
        "        for i, rake_num in enumerate(rakes_numbers, start=1):\n",
        "            data.at[index, f'Rake_PE_Num_{i}'] = rake_num\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply 'assign_rakes' function to create new Rakes columns\n",
        "train_test_data = assign_rakes(train_test_data, 'Prev_Night', rakes_mapping)\n",
        "unseen_data = assign_rakes(unseen_data, 'Prev_Night', rakes_mapping)\n",
        "\n",
        "# Save the updated datasets with 'Rakes' as new features\n",
        "train_test_data.to_csv(base_dir + 'L_Rakes_Train_Test_Data.csv', index=False)\n",
        "unseen_data.to_csv(base_dir + 'L_Rakes_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of both datasets to verify the \"Rakes\" assignment\n",
        "print(\"First few rows of train/test data with 'Rakes' assigned:\")\n",
        "print(train_test_data[['Prev_Night', 'Rake_PE_Num_1', 'Rake_PE_Num_2', 'Rake_PE_Num_3', 'Rake_PE_Num_4']].head())\n",
        "\n",
        "print(\"\\nFirst few rows of unseen data with 'Rakes' assigned:\")\n",
        "print(unseen_data[['Prev_Night', 'Rake_PE_Num_1', 'Rake_PE_Num_2', 'Rake_PE_Num_3', 'Rake_PE_Num_4']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNf2x1Y8IVe5",
        "outputId": "80cd41b8-5a81-4b80-81c7-8c7cbd9f19ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of train/test data with 'Rakes' assigned:\n",
            "   Prev_Night  Rake_PE_Num_1  Rake_PE_Num_2  Rake_PE_Num_3  Rake_PE_Num_4\n",
            "0          23             12             14             16             20\n",
            "1           9              7             11             24             32\n",
            "2          12              1             10             28             35\n",
            "3          35              3             12             14             16\n",
            "4          16             14             31             35             36\n",
            "\n",
            "First few rows of unseen data with 'Rakes' assigned:\n",
            "   Prev_Night  Rake_PE_Num_1  Rake_PE_Num_2  Rake_PE_Num_3  Rake_PE_Num_4\n",
            "0           5             20             23             31             33\n",
            "1          18              5              7             28             33\n",
            "2          28             12             13             18             33\n",
            "3           2              4             14             16             23\n",
            "4          12              1             10             28             35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5.1: Creation of Arithmetical Features for 'Draw1'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "train_test_data = load_dataset('L_Rakes_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('L_Rakes_Unseen_Data.csv')\n",
        "def create_arithmetical_features_draw1(df, window_sizes=[3, 5, 10]):\n",
        "    \"\"\"\n",
        "    Amend DataFrame in-place, adding rolling window calculations and other\n",
        "    mathematical features based specifically on 'Draw1'.\n",
        "    \"\"\"\n",
        "    for window in window_sizes:\n",
        "        # Calculate rolling features for 'Draw1'\n",
        "        df[f'Draw1_Moving_Avg_{window}'] = df['Draw1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Median_{window}'] = df['Draw1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Std_Dev_{window}'] = df['Draw1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_RMS_{window}'] = np.sqrt(df['Draw1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Rolling_Min_{window}'] = df['Draw1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Rolling_Max_{window}'] = df['Draw1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Skew_{window}'] = df['Draw1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Kurtosis_{window}'] = df['Draw1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
        "\n",
        "        # Ensure no future data is used in calculating these features\n",
        "        df[f'Draw1_EMA_{window}'] = df['Draw1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Rolling_Var_{window}'] = df['Draw1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
        "        df[f'Draw1_Rolling_Range_{window}'] = df[f'Draw1_Rolling_Max_{window}'] - df[f'Draw1_Rolling_Min_{window}']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature creation for 'Draw1'\n",
        "train_test_data = create_arithmetical_features_draw1(train_test_data)\n",
        "unseen_data = create_arithmetical_features_draw1(unseen_data)\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "\n",
        "# List of all the newly created feature names\n",
        "new_feature_names = [\n",
        "    'Draw1_Moving_Avg_3', 'Draw1_Median_3', 'Draw1_Std_Dev_3', 'Draw1_RMS_3', 'Draw1_Rolling_Min_3', 'Draw1_Rolling_Max_3', 'Draw1_Skew_3', 'Draw1_Kurtosis_3', 'Draw1_EMA_3', 'Draw1_Rolling_Var_3', 'Draw1_Rolling_Range_3',\n",
        "    'Draw1_Moving_Avg_5', 'Draw1_Median_5', 'Draw1_Std_Dev_5', 'Draw1_RMS_5', 'Draw1_Rolling_Min_5', 'Draw1_Rolling_Max_5', 'Draw1_Skew_5', 'Draw1_Kurtosis_5', 'Draw1_EMA_5', 'Draw1_Rolling_Var_5', 'Draw1_Rolling_Range_5',\n",
        "    'Draw1_Moving_Avg_10', 'Draw1_Median_10', 'Draw1_Std_Dev_10', 'Draw1_RMS_10', 'Draw1_Rolling_Min_10', 'Draw1_Rolling_Max_10', 'Draw1_Skew_10', 'Draw1_Kurtosis_10', 'Draw1_EMA_10', 'Draw1_Rolling_Var_10', 'Draw1_Rolling_Range_10'\n",
        "]\n",
        "\n",
        "# Fill NaN values for these features\n",
        "# You can choose to fill with 0, mean, median, or use forward/backward filling\n",
        "def fill_nan_values(df, features):\n",
        "    for feature in features:\n",
        "        df[feature] = df[feature].fillna(0)\n",
        "\n",
        "fill_nan_values(train_test_data, new_feature_names)\n",
        "fill_nan_values(unseen_data, new_feature_names)\n",
        "\n",
        "# Check if there are any remaining NaNs in these features\n",
        "remaining_nans = train_test_data[new_feature_names].isna().sum().sum()\n",
        "print(f\"Remaining NaNs in new features: {remaining_nans}\")\n",
        "\n",
        "# Debug: Confirm the addition of new features\n",
        "print(\"Shape of Training/Testing Data after feature addition:\", train_test_data.shape)\n",
        "print(\"Shape of Unseen Data after feature addition:\", unseen_data.shape)\n",
        "\n",
        "# Save the enhanced datasets\n",
        "save_dataset(train_test_data, 'M_Arithmetic_Features_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'M_Arithmetic_Features_Unseen_Data.csv')\n",
        "print(\"Enhanced datasets saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tVFL7RFw415",
        "outputId": "3adda1dd-b9c3-4dd5-f828-f58f8793cc8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining NaNs in new features: 0\n",
            "Shape of Training/Testing Data after feature addition: (1409, 84)\n",
            "Shape of Unseen Data after feature addition: (105, 84)\n",
            "Enhanced datasets saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5.2: Creation of Temporal Features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save dataset function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load your datasets\n",
        "train_test_data = load_dataset('M_Arithmetic_Features_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('M_Arithmetic_Features_Unseen_Data.csv')\n",
        "\n",
        "def create_temporal_features(df):\n",
        "    \"\"\"\n",
        "    Enhance the DataFrame with temporal features, carefully avoiding data leakage.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime format if not already done\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Extract Day of the Week from 'Date'\n",
        "    df['DayofWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "    # Shifting 'Draw1' to 'Draw4' to use only up to the previous day's results\n",
        "    for draw in ['Draw1', 'Draw2', 'Draw3', 'Draw4']:\n",
        "        df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
        "        df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
        "\n",
        "    # Adding Day of Year for seasonality\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "    # Interval Since Last Appearance and Cumulative Count for each number\n",
        "    max_num =  36\n",
        "    for num in range(1, max_num +  1):\n",
        "        mask = df[['Draw1', 'Draw2', 'Draw3', 'Draw4']].apply(lambda x: num in x.values, axis=1)\n",
        "        df[f'Num_{num}_Interval_Last'] = (~mask).cumsum()\n",
        "        df[f'Num_{num}_Cum_Count'] = mask.cumsum()\n",
        "\n",
        "    # Impute NaNs with zero\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Applying the function to your datasets\n",
        "train_test_data = create_temporal_features(train_test_data)\n",
        "unseen_data = create_temporal_features(unseen_data)\n",
        "\n",
        "# Debug: Print the shape to confirm the addition of new temporal features\n",
        "print(\"Shape of Training/Testing Data after temporal feature addition:\", train_test_data.shape)\n",
        "print(\"Shape of Unseen Data after temporal feature addition:\", unseen_data.shape)\n",
        "\n",
        "# Check for missing values (should be zero now)\n",
        "print(\"\\nMissing values in Training/Testing Data:\\n\", train_test_data.isnull().sum())\n",
        "print(\"\\nMissing values in Unseen Data:\\n\", unseen_data.isnull().sum())\n",
        "\n",
        "# Save the enhanced datasets\n",
        "save_dataset(train_test_data, 'N_Temporal_Features_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'N_Temporal_Features_Unseen_Data.csv')\n",
        "print(\"Datasets with temporal features saved successfully.\")\n",
        "\n",
        "# Debug: Confirm the addition of new features\n",
        "print(\"Columns in Training/Testing Data after temporal feature addition:\", train_test_data.columns)\n",
        "print(\"Columns in Unseen Data after temporal feature addition:\", unseen_data.columns)\n",
        "\n",
        "# Identify and print the new features\n",
        "original_columns = ['Date', 'Draw1', 'Draw2', 'Draw3', 'Draw4']  # Assuming these are the original columns\n",
        "new_columns = train_test_data.columns.tolist()  # Get the new columns\n",
        "\n",
        "# Filter new columns\n",
        "new_features = [col for col in new_columns if col not in original_columns]\n",
        "\n",
        "print(\"\\nNew features added:\", new_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONzJmXwTxsEF",
        "outputId": "0db6e30a-de66-4562-deb0-a650bfbc9ed8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training/Testing Data after temporal feature addition: (1409, 166)\n",
            "Shape of Unseen Data after temporal feature addition: (105, 166)\n",
            "\n",
            "Missing values in Training/Testing Data:\n",
            " Date                    0\n",
            "Row Number              0\n",
            "Data_Type               0\n",
            "Draw1                   0\n",
            "DR1_Prev_Week           0\n",
            "                       ..\n",
            "Num_34_Cum_Count        0\n",
            "Num_35_Interval_Last    0\n",
            "Num_35_Cum_Count        0\n",
            "Num_36_Interval_Last    0\n",
            "Num_36_Cum_Count        0\n",
            "Length: 166, dtype: int64\n",
            "\n",
            "Missing values in Unseen Data:\n",
            " Date                    0\n",
            "Row Number              0\n",
            "Data_Type               0\n",
            "Draw1                   0\n",
            "DR1_Prev_Week           0\n",
            "                       ..\n",
            "Num_34_Cum_Count        0\n",
            "Num_35_Interval_Last    0\n",
            "Num_35_Cum_Count        0\n",
            "Num_36_Interval_Last    0\n",
            "Num_36_Cum_Count        0\n",
            "Length: 166, dtype: int64\n",
            "Datasets with temporal features saved successfully.\n",
            "Columns in Training/Testing Data after temporal feature addition: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Mov_Avg_2',\n",
            "       ...\n",
            "       'Num_32_Interval_Last', 'Num_32_Cum_Count', 'Num_33_Interval_Last',\n",
            "       'Num_33_Cum_Count', 'Num_34_Interval_Last', 'Num_34_Cum_Count',\n",
            "       'Num_35_Interval_Last', 'Num_35_Cum_Count', 'Num_36_Interval_Last',\n",
            "       'Num_36_Cum_Count'],\n",
            "      dtype='object', length=166)\n",
            "Columns in Unseen Data after temporal feature addition: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Mov_Avg_2',\n",
            "       ...\n",
            "       'Num_32_Interval_Last', 'Num_32_Cum_Count', 'Num_33_Interval_Last',\n",
            "       'Num_33_Cum_Count', 'Num_34_Interval_Last', 'Num_34_Cum_Count',\n",
            "       'Num_35_Interval_Last', 'Num_35_Cum_Count', 'Num_36_Interval_Last',\n",
            "       'Num_36_Cum_Count'],\n",
            "      dtype='object', length=166)\n",
            "\n",
            "New features added: ['Row Number', 'Data_Type', 'DR1_Prev_Week', 'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg', 'DR1_Mov_Avg_2', 'DR2_Prev_Week', 'DR2_2Weeks', 'DR2_Prev_Entry', 'DR2_Prev_Entry-2', 'DR2_Mov_Avg', 'DR2_Mov_Avg_2', 'DR3_Prev_Week', 'DR3_2Weeks', 'DR3_Prev_Entry', 'DR3_Prev_Entry-2', 'DR3_Mov_Avg', 'DR3_Mov_Avg_2', 'DR4_Prev_Week', 'DR4_2Weeks', 'DR4_Prev_Entry', 'DR4_Prev_Entry-2', 'DR4_Mov_Avg', 'DR4_Mov_Avg_2', 'Year', 'Month', 'Day', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night', 'Is_Holiday', 'Is_Sunday', 'Is_Special_Day', 'Line_Prev_Entry', 'Line_PE_Num_1', 'Line_PE_Num_2', 'Line_PE_Num_3', 'Line_PE_Num_4', 'Spirit_PE_Num', 'Rake_PE_Num_1', 'Rake_PE_Num_2', 'Rake_PE_Num_3', 'Rake_PE_Num_4', 'Draw1_Moving_Avg_3', 'Draw1_Median_3', 'Draw1_Std_Dev_3', 'Draw1_RMS_3', 'Draw1_Rolling_Min_3', 'Draw1_Rolling_Max_3', 'Draw1_Skew_3', 'Draw1_Kurtosis_3', 'Draw1_EMA_3', 'Draw1_Rolling_Var_3', 'Draw1_Rolling_Range_3', 'Draw1_Moving_Avg_5', 'Draw1_Median_5', 'Draw1_Std_Dev_5', 'Draw1_RMS_5', 'Draw1_Rolling_Min_5', 'Draw1_Rolling_Max_5', 'Draw1_Skew_5', 'Draw1_Kurtosis_5', 'Draw1_EMA_5', 'Draw1_Rolling_Var_5', 'Draw1_Rolling_Range_5', 'Draw1_Moving_Avg_10', 'Draw1_Median_10', 'Draw1_Std_Dev_10', 'Draw1_RMS_10', 'Draw1_Rolling_Min_10', 'Draw1_Rolling_Max_10', 'Draw1_Skew_10', 'Draw1_Kurtosis_10', 'Draw1_EMA_10', 'Draw1_Rolling_Var_10', 'Draw1_Rolling_Range_10', 'DayofWeek', 'Draw1_Prev', 'Draw1_Change', 'Draw2_Prev', 'Draw2_Change', 'Draw3_Prev', 'Draw3_Change', 'Draw4_Prev', 'Draw4_Change', 'DayOfYear', 'Num_1_Interval_Last', 'Num_1_Cum_Count', 'Num_2_Interval_Last', 'Num_2_Cum_Count', 'Num_3_Interval_Last', 'Num_3_Cum_Count', 'Num_4_Interval_Last', 'Num_4_Cum_Count', 'Num_5_Interval_Last', 'Num_5_Cum_Count', 'Num_6_Interval_Last', 'Num_6_Cum_Count', 'Num_7_Interval_Last', 'Num_7_Cum_Count', 'Num_8_Interval_Last', 'Num_8_Cum_Count', 'Num_9_Interval_Last', 'Num_9_Cum_Count', 'Num_10_Interval_Last', 'Num_10_Cum_Count', 'Num_11_Interval_Last', 'Num_11_Cum_Count', 'Num_12_Interval_Last', 'Num_12_Cum_Count', 'Num_13_Interval_Last', 'Num_13_Cum_Count', 'Num_14_Interval_Last', 'Num_14_Cum_Count', 'Num_15_Interval_Last', 'Num_15_Cum_Count', 'Num_16_Interval_Last', 'Num_16_Cum_Count', 'Num_17_Interval_Last', 'Num_17_Cum_Count', 'Num_18_Interval_Last', 'Num_18_Cum_Count', 'Num_19_Interval_Last', 'Num_19_Cum_Count', 'Num_20_Interval_Last', 'Num_20_Cum_Count', 'Num_21_Interval_Last', 'Num_21_Cum_Count', 'Num_22_Interval_Last', 'Num_22_Cum_Count', 'Num_23_Interval_Last', 'Num_23_Cum_Count', 'Num_24_Interval_Last', 'Num_24_Cum_Count', 'Num_25_Interval_Last', 'Num_25_Cum_Count', 'Num_26_Interval_Last', 'Num_26_Cum_Count', 'Num_27_Interval_Last', 'Num_27_Cum_Count', 'Num_28_Interval_Last', 'Num_28_Cum_Count', 'Num_29_Interval_Last', 'Num_29_Cum_Count', 'Num_30_Interval_Last', 'Num_30_Cum_Count', 'Num_31_Interval_Last', 'Num_31_Cum_Count', 'Num_32_Interval_Last', 'Num_32_Cum_Count', 'Num_33_Interval_Last', 'Num_33_Cum_Count', 'Num_34_Interval_Last', 'Num_34_Cum_Count', 'Num_35_Interval_Last', 'Num_35_Cum_Count', 'Num_36_Interval_Last', 'Num_36_Cum_Count']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5.3: Feature Engineering (Interaction terms, Polynomial features, Domain-specific transformations, Clustering-based features)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save dataset function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load your datasets\n",
        "train_test_data = load_dataset('N_Temporal_Features_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('N_Temporal_Features_Unseen_Data.csv')\n",
        "\n",
        "# Print the original columns\n",
        "print(\"Original columns in DataFrame:\", train_test_data.columns)\n",
        "\n",
        "# Function to create specific interaction terms\n",
        "def create_specific_interaction_terms(data):\n",
        "    # Define interactions\n",
        "    interactions = [('Draw1_Change', 'Draw2_Change'), ('Draw1_Change', 'Draw3_Change'), ('Draw1_Change', 'Draw4_Change'), ('Draw2_Change', 'Draw3_Change'), ('Draw2_Change', 'Draw4_Change')]\n",
        "\n",
        "    # Dynamically generate interaction terms\n",
        "    for feature_a, feature_b in interactions:\n",
        "        if feature_a in data.columns and feature_b in data.columns:\n",
        "            interaction_feature_name = f'interaction_{feature_a}_{feature_b}'\n",
        "            data[interaction_feature_name] = data[feature_a] * data[feature_b]\n",
        "    return data\n",
        "\n",
        "# Update datasets with new interaction terms\n",
        "train_test_data = create_specific_interaction_terms(train_test_data.copy())\n",
        "unseen_data = create_specific_interaction_terms(unseen_data.copy())\n",
        "\n",
        "# Function to add polynomial features\n",
        "def add_polynomial_features(data, feature_list, degree=2):\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    valid_features = [feature for feature in feature_list if feature in data.columns]\n",
        "    poly_features = poly.fit_transform(data[valid_features])\n",
        "    feature_names = poly.get_feature_names_out(valid_features)\n",
        "    data_poly = pd.DataFrame(poly_features, columns=feature_names, index=data.index)\n",
        "    return pd.concat([data, data_poly], axis=1)\n",
        "\n",
        "# Function to add clustering features\n",
        "def add_clustering_features(data, feature_list, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    valid_features = [feature for feature in feature_list if feature in data.columns]\n",
        "    clusters = kmeans.fit_predict(data[valid_features])\n",
        "    data['cluster'] = clusters\n",
        "    return data\n",
        "\n",
        "# Apply polynomial and clustering features\n",
        "polynomial_feature_list = ['Draw1_Change', 'Draw2_Change', 'Draw3_Change', 'Draw4_Change']\n",
        "clustering_feature_list = ['DayofWeek']\n",
        "\n",
        "train_test_data = add_polynomial_features(train_test_data, polynomial_feature_list)\n",
        "unseen_data = add_polynomial_features(unseen_data, polynomial_feature_list)\n",
        "\n",
        "train_test_data = add_clustering_features(train_test_data, clustering_feature_list)\n",
        "unseen_data = add_clustering_features(unseen_data, clustering_feature_list)\n",
        "\n",
        "# Print the new columns after feature engineering\n",
        "print(\"\\nNew columns in DataFrame after feature engineering:\", train_test_data.columns)\n",
        "\n",
        "# Save the updated datasets with engineered features\n",
        "save_dataset(train_test_data, 'O_Polynomial_Features_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'O_Polynomial_Features_Unseen_Data.csv')\n",
        "\n",
        "# Check the shape of the processed datasets\n",
        "print(\"Shape of train_test_data after feature engineering:\", train_test_data.shape)\n",
        "print(\"Shape of unseen_data after feature engineering:\", unseen_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhIlJJUX0Lgp",
        "outputId": "272439dd-8623-4884-eaa2-d2b3fa89943f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original columns in DataFrame: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Mov_Avg_2',\n",
            "       ...\n",
            "       'Num_32_Interval_Last', 'Num_32_Cum_Count', 'Num_33_Interval_Last',\n",
            "       'Num_33_Cum_Count', 'Num_34_Interval_Last', 'Num_34_Cum_Count',\n",
            "       'Num_35_Interval_Last', 'Num_35_Cum_Count', 'Num_36_Interval_Last',\n",
            "       'Num_36_Cum_Count'],\n",
            "      dtype='object', length=166)\n",
            "\n",
            "New columns in DataFrame after feature engineering: Index(['Date', 'Row Number', 'Data_Type', 'Draw1', 'DR1_Prev_Week',\n",
            "       'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg',\n",
            "       'DR1_Mov_Avg_2',\n",
            "       ...\n",
            "       'Draw1_Change Draw2_Change', 'Draw1_Change Draw3_Change',\n",
            "       'Draw1_Change Draw4_Change', 'Draw2_Change^2',\n",
            "       'Draw2_Change Draw3_Change', 'Draw2_Change Draw4_Change',\n",
            "       'Draw3_Change^2', 'Draw3_Change Draw4_Change', 'Draw4_Change^2',\n",
            "       'cluster'],\n",
            "      dtype='object', length=186)\n",
            "Shape of train_test_data after feature engineering: (1409, 186)\n",
            "Shape of unseen_data after feature engineering: (105, 186)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.1: Identifying, defining, and dropping \"Sensitive\" columns to avoid data leakage\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save dataset function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load your datasets\n",
        "train_test_data = load_dataset('O_Polynomial_Features_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('O_Polynomial_Features_Unseen_Data.csv')\n",
        "\n",
        "# Function to drop sensitive columns\n",
        "def drop_sensitive_columns(df, columns_to_drop):\n",
        "    return df.drop(columns=columns_to_drop, inplace=False)\n",
        "\n",
        "# Function to keep only the specified columns in the dataset\n",
        "def keep_only(df, columns_to_keep):\n",
        "    return df[columns_to_keep]\n",
        "\n",
        "# List of sensitive columns to drop (replace with actual column names as needed)\n",
        "sensitive_columns_to_drop = ['Draw1', 'Draw2', 'Draw3', 'Draw4', 'DR2_Prev_Entry', 'DR3_Prev_Entry-2']\n",
        "\n",
        "# List of columns to keep (adjust as necessary)\n",
        "columns_to_keep = [\n",
        "    'Date', 'Row Number', 'Data_Type', 'DR1_Prev_Week', 'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2',\n",
        "    'DR1_Mov_Avg', 'DR1_Mov_Avg_2', 'Draw1_Moving_Avg_3', 'Draw1_Median_3', 'Draw1_Std_Dev_3', 'Draw1_RMS_3',\n",
        "    'Draw1_Rolling_Min_3', 'Draw1_Rolling_Max_3', 'Draw1_Skew_3', 'Draw1_Kurtosis_3', 'Draw1_EMA_3', 'Draw1_Rolling_Var_3',\n",
        "    'Draw1_Rolling_Range_3', 'Draw1_Moving_Avg_5', 'Draw1_Median_5', 'Draw1_Std_Dev_5', 'Draw1_RMS_5', 'Draw1_Rolling_Min_5',\n",
        "    'Draw1_Rolling_Max_5', 'Draw1_Skew_5', 'Draw1_Kurtosis_5', 'Draw1_EMA_5', 'Draw1_Rolling_Var_5', 'Draw1_Rolling_Range_5',\n",
        "    'Draw1_Moving_Avg_10', 'Draw1_Median_10', 'Draw1_Std_Dev_10', 'Draw1_RMS_10', 'Draw1_Rolling_Min_10', 'Draw1_Rolling_Max_10',\n",
        "    'Draw1_Skew_10', 'Draw1_Kurtosis_10', 'Draw1_EMA_10', 'Draw1_Rolling_Var_10', 'Draw1_Rolling_Range_10', 'Year', 'Month', 'Day',\n",
        "    'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night', 'Line_Prev_Entry', 'Line_PE_Num_1', 'Line_PE_Num_2',\n",
        "    'Line_PE_Num_3', 'Line_PE_Num_4', 'Spirit_PE_Num', 'Rake_PE_Num_1', 'Rake_PE_Num_2', 'Rake_PE_Num_3', 'Rake_PE_Num_4',\n",
        "    'DayofWeek', 'Draw1_Prev', 'Draw1_Change', 'Draw2_Prev', 'Draw2_Change', 'Draw3_Prev', 'Draw3_Change', 'Draw4_Prev',\n",
        "    'Draw4_Change', 'DayOfYear', 'Num_1_Interval_Last', 'Num_1_Cum_Count', 'Num_2_Interval_Last', 'Num_2_Cum_Count',\n",
        "    'Num_3_Interval_Last', 'Num_3_Cum_Count', 'Num_4_Interval_Last', 'Num_4_Cum_Count', 'Num_5_Interval_Last',\n",
        "    'Num_5_Cum_Count', 'Num_6_Interval_Last', 'Num_6_Cum_Count', 'Num_7_Interval_Last', 'Num_7_Cum_Count',\n",
        "    'Num_8_Interval_Last', 'Num_8_Cum_Count', 'Num_9_Interval_Last', 'Num_9_Cum_Count', 'Num_10_Interval_Last',\n",
        "    'Num_10_Cum_Count', 'Num_11_Interval_Last', 'Num_11_Cum_Count', 'Num_12_Interval_Last', 'Num_12_Cum_Count',\n",
        "    'Num_13_Interval_Last', 'Num_13_Cum_Count', 'Num_14_Interval_Last', 'Num_14_Cum_Count', 'Num_15_Interval_Last',\n",
        "    'Num_15_Cum_Count', 'Num_16_Interval_Last', 'Num_16_Cum_Count', 'Num_17_Interval_Last', 'Num_17_Cum_Count',\n",
        "    'Num_18_Interval_Last', 'Num_18_Cum_Count', 'Num_19_Interval_Last', 'Num_19_Cum_Count', 'Num_20_Interval_Last',\n",
        "    'Num_20_Cum_Count', 'Num_21_Interval_Last', 'Num_21_Cum_Count', 'Num_22_Interval_Last', 'Num_22_Cum_Count',\n",
        "    'Num_23_Interval_Last', 'Num_23_Cum_Count', 'Num_24_Interval_Last', 'Num_24_Cum_Count', 'Num_25_Interval_Last',\n",
        "    'Num_25_Cum_Count', 'Num_26_Interval_Last', 'Num_26_Cum_Count', 'Num_27_Interval_Last', 'Num_27_Cum_Count',\n",
        "    'Num_28_Interval_Last', 'Num_28_Cum_Count', 'Num_29_Interval_Last', 'Num_29_Cum_Count', 'Num_30_Interval_Last',\n",
        "    'Num_30_Cum_Count', 'Num_31_Interval_Last', 'Num_31_Cum_Count', 'Num_32_Interval_Last', 'Num_32_Cum_Count',\n",
        "    'Num_33_Interval_Last', 'Num_33_Cum_Count', 'Num_34_Interval_Last', 'Num_34_Cum_Count', 'Num_35_Interval_Last',\n",
        "    'Num_35_Cum_Count', 'Num_36_Interval_Last', 'Num_36_Cum_Count', 'Is_Special_Day', 'Is_Holiday', 'Is_Sunday'\n",
        "]\n",
        "\n",
        "# Drop sensitive columns for Training/Testing and Unseen Data\n",
        "train_test_data = drop_sensitive_columns(train_test_data, sensitive_columns_to_drop)\n",
        "unseen_data = drop_sensitive_columns(unseen_data, sensitive_columns_to_drop)\n",
        "\n",
        "# Apply 'Keep Only' for Training/Testing and Unseen Data\n",
        "train_test_data = keep_only(train_test_data, columns_to_keep)\n",
        "unseen_data = keep_only(unseen_data, columns_to_keep)\n",
        "\n",
        "# Save the datasets\n",
        "save_dataset(train_test_data, 'P_Keep_Only_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'P_Keep_Only_Unseen_Data.csv') # No need for to_frame conversion\n",
        "\n",
        "# Check the shape of the processed datasets\n",
        "print(\"Shape of train_test_data:\", train_test_data.shape)\n",
        "print(\"Shape of unseen_data:\", unseen_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SHp1abI3kub",
        "outputId": "b577eafd-8ffe-435a-8cf6-eeb3e85ef5ea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_test_data: (1409, 144)\n",
            "Shape of unseen_data: (105, 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6.2: Splitting Data into Training, Validation, and Test Sets - Chronological Split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save dataset function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load your datasets\n",
        "train_test_data = load_dataset('P_Keep_Only_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('P_Keep_Only_Unseen_Data.csv')\n",
        "\n",
        "# Sort by 'Date'\n",
        "train_test_data.sort_values(by=['Date'], inplace=True)\n",
        "\n",
        "# Separate the training/testing dataset into features and target\n",
        "X = train_test_data.drop(['Prediction1'], axis=1)\n",
        "y = train_test_data['Prediction1']\n",
        "\n",
        "# Calculate split indices\n",
        "train_size = int(len(X) * 0.7)\n",
        "val_size = int(len(X) * 0.15)\n",
        "test_size = len(X) - train_size - val_size\n",
        "\n",
        "# Split the dataset chronologically\n",
        "X_train, y_train = X.iloc[:train_size], y.iloc[:train_size]\n",
        "X_val, y_val = X.iloc[train_size:train_size+val_size], y.iloc[train_size:train_size+val_size]\n",
        "X_test, y_test = X.iloc[train_size+val_size:], pd.Series([0] * (len(X) - train_size - val_size))\n",
        "unseen_features = unseen_data.drop(['Prediction1'], axis=1)\n",
        "unseen_target = unseen_data['Prediction1']  # This line should exist before the save_dataset calls\n",
        "\n",
        "# Create actual results datasets for evaluation purposes\n",
        "actual_results_train = train_test_data.iloc[:train_size][['Date', 'Draw1']].copy()\n",
        "actual_results_val = train_test_data.iloc[train_size:train_size+val_size][['Date', 'Draw1']].copy()\n",
        "actual_results_test = train_test_data.iloc[train_size+val_size:][['Date', 'Draw1']].copy()\n",
        "actual_results_unseen = unseen_data[['Date', 'Draw1']].copy()\n",
        "\n",
        "# Save the datasets\n",
        "save_dataset(X_train, 'K_Train_Features.csv')\n",
        "save_dataset(y_train.to_frame('Prediction1'), 'K_Train_Target.csv')\n",
        "save_dataset(X_val, 'L_Val_Features.csv')\n",
        "save_dataset(y_val.to_frame('Prediction1'), 'L_Val_Target.csv')\n",
        "save_dataset(X_test, 'M_Test_Features.csv')\n",
        "save_dataset(y_test.to_frame('Prediction1'), 'M_Test_Target.csv')\n",
        "save_dataset(unseen_features, 'N_Unseen_Features.csv')\n",
        "save_dataset(unseen_target.to_frame('Prediction1'), 'N_Unseen_Target.csv')\n",
        "\n",
        "# Save the actual results datasets\n",
        "save_dataset(actual_results_train, 'Actual_Results_Train.csv')\n",
        "save_dataset(actual_results_val, 'Actual_Results_Val.csv')\n",
        "save_dataset(actual_results_test, 'Actual_Results_Test.csv')\n",
        "save_dataset(actual_results_unseen, 'Actual_Results_Unseen.csv')\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"Shape of unseen_features:\", unseen_features.shape)\n",
        "print(\"Shape of unseen_target:\", unseen_target.shape)\n"
      ],
      "metadata": {
        "id": "KJW6r5C3Bk3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5.1: Final checks and verifications.\n",
        "\n",
        "# Check for NaN values in train/test data\n",
        "print(\"NaN check for train/test data:\")\n",
        "print(train_test_data.isnull().sum())\n",
        "\n",
        "# Check data types in train/test data\n",
        "print(\"\\nData types in train/test data:\")\n",
        "print(train_test_data.dtypes)\n",
        "\n",
        "# Check for NaN values in unseen data\n",
        "print(\"\\nNaN check for unseen data:\")\n",
        "print(unseen_data.isnull().sum())\n",
        "\n",
        "# Check data types in unseen data\n",
        "print(\"\\nData types in unseen data:\")\n",
        "print(unseen_data.dtypes)\n"
      ],
      "metadata": {
        "id": "FPuNXxZPG2Rf",
        "outputId": "b748d7b0-65f4-43a6-e0db-d2e79f6c623c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN check for train/test data:\n",
            "Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        0\n",
            "Prev_Afternoon      0\n",
            "Prev_Evening        0\n",
            "Prev_Night          0\n",
            "Prediction1         0\n",
            "Line_Prev_Entry     0\n",
            "Line_PE_Num_1       0\n",
            "Line_PE_Num_2       0\n",
            "Line_PE_Num_3       0\n",
            "Line_PE_Num_4       0\n",
            "Spirit_PE_Num       0\n",
            "Rake_PE_Num_1       0\n",
            "Rake_PE_Num_2       0\n",
            "Rake_PE_Num_3       0\n",
            "Rake_PE_Num_4       0\n",
            "dtype: int64\n",
            "\n",
            "Data types in train/test data:\n",
            "Date                object\n",
            "Row Number           int64\n",
            "Data_Type           object\n",
            "Draw1                int64\n",
            "DR1_Prev_Week        int64\n",
            "DR1_2Weeks           int64\n",
            "DR1_Prev_Entry       int64\n",
            "DR1_Prev_Entry-2     int64\n",
            "DR1_Mov_Avg          int64\n",
            "DR1_Vert_Avg         int64\n",
            "Draw2                int64\n",
            "DR2_Prev_Week        int64\n",
            "DR2_2Weeks           int64\n",
            "DR2_Prev_Entry       int64\n",
            "DR2_Prev_Entry-2     int64\n",
            "DR2_Mov_Avg          int64\n",
            "DR2_Vert_Avg         int64\n",
            "Draw3                int64\n",
            "DR3_Prev_Week        int64\n",
            "DR3_2Weeks           int64\n",
            "DR3_Prev_Entry       int64\n",
            "DR3_Prev_Entry-2     int64\n",
            "DR3_Mov_Avg          int64\n",
            "DR3_Vert_Avg         int64\n",
            "Draw4                int64\n",
            "DR4_Prev_Week        int64\n",
            "DR4_2Weeks           int64\n",
            "DR4_Prev_Entry       int64\n",
            "DR4_Prev_Entry-2     int64\n",
            "DR4_Mov_Avg          int64\n",
            "DR4_Vert_Avg         int64\n",
            "Year                 int64\n",
            "Month                int64\n",
            "Day                  int64\n",
            "Prev_Morning         int64\n",
            "Prev_Afternoon       int64\n",
            "Prev_Evening         int64\n",
            "Prev_Night           int64\n",
            "Prediction1          int64\n",
            "Line_Prev_Entry      int64\n",
            "Line_PE_Num_1        int64\n",
            "Line_PE_Num_2        int64\n",
            "Line_PE_Num_3        int64\n",
            "Line_PE_Num_4        int64\n",
            "Spirit_PE_Num        int64\n",
            "Rake_PE_Num_1        int64\n",
            "Rake_PE_Num_2        int64\n",
            "Rake_PE_Num_3        int64\n",
            "Rake_PE_Num_4        int64\n",
            "dtype: object\n",
            "\n",
            "NaN check for unseen data:\n",
            "Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        0\n",
            "Prev_Afternoon      0\n",
            "Prev_Evening        0\n",
            "Prev_Night          0\n",
            "Prediction1         0\n",
            "Line_Prev_Entry     0\n",
            "Line_PE_Num_1       0\n",
            "Line_PE_Num_2       0\n",
            "Line_PE_Num_3       0\n",
            "Line_PE_Num_4       0\n",
            "Spirit_PE_Num       0\n",
            "Rake_PE_Num_1       0\n",
            "Rake_PE_Num_2       0\n",
            "Rake_PE_Num_3       0\n",
            "Rake_PE_Num_4       0\n",
            "dtype: int64\n",
            "\n",
            "Data types in unseen data:\n",
            "Date                object\n",
            "Row Number           int64\n",
            "Data_Type           object\n",
            "Draw1                int64\n",
            "DR1_Prev_Week        int64\n",
            "DR1_2Weeks           int64\n",
            "DR1_Prev_Entry       int64\n",
            "DR1_Prev_Entry-2     int64\n",
            "DR1_Mov_Avg          int64\n",
            "DR1_Vert_Avg         int64\n",
            "Draw2                int64\n",
            "DR2_Prev_Week        int64\n",
            "DR2_2Weeks           int64\n",
            "DR2_Prev_Entry       int64\n",
            "DR2_Prev_Entry-2     int64\n",
            "DR2_Mov_Avg          int64\n",
            "DR2_Vert_Avg         int64\n",
            "Draw3                int64\n",
            "DR3_Prev_Week        int64\n",
            "DR3_2Weeks           int64\n",
            "DR3_Prev_Entry       int64\n",
            "DR3_Prev_Entry-2     int64\n",
            "DR3_Mov_Avg          int64\n",
            "DR3_Vert_Avg         int64\n",
            "Draw4                int64\n",
            "DR4_Prev_Week        int64\n",
            "DR4_2Weeks           int64\n",
            "DR4_Prev_Entry       int64\n",
            "DR4_Prev_Entry-2     int64\n",
            "DR4_Mov_Avg          int64\n",
            "DR4_Vert_Avg         int64\n",
            "Year                 int64\n",
            "Month                int64\n",
            "Day                  int64\n",
            "Prev_Morning         int64\n",
            "Prev_Afternoon       int64\n",
            "Prev_Evening         int64\n",
            "Prev_Night           int64\n",
            "Prediction1          int64\n",
            "Line_Prev_Entry      int64\n",
            "Line_PE_Num_1        int64\n",
            "Line_PE_Num_2        int64\n",
            "Line_PE_Num_3        int64\n",
            "Line_PE_Num_4        int64\n",
            "Spirit_PE_Num        int64\n",
            "Rake_PE_Num_1        int64\n",
            "Rake_PE_Num_2        int64\n",
            "Rake_PE_Num_3        int64\n",
            "Rake_PE_Num_4        int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the first copy to the first directory\n",
        "train_test_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/S_Final_Train_Test_Data.csv', index=False)\n",
        "unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Initial_Data_Prep/T_Final_Unseen_Data.csv', index=False)\n",
        "\n",
        "# Save the second copy to the second directory AS INITIAL DATASETS FOR DRAW 1 PREDICTIVE SCRIPT\n",
        "train_test_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/A_Initial_Train_Test_Data.csv', index=False)\n",
        "unseen_data.to_csv('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/B_Initial_Unseen_Data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "lixNbXei4ghR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# :::::**THE END** *Thank You*:::::"
      ],
      "metadata": {
        "id": "c7irQ4xrLOIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Last Revision***\n",
        "***04/02/24***\n",
        "11:24pm"
      ],
      "metadata": {
        "id": "PRJWR0NLS1Ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Revision***\n",
        "27/3/24\n",
        "*Updated Unseen Results up to 30/11/23*"
      ],
      "metadata": {
        "id": "jauzzY0jdZzZ"
      }
    }
  ]
}