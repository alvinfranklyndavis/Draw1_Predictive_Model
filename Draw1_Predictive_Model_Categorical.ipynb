{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinfranklyndavis/Draw1_Predictive_Model/blob/main/Draw1_Predictive_Model_Categorical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Plnc-ffhAUCk",
        "outputId": "1c5b82d3-3d96-4932-ebe2-c77480e8dd79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting shap\n",
            "  Downloading shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting black\n",
            "  Downloading black-24.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.7/535.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading black-24.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tzdata, slicer, pathspec, numpy, mypy-extensions, pandas, black, scikit-learn, matplotlib, gdown, shap, imbalanced-learn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.10.1\n",
            "    Uninstalling imbalanced-learn-0.10.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.21.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed black-24.2.0 gdown-5.1.0 imbalanced-learn-0.12.0 matplotlib-3.8.3 mypy-extensions-1.0.0 numpy-1.26.4 pandas-2.2.1 pathspec-0.12.1 scikit-learn-1.4.1.post1 shap-0.44.1 slicer-0.0.7 tzdata-2024.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: seaborn\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "Successfully installed seaborn-0.13.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# CELL 1.1: Package Installation\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -U pip\n",
        "!pip install -U pandas gdown numpy matplotlib scikit-learn xgboost shap imbalanced-learn black\n",
        "!pip install --upgrade numpy seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S821bxd_vm7O"
      },
      "outputs": [],
      "source": [
        "# CELL 1.2: Cloning GitHub Repository and Setup\n",
        "\n",
        "#import os\n",
        "\n",
        "# Git Configuration (Run this only once)\n",
        "#!git config --global user.name \"alvinfranklyndavis\"\n",
        "#!git config --global user.email \"alvinfranklyndavis@gmail.com\"\n",
        "\n",
        "# Clone the new repository (Run this only once)\n",
        "#repository_path = '/content/Draw1_Predictive_Model_Jan_24'\n",
        "#if not os.path.exists(repository_path):\n",
        "#    !git clone https://github.com/alvinfranklyndavis/Draw1_Predictive_Model_Jan_24.git\n",
        "#%cd Draw1_Predictive_Model_Jan_24\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFs8Zxfd42Tf",
        "outputId": "88f1cb4a-d81b-4c57-837e-c6a033699f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# CELL 1.3: Mount Google Drive for GitHub Repository Push in subsequent Cell\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory path for loading and saving\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Then, run your GitHub cells for cloning, adding, committing, and pushing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTmObExQtzyZ"
      },
      "source": [
        "# GitHub Backup\n",
        "\n",
        "1. **Review and commit changes:** Check your changes and commit them using the Git commands below.\n",
        "2. **Push to GitHub:**  Run the cell below to push your committed changes to the repository\n",
        "![GitHub Octocat.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7QG_qT43Hln",
        "outputId": "1665d682-4654-451a-9379-876eb6bfa333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Draw1_Predictive_Model_Jan_24'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "[Errno 2] No such file or directory: 'Draw1_Predictive_Model_Jan_24'\n",
            "/content\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# CELL 1.4: Manual push to your new GitHub repository\n",
        "\n",
        "# Ensure we're in the /content directory\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if the repository directory exists and change into it\n",
        "repo_name = 'Draw1_Predictive_Model_Jan_24'\n",
        "if not os.path.exists(repo_name):\n",
        "    # If the repository doesn't exist, clone it\n",
        "    !git clone https://github.com/alvinfranklyndavis/Draw1_Predictive_Model_Jan_24.git\n",
        "%cd $repo_name\n",
        "\n",
        "# Verify if the repository is correctly initialized\n",
        "!git status\n",
        "\n",
        "# Copy the notebook from its original location to the repository directory\n",
        "notebook_path = '/content/drive/My Drive/Colab Notebooks/Draw1_Predictive_Model_Jan_24.ipynb'\n",
        "!cp \"$notebook_path\" .\n",
        "\n",
        "# Add the notebook to the staging area and commit\n",
        "!git add Draw1_Predictive_Model_Jan_24.ipynb\n",
        "!git commit -m \"Update notebook\"\n",
        "\n",
        "# Set up Git to use your PAT for authentication\n",
        "pat = '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'  # Replace with your actual PAT\n",
        "username = 'alvinfranklyndavis'  # Your GitHub username\n",
        "repository_url = 'github.com/alvinfranklyndavis/Draw1_Predictive_Model_Jan_24.git'\n",
        "remote_url = f'https://{username}:{pat}@{repository_url}'\n",
        "!git remote set-url origin $remote_url\n",
        "\n",
        "# Push the changes to GitHub\n",
        "!git push -u origin main\n",
        "\n",
        "# Change back to the /content directory\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP_Q74gUBGQi",
        "outputId": "cb10b5b4-143c-4b36-9ec7-26179d887d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset 'A_Initial_Train_Test_Data.csv' loaded successfully.\n",
            "Dataset 'B_Initial_Unseen_Data.csv' loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# CELL 2.1: Loading and Inspecting Data\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory path for loading and saving\n",
        "base_dir = Path('/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/')\n",
        "\n",
        "def load_dataset(filename, verbose=True):\n",
        "    \"\"\"\n",
        "    Loads a dataset from a specified filename within the base directory.\n",
        "\n",
        "    Parameters:\n",
        "    - filename: str, name of the file to load (including .csv extension).\n",
        "    - verbose: bool, optional, default True. Prints a message upon successful loading.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame loaded from the CSV file.\n",
        "    \"\"\"\n",
        "    full_path = base_dir / filename\n",
        "    try:\n",
        "        data = pd.read_csv(full_path)\n",
        "        if verbose:\n",
        "            print(f\"Dataset '{filename}' loaded successfully.\")\n",
        "        return data\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. File '{filename}' not found in the directory '{base_dir}'.\")\n",
        "\n",
        "def save_dataset(df, filename, verbose=True):\n",
        "    \"\"\"\n",
        "    Saves a DataFrame to a CSV file within the base directory.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame, the DataFrame to save.\n",
        "    - filename: str, name of the file to save (should include .csv extension).\n",
        "    - verbose: bool, optional, default True. Prints a message upon successful saving.\n",
        "    \"\"\"\n",
        "    if not filename.endswith('.csv'):\n",
        "        filename += '.csv'\n",
        "    full_path = base_dir / filename\n",
        "    try:\n",
        "        df.to_csv(full_path, index=False)\n",
        "        if verbose:\n",
        "            print(f\"Dataset saved successfully as '{filename}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving the dataset: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "# Load datasets\n",
        "train_test_data = load_dataset('A_Initial_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('B_Initial_Unseen_Data.csv')\n",
        "\n",
        "# Optionally save a dataset\n",
        "# save_dataset(train_test_data, 'Example_Save.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up0qBbUJvkKe",
        "outputId": "fcdf9b46-e839-481e-fe3a-3171f75f349b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of Training/Testing Data:\n",
            "          Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2018-08-01           1  Training     19              7          27   \n",
            "1  2018-08-02           2  Training     31             11           1   \n",
            "2  2018-08-03           3  Training     15             19          21   \n",
            "3  2018-08-04           4  Training     31             35          18   \n",
            "4  2018-08-05           5         0      0              0           0   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0              23                32           27            17  ...   \n",
            "1               9                33           21             6  ...   \n",
            "2              12                35           23            20  ...   \n",
            "3              35                23           29            26  ...   \n",
            "4               0                 0            0             0  ...   \n",
            "\n",
            "   Line_Prev_Entry  Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  \\\n",
            "0                5              5             14             23   \n",
            "1                9              9             18             27   \n",
            "2                3              3             12             21   \n",
            "3                8              8             17             26   \n",
            "4                0              0              0              0   \n",
            "\n",
            "   Line_PE_Num_4  Spirit_PE_Num  Rake_PE_Num_1  Rake_PE_Num_2  Rake_PE_Num_3  \\\n",
            "0             32             21             12             14             16   \n",
            "1             36             33              7             11             24   \n",
            "2             30             32              1             10             28   \n",
            "3             35              4              3             12             14   \n",
            "4              0              0              0              0              0   \n",
            "\n",
            "   Rake_PE_Num_4  \n",
            "0             20  \n",
            "1             32  \n",
            "2             35  \n",
            "3             16  \n",
            "4              0  \n",
            "\n",
            "[5 rows x 49 columns]\n",
            "\n",
            "First 5 rows of Unseen Data:\n",
            "          Date  Row Number Data_Type  Draw1  DR1_Prev_Week  DR1_2Weeks  \\\n",
            "0  2023-08-01        1410    Unseen     13             27          25   \n",
            "1  2023-08-02        1411    Unseen     21             33          12   \n",
            "2  2023-08-03        1412    Unseen     15             27           3   \n",
            "3  2023-08-04        1413    Unseen     13             20          11   \n",
            "4  2023-08-05        1414    Unseen     12             29          14   \n",
            "\n",
            "   DR1_Prev_Entry  DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg  ...  \\\n",
            "0               5                 7            6            26  ...   \n",
            "1              18                26           22            22  ...   \n",
            "2              28                 7           17            15  ...   \n",
            "3               2                 2            2            15  ...   \n",
            "4              12                22           17            21  ...   \n",
            "\n",
            "   Line_Prev_Entry  Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  \\\n",
            "0                5              5             14             23   \n",
            "1                9              9             18             27   \n",
            "2                1              1             10             19   \n",
            "3                2              2             11             20   \n",
            "4                3              3             12             21   \n",
            "\n",
            "   Line_PE_Num_4  Spirit_PE_Num  Rake_PE_Num_1  Rake_PE_Num_2  Rake_PE_Num_3  \\\n",
            "0             32              1             20             23             31   \n",
            "1             36             30              5              7             28   \n",
            "2             28             10             12             13             18   \n",
            "3             29             24              4             14             16   \n",
            "4             30             32              1             10             28   \n",
            "\n",
            "   Rake_PE_Num_4  \n",
            "0             33  \n",
            "1             33  \n",
            "2             33  \n",
            "3             23  \n",
            "4             35  \n",
            "\n",
            "[5 rows x 49 columns]\n",
            "\n",
            "Shape of Training/Testing Data: (1672, 49)\n",
            "Shape of Unseen Data: (31, 49)\n",
            "\n",
            "Summary Statistics of Training/Testing Data:\n",
            "         Row Number        Draw1  DR1_Prev_Week   DR1_2Weeks  DR1_Prev_Entry  \\\n",
            "count  1672.000000  1672.000000    1672.000000  1672.000000     1672.000000   \n",
            "mean    836.500000    15.814593      15.779904    15.794856       15.271531   \n",
            "std     482.809141    11.648670      11.640151    11.642866       11.583556   \n",
            "min       1.000000     0.000000       0.000000     0.000000        0.000000   \n",
            "25%     418.750000     5.000000       5.000000     5.000000        4.000000   \n",
            "50%     836.500000    16.000000      16.000000    16.000000       15.000000   \n",
            "75%    1254.250000    26.000000      26.000000    26.000000       25.000000   \n",
            "max    1672.000000    36.000000      36.000000    36.000000       36.000000   \n",
            "\n",
            "       DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg        Draw2  \\\n",
            "count       1672.000000  1672.000000   1672.000000  1672.000000   \n",
            "mean          15.627990    15.248804     15.575359    15.686005   \n",
            "std           11.576163     9.341339      9.516388    11.675960   \n",
            "min            0.000000     0.000000      0.000000     0.000000   \n",
            "25%            5.000000     9.000000      9.000000     5.000000   \n",
            "50%           15.000000    16.000000     16.000000    15.000000   \n",
            "75%           25.000000    22.000000     23.000000    26.000000   \n",
            "max           36.000000    36.000000     36.000000    36.000000   \n",
            "\n",
            "       DR2_Prev_Week  ...  Line_Prev_Entry  Line_PE_Num_1  Line_PE_Num_2  \\\n",
            "count    1672.000000  ...      1672.000000    1672.000000    1672.000000   \n",
            "mean       15.712919  ...         4.247608       4.247608      11.831938   \n",
            "std        11.680727  ...         3.004235       3.004235       5.639360   \n",
            "min         0.000000  ...         0.000000       0.000000       0.000000   \n",
            "25%         5.000000  ...         1.000000       1.000000      10.000000   \n",
            "50%        15.000000  ...         4.000000       4.000000      13.000000   \n",
            "75%        26.000000  ...         7.000000       7.000000      16.000000   \n",
            "max        36.000000  ...         9.000000       9.000000      18.000000   \n",
            "\n",
            "       Line_PE_Num_3  Line_PE_Num_4  Spirit_PE_Num  Rake_PE_Num_1  \\\n",
            "count    1672.000000    1672.000000    1672.000000    1672.000000   \n",
            "mean       19.416268      27.000598      15.946172       6.678230   \n",
            "std         8.721582      11.908664      11.788003       5.673908   \n",
            "min         0.000000       0.000000       0.000000       0.000000   \n",
            "25%        19.000000      28.000000       4.750000       1.000000   \n",
            "50%        22.000000      31.000000      16.000000       5.000000   \n",
            "75%        25.000000      34.000000      27.000000      11.000000   \n",
            "max        27.000000      36.000000      36.000000      20.000000   \n",
            "\n",
            "       Rake_PE_Num_2  Rake_PE_Num_3  Rake_PE_Num_4  \n",
            "count    1672.000000    1672.000000    1672.000000  \n",
            "mean       12.014354      18.827153      24.989234  \n",
            "std         8.570462      10.542299      12.266095  \n",
            "min         0.000000       0.000000       0.000000  \n",
            "25%         6.000000      14.000000      20.000000  \n",
            "50%        12.000000      19.000000      29.500000  \n",
            "75%        16.000000      28.000000      35.000000  \n",
            "max        31.000000      35.000000      36.000000  \n",
            "\n",
            "[8 rows x 47 columns]\n",
            "\n",
            "Summary Statistics of Unseen Data:\n",
            "         Row Number      Draw1  DR1_Prev_Week  DR1_2Weeks  DR1_Prev_Entry  \\\n",
            "count    31.000000  31.000000      31.000000   31.000000       31.000000   \n",
            "mean   1425.000000  15.225806      17.645161   16.419355       15.290323   \n",
            "std       9.092121  10.203626      10.756543   10.965930       11.713791   \n",
            "min    1410.000000   0.000000       0.000000    0.000000        0.000000   \n",
            "25%    1417.500000  10.500000      12.000000   10.500000        4.500000   \n",
            "50%    1425.000000  13.000000      17.000000   14.000000       17.000000   \n",
            "75%    1432.500000  21.000000      26.000000   25.000000       25.000000   \n",
            "max    1440.000000  36.000000      36.000000   36.000000       33.000000   \n",
            "\n",
            "       DR1_Prev_Entry-2  DR1_Mov_Avg  DR1_Vert_Avg      Draw2  DR2_Prev_Week  \\\n",
            "count         31.000000    31.000000     31.000000  31.000000      31.000000   \n",
            "mean          15.870968    15.419355     16.806452  17.548387      16.483871   \n",
            "std           11.348251    10.281939      7.939015  12.803225      12.277000   \n",
            "min            0.000000     0.000000      0.000000   0.000000       0.000000   \n",
            "25%            4.500000     4.500000     15.000000   4.500000       4.500000   \n",
            "50%           18.000000    17.000000     18.000000  21.000000      20.000000   \n",
            "75%           25.500000    23.500000     22.000000  28.500000      27.500000   \n",
            "max           31.000000    32.000000     28.000000  35.000000      35.000000   \n",
            "\n",
            "       ...  Line_Prev_Entry  Line_PE_Num_1  Line_PE_Num_2  Line_PE_Num_3  \\\n",
            "count  ...        31.000000      31.000000      31.000000      31.000000   \n",
            "mean   ...         4.258065       4.258065      12.096774      19.935484   \n",
            "std    ...         2.909384       2.909384       5.300030       8.156533   \n",
            "min    ...         0.000000       0.000000       0.000000       0.000000   \n",
            "25%    ...         2.000000       2.000000      11.000000      20.000000   \n",
            "50%    ...         5.000000       5.000000      14.000000      23.000000   \n",
            "75%    ...         6.500000       6.500000      15.500000      24.500000   \n",
            "max    ...         9.000000       9.000000      18.000000      27.000000   \n",
            "\n",
            "       Line_PE_Num_4  Spirit_PE_Num  Rake_PE_Num_1  Rake_PE_Num_2  \\\n",
            "count      31.000000      31.000000      31.000000      31.000000   \n",
            "mean       27.774194      15.129032       6.806452      11.677419   \n",
            "std        11.125675      11.669738       5.861850       8.154292   \n",
            "min         0.000000       0.000000       0.000000       0.000000   \n",
            "25%        29.000000       6.500000       4.000000       7.000000   \n",
            "50%        32.000000      14.000000       5.000000      11.000000   \n",
            "75%        33.500000      23.000000      10.500000      14.000000   \n",
            "max        36.000000      36.000000      20.000000      31.000000   \n",
            "\n",
            "       Rake_PE_Num_3  Rake_PE_Num_4  \n",
            "count      31.000000      31.000000  \n",
            "mean       19.451613      25.451613  \n",
            "std        10.375737      11.629958  \n",
            "min         0.000000       0.000000  \n",
            "25%        14.000000      20.000000  \n",
            "50%        17.000000      30.000000  \n",
            "75%        28.000000      33.000000  \n",
            "max        35.000000      36.000000  \n",
            "\n",
            "[8 rows x 47 columns]\n",
            "\n",
            "Data Types in Training/Testing Data:\n",
            " Date                object\n",
            "Row Number           int64\n",
            "Data_Type           object\n",
            "Draw1                int64\n",
            "DR1_Prev_Week        int64\n",
            "DR1_2Weeks           int64\n",
            "DR1_Prev_Entry       int64\n",
            "DR1_Prev_Entry-2     int64\n",
            "DR1_Mov_Avg          int64\n",
            "DR1_Vert_Avg         int64\n",
            "Draw2                int64\n",
            "DR2_Prev_Week        int64\n",
            "DR2_2Weeks           int64\n",
            "DR2_Prev_Entry       int64\n",
            "DR2_Prev_Entry-2     int64\n",
            "DR2_Mov_Avg          int64\n",
            "DR2_Vert_Avg         int64\n",
            "Draw3                int64\n",
            "DR3_Prev_Week        int64\n",
            "DR3_2Weeks           int64\n",
            "DR3_Prev_Entry       int64\n",
            "DR3_Prev_Entry-2     int64\n",
            "DR3_Mov_Avg          int64\n",
            "DR3_Vert_Avg         int64\n",
            "Draw4                int64\n",
            "DR4_Prev_Week        int64\n",
            "DR4_2Weeks           int64\n",
            "DR4_Prev_Entry       int64\n",
            "DR4_Prev_Entry-2     int64\n",
            "DR4_Mov_Avg          int64\n",
            "DR4_Vert_Avg         int64\n",
            "Year                 int64\n",
            "Month                int64\n",
            "Day                  int64\n",
            "Prev_Morning         int64\n",
            "Prev_Afternoon       int64\n",
            "Prev_Evening         int64\n",
            "Prev_Night           int64\n",
            "Prediction1          int64\n",
            "Line_Prev_Entry      int64\n",
            "Line_PE_Num_1        int64\n",
            "Line_PE_Num_2        int64\n",
            "Line_PE_Num_3        int64\n",
            "Line_PE_Num_4        int64\n",
            "Spirit_PE_Num        int64\n",
            "Rake_PE_Num_1        int64\n",
            "Rake_PE_Num_2        int64\n",
            "Rake_PE_Num_3        int64\n",
            "Rake_PE_Num_4        int64\n",
            "dtype: object\n",
            "\n",
            "Data Types in Unseen Data:\n",
            " Date                object\n",
            "Row Number           int64\n",
            "Data_Type           object\n",
            "Draw1                int64\n",
            "DR1_Prev_Week        int64\n",
            "DR1_2Weeks           int64\n",
            "DR1_Prev_Entry       int64\n",
            "DR1_Prev_Entry-2     int64\n",
            "DR1_Mov_Avg          int64\n",
            "DR1_Vert_Avg         int64\n",
            "Draw2                int64\n",
            "DR2_Prev_Week        int64\n",
            "DR2_2Weeks           int64\n",
            "DR2_Prev_Entry       int64\n",
            "DR2_Prev_Entry-2     int64\n",
            "DR2_Mov_Avg          int64\n",
            "DR2_Vert_Avg         int64\n",
            "Draw3                int64\n",
            "DR3_Prev_Week        int64\n",
            "DR3_2Weeks           int64\n",
            "DR3_Prev_Entry       int64\n",
            "DR3_Prev_Entry-2     int64\n",
            "DR3_Mov_Avg          int64\n",
            "DR3_Vert_Avg         int64\n",
            "Draw4                int64\n",
            "DR4_Prev_Week        int64\n",
            "DR4_2Weeks           int64\n",
            "DR4_Prev_Entry       int64\n",
            "DR4_Prev_Entry-2     int64\n",
            "DR4_Mov_Avg          int64\n",
            "DR4_Vert_Avg         int64\n",
            "Year                 int64\n",
            "Month                int64\n",
            "Day                  int64\n",
            "Prev_Morning         int64\n",
            "Prev_Afternoon       int64\n",
            "Prev_Evening         int64\n",
            "Prev_Night           int64\n",
            "Prediction1          int64\n",
            "Line_Prev_Entry      int64\n",
            "Line_PE_Num_1        int64\n",
            "Line_PE_Num_2        int64\n",
            "Line_PE_Num_3        int64\n",
            "Line_PE_Num_4        int64\n",
            "Spirit_PE_Num        int64\n",
            "Rake_PE_Num_1        int64\n",
            "Rake_PE_Num_2        int64\n",
            "Rake_PE_Num_3        int64\n",
            "Rake_PE_Num_4        int64\n",
            "dtype: object\n",
            "\n",
            "Missing values in Training/Testing Data:\n",
            " Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        0\n",
            "Prev_Afternoon      0\n",
            "Prev_Evening        0\n",
            "Prev_Night          0\n",
            "Prediction1         0\n",
            "Line_Prev_Entry     0\n",
            "Line_PE_Num_1       0\n",
            "Line_PE_Num_2       0\n",
            "Line_PE_Num_3       0\n",
            "Line_PE_Num_4       0\n",
            "Spirit_PE_Num       0\n",
            "Rake_PE_Num_1       0\n",
            "Rake_PE_Num_2       0\n",
            "Rake_PE_Num_3       0\n",
            "Rake_PE_Num_4       0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in Unseen Data:\n",
            " Date                0\n",
            "Row Number          0\n",
            "Data_Type           0\n",
            "Draw1               0\n",
            "DR1_Prev_Week       0\n",
            "DR1_2Weeks          0\n",
            "DR1_Prev_Entry      0\n",
            "DR1_Prev_Entry-2    0\n",
            "DR1_Mov_Avg         0\n",
            "DR1_Vert_Avg        0\n",
            "Draw2               0\n",
            "DR2_Prev_Week       0\n",
            "DR2_2Weeks          0\n",
            "DR2_Prev_Entry      0\n",
            "DR2_Prev_Entry-2    0\n",
            "DR2_Mov_Avg         0\n",
            "DR2_Vert_Avg        0\n",
            "Draw3               0\n",
            "DR3_Prev_Week       0\n",
            "DR3_2Weeks          0\n",
            "DR3_Prev_Entry      0\n",
            "DR3_Prev_Entry-2    0\n",
            "DR3_Mov_Avg         0\n",
            "DR3_Vert_Avg        0\n",
            "Draw4               0\n",
            "DR4_Prev_Week       0\n",
            "DR4_2Weeks          0\n",
            "DR4_Prev_Entry      0\n",
            "DR4_Prev_Entry-2    0\n",
            "DR4_Mov_Avg         0\n",
            "DR4_Vert_Avg        0\n",
            "Year                0\n",
            "Month               0\n",
            "Day                 0\n",
            "Prev_Morning        0\n",
            "Prev_Afternoon      0\n",
            "Prev_Evening        0\n",
            "Prev_Night          0\n",
            "Prediction1         0\n",
            "Line_Prev_Entry     0\n",
            "Line_PE_Num_1       0\n",
            "Line_PE_Num_2       0\n",
            "Line_PE_Num_3       0\n",
            "Line_PE_Num_4       0\n",
            "Spirit_PE_Num       0\n",
            "Rake_PE_Num_1       0\n",
            "Rake_PE_Num_2       0\n",
            "Rake_PE_Num_3       0\n",
            "Rake_PE_Num_4       0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate rows in Training/Testing Data: 0\n",
            "Duplicate rows in Unseen Data: 0\n"
          ]
        }
      ],
      "source": [
        "# CELL 2.2: Surveillance checks on both datasets\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(base_dir, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save a dataset\n",
        "def save_dataset(df, base_dir, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Load datasets\n",
        "train_test_data = load_dataset(base_dir, 'A_Initial_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset(base_dir, 'B_Initial_Unseen_Data.csv')\n",
        "\n",
        "# Print the first few rows of the datasets\n",
        "print(\"First 5 rows of Training/Testing Data:\\n\", train_test_data.head())\n",
        "print(\"\\nFirst 5 rows of Unseen Data:\\n\", unseen_data.head())\n",
        "\n",
        "# Dataset dimensions\n",
        "print(\"\\nShape of Training/Testing Data:\", train_test_data.shape)\n",
        "print(\"Shape of Unseen Data:\", unseen_data.shape)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics of Training/Testing Data:\\n\", train_test_data.describe())\n",
        "print(\"\\nSummary Statistics of Unseen Data:\\n\", unseen_data.describe())\n",
        "\n",
        "# Data types of columns\n",
        "print(\"\\nData Types in Training/Testing Data:\\n\", train_test_data.dtypes)\n",
        "print(\"\\nData Types in Unseen Data:\\n\", unseen_data.dtypes)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in Training/Testing Data:\\n\", train_test_data.isnull().sum())\n",
        "print(\"\\nMissing values in Unseen Data:\\n\", unseen_data.isnull().sum())\n",
        "\n",
        "# Check for duplicate rows\n",
        "print(\"\\nDuplicate rows in Training/Testing Data:\", train_test_data.duplicated().sum())\n",
        "print(\"Duplicate rows in Unseen Data:\", unseen_data.duplicated().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2.3: Exemption of Sundays and Public Holidays\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save datasets function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the initial datasets\n",
        "train_test_data = load_dataset('A_Initial_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('B_Initial_Unseen_Data.csv')\n",
        "\n",
        "# Ensure Date Column is in DateTime Format for both datasets\n",
        "train_test_data['Date'] = pd.to_datetime(train_test_data['Date'])\n",
        "unseen_data['Date'] = pd.to_datetime(unseen_data['Date'])\n",
        "\n",
        "# Define the list of public holidays\n",
        "public_holidays_list = [\n",
        "    \"2018-03-30\", \"2018-05-31\", \"2018-06-15\", \"2018-11-06\", \"2018-12-25\",\n",
        "    \"2019-03-30\", \"2019-04-19\", \"2019-06-05\", \"2019-06-20\", \"2019-12-25\",\n",
        "    \"2020-03-30\", \"2020-04-10\", \"2020-05-24\", \"2020-06-11\", \"2020-11-14\", \"2020-12-25\",\n",
        "    \"2021-03-30\", \"2021-04-02\", \"2021-05-13\", \"2021-06-03\", \"2021-11-04\", \"2021-12-25\",\n",
        "    \"2022-03-30\", \"2022-04-15\", \"2022-05-02\", \"2022-06-16\", \"2022-10-24\", \"2022-12-26\",\n",
        "    \"2023-03-30\", \"2023-04-07\", \"2023-04-22\", \"2023-06-08\", \"2023-11-12\", \"2023-12-25\",\n",
        "    \"2024-03-29\", \"2024-03-30\", \"2024-04-10\", \"2024-05-30\", \"2024-10-31\", \"2024-12-25\"\n",
        "]\n",
        "\n",
        "# Convert public holidays list to datetime for comparison\n",
        "public_holidays = pd.to_datetime(public_holidays_list)\n",
        "\n",
        "# Flag public holidays in the datasets\n",
        "train_test_data['Is_Holiday'] = train_test_data['Date'].isin(public_holidays)\n",
        "unseen_data['Is_Holiday'] = unseen_data['Date'].isin(public_holidays)\n",
        "\n",
        "# Flag Sundays as special days alongside public holidays\n",
        "train_test_data['Is_Sunday'] = train_test_data['Date'].dt.dayofweek == 6\n",
        "unseen_data['Is_Sunday'] = unseen_data['Date'].dt.dayofweek == 6\n",
        "\n",
        "# Combine flags to identify any special day\n",
        "train_test_data['Is_Special_Day'] = train_test_data['Is_Holiday'] | train_test_data['Is_Sunday']\n",
        "unseen_data['Is_Special_Day'] = unseen_data['Is_Holiday'] | unseen_data['Is_Sunday']\n",
        "\n",
        "# Debugging - Print to verify flags for a subset\n",
        "print(train_test_data[['Date', 'Is_Special_Day', 'Is_Holiday', 'Is_Sunday']].head(20))\n",
        "\n",
        "# Since the logic for 'DR1_2Weeks' and similar calculations would need actual implementation,\n",
        "# we'll skip that part for this script to focus on special days handling.\n",
        "\n",
        "# Save the enhanced datasets\n",
        "save_dataset(train_test_data, 'C_Special_Days_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'D_Special_Days_Unseen_Data.csv')\n",
        "print(\"Enhanced datasets saved successfully.\")\n"
      ],
      "metadata": {
        "id": "4jRo5CYpd5B3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd36592-6dd4-40ae-e842-9b66d8847511"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date  Is_Special_Day  Is_Holiday  Is_Sunday\n",
            "0  2018-08-01           False       False      False\n",
            "1  2018-08-02           False       False      False\n",
            "2  2018-08-03           False       False      False\n",
            "3  2018-08-04           False       False      False\n",
            "4  2018-08-05            True       False       True\n",
            "5  2018-08-06           False       False      False\n",
            "6  2018-08-07           False       False      False\n",
            "7  2018-08-08           False       False      False\n",
            "8  2018-08-09           False       False      False\n",
            "9  2018-08-10           False       False      False\n",
            "10 2018-08-11           False       False      False\n",
            "11 2018-08-12            True       False       True\n",
            "12 2018-08-13           False       False      False\n",
            "13 2018-08-14           False       False      False\n",
            "14 2018-08-15           False       False      False\n",
            "15 2018-08-16           False       False      False\n",
            "16 2018-08-17           False       False      False\n",
            "17 2018-08-18           False       False      False\n",
            "18 2018-08-19            True       False       True\n",
            "19 2018-08-20           False       False      False\n",
            "Enhanced datasets saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2.4: Logic Functions for handling Sundays and Public Holidays\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Function to load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save datasets\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the initial datasets\n",
        "train_test_data = load_dataset('C_Special_Days_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('D_Special_Days_Unseen_Data.csv')\n",
        "\n",
        "# Ensure Date Column is in DateTime Format for both datasets\n",
        "train_test_data['Date'] = pd.to_datetime(train_test_data['Date'])\n",
        "unseen_data['Date'] = pd.to_datetime(unseen_data['Date'])\n",
        "\n",
        "# Define additional columns for forward fill\n",
        "additional_columns_to_forward_fill = [\n",
        "    'Line_Prev_Entry', 'Line_PE_Num_1', 'Line_PE_Num_2', 'Line_PE_Num_3', 'Line_PE_Num_4',\n",
        "    'Spirit_PE_Num', 'Rake_PE_Num_1', 'Rake_PE_Num_2', 'Rake_PE_Num_3', 'Rake_PE_Num_4'\n",
        "]\n",
        "\n",
        "# Combine with existing 'Prev_' columns\n",
        "columns_to_forward_fill.extend(additional_columns_to_forward_fill)\n",
        "\n",
        "for col in columns_to_forward_fill:\n",
        "    # First, temporarily replace 0 with NaN to selectively forward fill zeros only\n",
        "    train_test_data[col] = train_test_data[col].replace(0, np.NaN)\n",
        "    unseen_data[col] = unseen_data[col].replace(0, np.NaN)\n",
        "\n",
        "    # Apply forward fill to carry the last valid draw information\n",
        "    train_test_data[col] = train_test_data[col].fillna(method='ffill')\n",
        "    unseen_data[col] = unseen_data[col].fillna(method='ffill')\n",
        "\n",
        "    # Replace any remaining NaNs (e.g., at the start of the dataset) with 0 or another suitable default value\n",
        "    train_test_data[col] = train_test_data[col].fillna(0)\n",
        "    unseen_data[col] = unseen_data[col].fillna(0)\n",
        "\n",
        "# Save the datasets with adjusted 'Prev_' columns\n",
        "save_dataset(train_test_data, 'E_Logic_Adjusted_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'F_Logic_Adjusted_Unseen_Data.csv')\n",
        "\n",
        "print(\"Datasets with forward filled columns saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P76J1r5_uPZz",
        "outputId": "e19c696c-8dd5-4655-a179-ead25ab96e9e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets with forward filled columns saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-c5dff3effd20>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_test_data[col] = train_test_data[col].fillna(method='ffill')\n",
            "<ipython-input-27-c5dff3effd20>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  unseen_data[col] = unseen_data[col].fillna(method='ffill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2.5: Creation of Arithmetical Features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "train_test_data = load_dataset('E_Logic_Adjusted_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('F_Logic_Adjusted_Unseen_Data.csv')\n",
        "\n",
        "# Print the shape of the dataset after filtering\n",
        "print(\"Shape of train_test_data after filtering:\", train_test_data.shape)\n",
        "print(\"Shape of unseen_data after filtering:\", unseen_data.shape)\n",
        "\n",
        "def create_arithmetical_features(df, window_sizes=[2, 5, 10]):\n",
        "    \"\"\"\n",
        "    Amend DataFrame in-place, adding rolling window calculations and other\n",
        "    mathematical features based on the 'Prediction1' column.\n",
        "    \"\"\"\n",
        "    for window in window_sizes:\n",
        "        # Existing rolling features\n",
        "        df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
        "        df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
        "        df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
        "        df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
        "        df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
        "        df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
        "        df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
        "        df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
        "\n",
        "        # New mathematical features\n",
        "        df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
        "        df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
        "        df[f'Rolling_Range_{window}'] = df[f'Rolling_Max_{window}'] - df[f'Rolling_Min_{window}']\n",
        "\n",
        "    # Additional global features not dependent on window size\n",
        "    df['Prediction1_Diff'] = df['Prediction1'].diff().shift(-1).fillna(method='bfill')\n",
        "    df['Prediction1_Pct_Change'] = df['Prediction1'].pct_change().shift(-1).fillna(0)\n",
        "    df['Prediction1_Cumsum'] = df['Prediction1'].cumsum().shift(1).fillna(method='bfill')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature creation\n",
        "train_test_data = create_arithmetical_features(train_test_data)\n",
        "unseen_data = create_arithmetical_features(unseen_data)\n",
        "\n",
        "# Debug: Confirm the addition of new features\n",
        "print(\"Shape of Training/Testing Data after feature addition:\", train_test_data.shape)\n",
        "print(\"Shape of Unseen Data after feature addition:\", unseen_data.shape)\n",
        "\n",
        "# Save the enhanced datasets\n",
        "save_dataset(train_test_data, 'G_Arithmetic_Features_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'H_Arithmetic_Features_Unseen_Data.csv')\n",
        "print(\"Enhanced datasets saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AgCVi16kcbK",
        "outputId": "c634fb5c-8490-4873-8e0d-3b6c92e561a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_test_data after filtering: (1672, 52)\n",
            "Shape of unseen_data after filtering: (31, 52)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d86da175248a>:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:38: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:38: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:38: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:47: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Prediction1_Diff'] = df['Prediction1'].diff().shift(-1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:49: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Prediction1_Cumsum'] = df['Prediction1'].cumsum().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:38: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:38: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Moving_Avg_{window}'] = df['Prediction1'].rolling(window=window).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Median_{window}'] = df['Prediction1'].rolling(window=window).median().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Std_Dev_{window}'] = df['Prediction1'].rolling(window=window).std().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'RMS_{window}'] = np.sqrt(df['Prediction1'].rolling(window=window).apply(lambda x: np.mean(np.square(x)))).shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Min_{window}'] = df['Prediction1'].rolling(window=window).min().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Max_{window}'] = df['Prediction1'].rolling(window=window).max().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:38: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Skew_{window}'] = df['Prediction1'].rolling(window=window).skew().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Kurtosis_{window}'] = df['Prediction1'].rolling(window=window).kurt().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'EMA_{window}'] = df['Prediction1'].ewm(span=window, adjust=False).mean().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'Rolling_Var_{window}'] = df['Prediction1'].rolling(window=window).var().shift(1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:47: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Prediction1_Diff'] = df['Prediction1'].diff().shift(-1).fillna(method='bfill')\n",
            "<ipython-input-28-d86da175248a>:49: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Prediction1_Cumsum'] = df['Prediction1'].cumsum().shift(1).fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training/Testing Data after feature addition: (1672, 88)\n",
            "Shape of Unseen Data after feature addition: (31, 88)\n",
            "Enhanced datasets saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2.4: Creation of Temporal Features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Load datasets function\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Save dataset function\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load your datasets\n",
        "train_test_data = load_dataset('G_Arithmetic_Features_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('H_Arithmetic_Features_Unseen_Data.csv')\n",
        "\n",
        "def create_temporal_features(df):\n",
        "    \"\"\"\n",
        "    Enhance the DataFrame with temporal features, carefully avoiding data leakage.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime format if not already done\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Extract Day of the Week from 'Date'\n",
        "    df['DayofWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "    # Shifting 'Draw1' to 'Draw4' to use only up to the previous day's results\n",
        "    for draw in ['Draw1', 'Draw2', 'Draw3', 'Draw4']:\n",
        "        df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
        "        df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
        "\n",
        "    # Adding Day of Year for seasonality\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "    # Interval Since Last Appearance and Cumulative Count for each number\n",
        "    max_num = 36\n",
        "    for num in range(1, max_num + 1):\n",
        "        mask = df[['Draw1', 'Draw2', 'Draw3', 'Draw4']].apply(lambda x: num in x.values, axis=1)\n",
        "        df[f'Num_{num}_Interval_Last'] = (~mask).cumsum()\n",
        "        df[f'Num_{num}_Cum_Count'] = mask.cumsum()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Applying the function to your datasets\n",
        "train_test_data = create_temporal_features(train_test_data)\n",
        "unseen_data = create_temporal_features(unseen_data)\n",
        "\n",
        "# Debug: Print the shape to confirm the addition of new temporal features\n",
        "print(\"Shape of Training/Testing Data after temporal feature addition:\", train_test_data.shape)\n",
        "print(\"Shape of Unseen Data after temporal feature addition:\", unseen_data.shape)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in Training/Testing Data:\\n\", train_test_data.isnull().sum())\n",
        "print(\"\\nMissing values in Unseen Data:\\n\", unseen_data.isnull().sum())\n",
        "\n",
        "# Save the enhanced datasets\n",
        "save_dataset(train_test_data, 'I_Temporal_Features_Train_Test_Data.csv')\n",
        "save_dataset(unseen_data, 'J_Temporal_Features_Unseen_Data.csv')\n",
        "print(\"Datasets with temporal features saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-4kmR8ExYtf",
        "outputId": "f25aebe9-4100-4bf6-851f-6a4ffdf9840d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Prev'] = df[draw].shift(1).fillna(method='bfill')\n",
            "<ipython-input-29-4cd6607b4c7f>:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[f'{draw}_Change'] = df[f'{draw}_Prev'].diff().fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training/Testing Data after temporal feature addition: (1672, 170)\n",
            "Shape of Unseen Data after temporal feature addition: (31, 170)\n",
            "\n",
            "Missing values in Training/Testing Data:\n",
            " Date                    0\n",
            "Row Number              0\n",
            "Data_Type               0\n",
            "Draw1                   0\n",
            "DR1_Prev_Week           0\n",
            "                       ..\n",
            "Num_34_Cum_Count        0\n",
            "Num_35_Interval_Last    0\n",
            "Num_35_Cum_Count        0\n",
            "Num_36_Interval_Last    0\n",
            "Num_36_Cum_Count        0\n",
            "Length: 170, dtype: int64\n",
            "\n",
            "Missing values in Unseen Data:\n",
            " Date                    0\n",
            "Row Number              0\n",
            "Data_Type               0\n",
            "Draw1                   0\n",
            "DR1_Prev_Week           0\n",
            "                       ..\n",
            "Num_34_Cum_Count        0\n",
            "Num_35_Interval_Last    0\n",
            "Num_35_Cum_Count        0\n",
            "Num_36_Interval_Last    0\n",
            "Num_36_Cum_Count        0\n",
            "Length: 170, dtype: int64\n",
            "Datasets with temporal features saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3.2: Outlier and Skewness check\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(base_dir, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save a dataset\n",
        "def save_dataset(df, base_dir, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Two_Tier_Class_Regr/'\n",
        "\n",
        "# Load datasets\n",
        "train_test_data = load_dataset(base_dir, 'I_Temporal_Features_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset(base_dir, 'J_Temporal_Features_Unseen_Data.csv')\n",
        "\n",
        "# Assuming train_test_data is your DataFrame\n",
        "numerical_features = train_test_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Removing 'Draw' columns from the analysis if they're considered categorical\n",
        "numerical_features = [feat for feat in numerical_features if 'Draw' not in feat]\n",
        "\n",
        "outliers_count = {}\n",
        "skewness_values = {}\n",
        "\n",
        "for feature in numerical_features:\n",
        "    # Calculate Q1, Q3, and IQR\n",
        "    Q1 = train_test_data[feature].quantile(0.25)\n",
        "    Q3 = train_test_data[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Identifying outliers\n",
        "    outliers = train_test_data[(train_test_data[feature] < (Q1 - 1.5 * IQR)) | (train_test_data[feature] > (Q3 + 1.5 * IQR))]\n",
        "    outliers_count[feature] = len(outliers)\n",
        "\n",
        "    # Calculating skewness\n",
        "    skewness = train_test_data[feature].skew()\n",
        "    skewness_values[feature] = skewness\n",
        "\n",
        "# Printing the results\n",
        "print(\"Outliers count per feature:\")\n",
        "for feature, count in outliers_count.items():\n",
        "    print(f\"{feature}: {count}\")\n",
        "print(\"\\nSkewness values per feature:\")\n",
        "for feature, skewness in skewness_values.items():\n",
        "    print(f\"{feature}: {skewness}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nsq6Qj1K-bqu",
        "outputId": "da58527c-e336-4c66-efdc-c5c61758df31"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py:1256: RuntimeWarning: invalid value encountered in subtract\n",
            "  adjusted = values - mean\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers count per feature:\n",
            "Row Number: 0\n",
            "DR1_Prev_Week: 0\n",
            "DR1_2Weeks: 0\n",
            "DR1_Prev_Entry: 0\n",
            "DR1_Prev_Entry-2: 0\n",
            "DR1_Mov_Avg: 0\n",
            "DR1_Vert_Avg: 0\n",
            "DR2_Prev_Week: 0\n",
            "DR2_2Weeks: 0\n",
            "DR2_Prev_Entry: 0\n",
            "DR2_Prev_Entry-2: 0\n",
            "DR2_Mov_Avg: 0\n",
            "DR2_Vert_Avg: 0\n",
            "DR3_Prev_Week: 0\n",
            "DR3_2Weeks: 0\n",
            "DR3_Prev_Entry: 0\n",
            "DR3_Prev_Entry-2: 0\n",
            "DR3_Mov_Avg: 0\n",
            "DR3_Vert_Avg: 0\n",
            "DR4_Prev_Week: 0\n",
            "DR4_2Weeks: 0\n",
            "DR4_Prev_Entry: 0\n",
            "DR4_Prev_Entry-2: 0\n",
            "DR4_Mov_Avg: 0\n",
            "DR4_Vert_Avg: 0\n",
            "Year: 0\n",
            "Month: 0\n",
            "Day: 0\n",
            "Prev_Morning: 0\n",
            "Prev_Afternoon: 0\n",
            "Prev_Evening: 0\n",
            "Prev_Night: 0\n",
            "Prediction1: 0\n",
            "Line_Prev_Entry: 0\n",
            "Line_PE_Num_1: 0\n",
            "Line_PE_Num_2: 0\n",
            "Line_PE_Num_3: 0\n",
            "Line_PE_Num_4: 0\n",
            "Spirit_PE_Num: 0\n",
            "Rake_PE_Num_1: 0\n",
            "Rake_PE_Num_2: 143\n",
            "Rake_PE_Num_3: 0\n",
            "Rake_PE_Num_4: 0\n",
            "Moving_Avg_2: 0\n",
            "Median_2: 0\n",
            "Std_Dev_2: 0\n",
            "RMS_2: 0\n",
            "Rolling_Min_2: 0\n",
            "Rolling_Max_2: 0\n",
            "Skew_2: 0\n",
            "Kurtosis_2: 0\n",
            "EMA_2: 0\n",
            "Rolling_Var_2: 69\n",
            "Rolling_Range_2: 0\n",
            "Moving_Avg_5: 8\n",
            "Median_5: 0\n",
            "Std_Dev_5: 6\n",
            "RMS_5: 17\n",
            "Rolling_Min_5: 323\n",
            "Rolling_Max_5: 52\n",
            "Skew_5: 0\n",
            "Kurtosis_5: 0\n",
            "EMA_5: 1\n",
            "Rolling_Var_5: 3\n",
            "Rolling_Range_5: 17\n",
            "Moving_Avg_10: 12\n",
            "Median_10: 2\n",
            "Std_Dev_10: 19\n",
            "RMS_10: 14\n",
            "Rolling_Min_10: 0\n",
            "Rolling_Max_10: 45\n",
            "Skew_10: 37\n",
            "Kurtosis_10: 58\n",
            "EMA_10: 6\n",
            "Rolling_Var_10: 10\n",
            "Rolling_Range_10: 45\n",
            "Prediction1_Diff: 0\n",
            "Prediction1_Pct_Change: 317\n",
            "Prediction1_Cumsum: 0\n",
            "DayofWeek: 0\n",
            "DayOfYear: 0\n",
            "Num_1_Interval_Last: 0\n",
            "Num_1_Cum_Count: 0\n",
            "Num_2_Interval_Last: 0\n",
            "Num_2_Cum_Count: 0\n",
            "Num_3_Interval_Last: 0\n",
            "Num_3_Cum_Count: 0\n",
            "Num_4_Interval_Last: 0\n",
            "Num_4_Cum_Count: 0\n",
            "Num_5_Interval_Last: 0\n",
            "Num_5_Cum_Count: 0\n",
            "Num_6_Interval_Last: 0\n",
            "Num_6_Cum_Count: 0\n",
            "Num_7_Interval_Last: 0\n",
            "Num_7_Cum_Count: 0\n",
            "Num_8_Interval_Last: 0\n",
            "Num_8_Cum_Count: 0\n",
            "Num_9_Interval_Last: 0\n",
            "Num_9_Cum_Count: 0\n",
            "Num_10_Interval_Last: 0\n",
            "Num_10_Cum_Count: 0\n",
            "Num_11_Interval_Last: 0\n",
            "Num_11_Cum_Count: 0\n",
            "Num_12_Interval_Last: 0\n",
            "Num_12_Cum_Count: 0\n",
            "Num_13_Interval_Last: 0\n",
            "Num_13_Cum_Count: 0\n",
            "Num_14_Interval_Last: 0\n",
            "Num_14_Cum_Count: 0\n",
            "Num_15_Interval_Last: 0\n",
            "Num_15_Cum_Count: 0\n",
            "Num_16_Interval_Last: 0\n",
            "Num_16_Cum_Count: 0\n",
            "Num_17_Interval_Last: 0\n",
            "Num_17_Cum_Count: 0\n",
            "Num_18_Interval_Last: 0\n",
            "Num_18_Cum_Count: 0\n",
            "Num_19_Interval_Last: 0\n",
            "Num_19_Cum_Count: 0\n",
            "Num_20_Interval_Last: 0\n",
            "Num_20_Cum_Count: 0\n",
            "Num_21_Interval_Last: 0\n",
            "Num_21_Cum_Count: 0\n",
            "Num_22_Interval_Last: 0\n",
            "Num_22_Cum_Count: 0\n",
            "Num_23_Interval_Last: 0\n",
            "Num_23_Cum_Count: 0\n",
            "Num_24_Interval_Last: 0\n",
            "Num_24_Cum_Count: 0\n",
            "Num_25_Interval_Last: 0\n",
            "Num_25_Cum_Count: 0\n",
            "Num_26_Interval_Last: 0\n",
            "Num_26_Cum_Count: 0\n",
            "Num_27_Interval_Last: 0\n",
            "Num_27_Cum_Count: 0\n",
            "Num_28_Interval_Last: 0\n",
            "Num_28_Cum_Count: 0\n",
            "Num_29_Interval_Last: 0\n",
            "Num_29_Cum_Count: 0\n",
            "Num_30_Interval_Last: 0\n",
            "Num_30_Cum_Count: 0\n",
            "Num_31_Interval_Last: 0\n",
            "Num_31_Cum_Count: 0\n",
            "Num_32_Interval_Last: 0\n",
            "Num_32_Cum_Count: 0\n",
            "Num_33_Interval_Last: 0\n",
            "Num_33_Cum_Count: 0\n",
            "Num_34_Interval_Last: 0\n",
            "Num_34_Cum_Count: 0\n",
            "Num_35_Interval_Last: 0\n",
            "Num_35_Cum_Count: 0\n",
            "Num_36_Interval_Last: 0\n",
            "Num_36_Cum_Count: 0\n",
            "\n",
            "Skewness values per feature:\n",
            "Row Number: 0.0\n",
            "DR1_Prev_Week: 0.09477127020890604\n",
            "DR1_2Weeks: 0.09098713254584498\n",
            "DR1_Prev_Entry: 0.1810119318306252\n",
            "DR1_Prev_Entry-2: 0.1324534614244985\n",
            "DR1_Mov_Avg: -0.19944351500323879\n",
            "DR1_Vert_Avg: -0.17147579610725078\n",
            "DR2_Prev_Week: 0.11114256911501852\n",
            "DR2_2Weeks: 0.11679911246431848\n",
            "DR2_Prev_Entry: 0.08742475498960539\n",
            "DR2_Prev_Entry-2: 0.1810119318306252\n",
            "DR2_Mov_Avg: -0.1542261231300085\n",
            "DR2_Vert_Avg: -0.17917058727768018\n",
            "DR3_Prev_Week: 0.13117855676927442\n",
            "DR3_2Weeks: 0.13284555911313994\n",
            "DR3_Prev_Entry: 0.1150806251875924\n",
            "DR3_Prev_Entry-2: 0.08742475498960539\n",
            "DR3_Mov_Avg: -0.2218104523095681\n",
            "DR3_Vert_Avg: -0.18262633712371745\n",
            "DR4_Prev_Week: 0.18315353106719442\n",
            "DR4_2Weeks: 0.17427221410036697\n",
            "DR4_Prev_Entry: 0.13456143158637357\n",
            "DR4_Prev_Entry-2: 0.1150806251875924\n",
            "DR4_Mov_Avg: -0.18717356381418054\n",
            "DR4_Vert_Avg: -0.15077215093236596\n",
            "Year: -0.008050730378182401\n",
            "Month: -0.07908085634540019\n",
            "Day: 0.019997299624065747\n",
            "Prev_Morning: -0.04513687765621516\n",
            "Prev_Afternoon: -0.015598261414734577\n",
            "Prev_Evening: 0.012612624055853327\n",
            "Prev_Night: 0.04179651201629728\n",
            "Prediction1: 0.08742475498960539\n",
            "Line_Prev_Entry: -0.04264445611990939\n",
            "Line_PE_Num_1: -0.04264445611990939\n",
            "Line_PE_Num_2: -0.04264445611990834\n",
            "Line_PE_Num_3: -0.04264445611991036\n",
            "Line_PE_Num_4: -0.04264445611990628\n",
            "Spirit_PE_Num: -0.0798031056160704\n",
            "Rake_PE_Num_1: 0.5169823400054253\n",
            "Rake_PE_Num_2: 0.6901153869164334\n",
            "Rake_PE_Num_3: 0.20729589173709698\n",
            "Rake_PE_Num_4: -0.7197807855476803\n",
            "Moving_Avg_2: 0.15708798650939748\n",
            "Median_2: 0.15708798650939748\n",
            "Std_Dev_2: 0.39713520628810967\n",
            "RMS_2: -0.2847346291788116\n",
            "Rolling_Min_2: 0.8393278869631927\n",
            "Rolling_Max_2: -0.5402320151639259\n",
            "Skew_2: nan\n",
            "Kurtosis_2: nan\n",
            "EMA_2: 0.13762821693267602\n",
            "Rolling_Var_2: 1.235511764804843\n",
            "Rolling_Range_2: 0.39713520628811\n",
            "Moving_Avg_5: 0.056944005127225666\n",
            "Median_5: 0.0864009261026671\n",
            "Std_Dev_5: -0.47363903456396333\n",
            "RMS_5: -0.4085057130256304\n",
            "Rolling_Min_5: 2.677108843192215\n",
            "Rolling_Max_5: -1.1220615061881105\n",
            "Skew_5: -0.007654046031470925\n",
            "Kurtosis_5: 0.5814043867971158\n",
            "EMA_5: 0.11724113515165437\n",
            "Rolling_Var_5: 0.13651140657188998\n",
            "Rolling_Range_5: -0.7812681925436537\n",
            "Moving_Avg_10: -0.10449983890400144\n",
            "Median_10: -0.03277191672874415\n",
            "Std_Dev_10: -0.30663372348674117\n",
            "RMS_10: -0.39339215612893985\n",
            "Rolling_Min_10: 0.0\n",
            "Rolling_Max_10: -1.3792827633249904\n",
            "Skew_10: 0.44227151945886056\n",
            "Kurtosis_10: 2.8964510753433372\n",
            "EMA_10: 0.006173001680873016\n",
            "Rolling_Var_10: 0.15592460107339995\n",
            "Rolling_Range_10: -1.3792827633249904\n",
            "Prediction1_Diff: -0.0014774937473827686\n",
            "Prediction1_Pct_Change: nan\n",
            "Prediction1_Cumsum: -0.028585036649933414\n",
            "DayofWeek: -0.0011972468155822013\n",
            "DayOfYear: -0.06977308257103089\n",
            "Num_1_Interval_Last: 0.0009410862655544384\n",
            "Num_1_Cum_Count: -0.010735666042364412\n",
            "Num_2_Interval_Last: 0.007046575861053041\n",
            "Num_2_Cum_Count: -0.06987320599978872\n",
            "Num_3_Interval_Last: 0.002687593118279185\n",
            "Num_3_Cum_Count: -0.02852362318660743\n",
            "Num_4_Interval_Last: -0.011090935626161985\n",
            "Num_4_Cum_Count: 0.11932521261656161\n",
            "Num_5_Interval_Last: -0.001690006866824168\n",
            "Num_5_Cum_Count: 0.015466929503790233\n",
            "Num_6_Interval_Last: -0.014473186045667039\n",
            "Num_6_Cum_Count: 0.13148242988796444\n",
            "Num_7_Interval_Last: 0.005209414253227085\n",
            "Num_7_Cum_Count: -0.04176565995706486\n",
            "Num_8_Interval_Last: -0.0035552349850389887\n",
            "Num_8_Cum_Count: 0.033666139351127215\n",
            "Num_9_Interval_Last: 0.0031798522942199023\n",
            "Num_9_Cum_Count: -0.036738683454522705\n",
            "Num_10_Interval_Last: -0.00638220376838943\n",
            "Num_10_Cum_Count: 0.06803978836949473\n",
            "Num_11_Interval_Last: 0.00728002070768231\n",
            "Num_11_Cum_Count: -0.090819456026922\n",
            "Num_12_Interval_Last: -0.02050314199375076\n",
            "Num_12_Cum_Count: 0.1921734344858446\n",
            "Num_13_Interval_Last: -0.014422917542209212\n",
            "Num_13_Cum_Count: 0.17714883384840854\n",
            "Num_14_Interval_Last: 0.01140946167314764\n",
            "Num_14_Cum_Count: -0.13235592258627174\n",
            "Num_15_Interval_Last: -0.0014597570951580868\n",
            "Num_15_Cum_Count: 0.01609276243679903\n",
            "Num_16_Interval_Last: 0.005050958284632881\n",
            "Num_16_Cum_Count: -0.04328080625665374\n",
            "Num_17_Interval_Last: 0.0038305659086496505\n",
            "Num_17_Cum_Count: -0.039854800928020226\n",
            "Num_18_Interval_Last: 0.00979370765726356\n",
            "Num_18_Cum_Count: -0.10229610334140615\n",
            "Num_19_Interval_Last: 0.0004239728791556932\n",
            "Num_19_Cum_Count: -0.0006526766419505084\n",
            "Num_20_Interval_Last: 0.004650184129464598\n",
            "Num_20_Cum_Count: -0.03580358425684609\n",
            "Num_21_Interval_Last: 0.000553472683281052\n",
            "Num_21_Cum_Count: -0.0026324705910035075\n",
            "Num_22_Interval_Last: -0.002924787243806771\n",
            "Num_22_Cum_Count: 0.031067471461881227\n",
            "Num_23_Interval_Last: 0.02378479588821306\n",
            "Num_23_Cum_Count: -0.22681560157183778\n",
            "Num_24_Interval_Last: 0.017471449956156838\n",
            "Num_24_Cum_Count: -0.13093723019945913\n",
            "Num_25_Interval_Last: 0.0035018098270690368\n",
            "Num_25_Cum_Count: -0.03515456870884249\n",
            "Num_26_Interval_Last: -0.0023036121830342705\n",
            "Num_26_Cum_Count: 0.035230506014142335\n",
            "Num_27_Interval_Last: 0.017426997956908163\n",
            "Num_27_Cum_Count: -0.20839237168275349\n",
            "Num_28_Interval_Last: -0.003816694359234427\n",
            "Num_28_Cum_Count: 0.03958550200105852\n",
            "Num_29_Interval_Last: -0.0006773821745725591\n",
            "Num_29_Cum_Count: 0.008422403511786932\n",
            "Num_30_Interval_Last: 0.015652244766768166\n",
            "Num_30_Cum_Count: -0.1292996393305439\n",
            "Num_31_Interval_Last: -0.0007317081548970761\n",
            "Num_31_Cum_Count: 0.002895594943106461\n",
            "Num_32_Interval_Last: -0.008732176136970035\n",
            "Num_32_Cum_Count: 0.09283291167748785\n",
            "Num_33_Interval_Last: -0.02223201572379444\n",
            "Num_33_Cum_Count: 0.20470769040820216\n",
            "Num_34_Interval_Last: -0.016444657818536407\n",
            "Num_34_Cum_Count: 0.16419145925613432\n",
            "Num_35_Interval_Last: -0.0027163200286302084\n",
            "Num_35_Cum_Count: 0.036517435203398736\n",
            "Num_36_Interval_Last: 0.008870602944275432\n",
            "Num_36_Cum_Count: -0.10488708695738722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check Point**"
      ],
      "metadata": {
        "id": "-8wbf69kgvut"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76esQGp6gthd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns and their expected ranges\n",
        "columns_ranges = {\n",
        "    'Line_PE_Num_2': (1, 36),\n",
        "    'Line_PE_Num_3': (1, 36),\n",
        "    'Line_PE_Num_4': (1, 36),\n",
        "    'Rake_PE_Num_4': (1, 36),\n",
        "    'Moving_Avg_5': (1, 36),\n",
        "    'Moving_Avg_10': (1, 36)\n",
        "}\n",
        "\n",
        "# Identify rows with out-of-range values for each column\n",
        "for column, (min_val, max_val) in columns_ranges.items():\n",
        "    out_of_range_mask = (train_test_data[column] < min_val) | (train_test_data[column] > max_val)\n",
        "    out_of_range_rows = train_test_data[out_of_range_mask]\n",
        "    if not out_of_range_rows.empty:\n",
        "        print(f\"Rows with out-of-range values for {column}:\")\n",
        "        print(out_of_range_rows[[column]])\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "Q71hChhZIs7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rBIE_xRqAri"
      },
      "outputs": [],
      "source": [
        "# CELL 3.5.1: Handling Outliers (Commented out for potential future use)\n",
        "\n",
        "# # Define the base directory for file paths\n",
        "# base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# # Function to load a dataset\n",
        "# def load_dataset(filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     return pd.read_csv(full_path)\n",
        "\n",
        "# # Function to save the dataset at the specified path\n",
        "# def save_dataset(data, filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     data.to_csv(full_path, index=False)\n",
        "#     print(f\"Dataset saved to {full_path}\")\n",
        "\n",
        "# # Load datasets with relevant columns (replace with appropriate dataset)\n",
        "# train_test_data = load_dataset('your_dataset.csv')\n",
        "# unseen_data = load_dataset('your_dataset.csv')\n",
        "\n",
        "# # Import necessary libraries\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Function to identify and remove outliers using IQR for a given feature\n",
        "# def remove_outliers(data, feature):\n",
        "#     Q1 = np.percentile(data[feature], 25)\n",
        "#     Q3 = np.percentile(data[feature], 75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     outlier_step = 1.5 * IQR\n",
        "\n",
        "#     outliers = data[(data[feature] < Q1 - outlier_step) | (data[feature] > Q3 + outlier_step)]\n",
        "#     print(f\"Number of outliers in {feature}: {outliers.shape[0]}\")\n",
        "\n",
        "#     return data.drop(outliers.index)\n",
        "\n",
        "# # Explicitly list all numeric features to be checked for outliers, excluding binary \"Lines\" features\n",
        "# numeric_features = ['DR1_Prev_Week', 'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg', 'DR1_Vert_Avg', 'Year', 'Month', 'Day', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night', 'Prediction1']\n",
        "\n",
        "# # Apply the remove_outliers function to each numeric feature\n",
        "# for feature in numeric_features:\n",
        "#     train_test_data = remove_outliers(train_test_data, feature)\n",
        "#     unseen_data = remove_outliers(unseen_data, feature)\n",
        "\n",
        "# # Save the datasets with outliers removed (replace with appropriate dataset)\n",
        "# save_dataset(train_test_data, 'your_dataset.csv')\n",
        "# save_dataset(unseen_data, 'your_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH4FoCLcK78V"
      },
      "outputs": [],
      "source": [
        "# CELL 3.5.2: Visualize The Outliers After The Removal Process (Commented out for potential future use)\n",
        "\n",
        "# #import matplotlib.pyplot as plt\n",
        "# #import seaborn as sns\n",
        "\n",
        "# # Define the base directory for file paths\n",
        "# base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# # Function to load a dataset\n",
        "# def load_dataset(filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     return pd.read_csv(full_path)\n",
        "\n",
        "# # Load datasets with relevant columns (replace with appropriate dataset)\n",
        "# train_test_data = load_dataset('your_dataset.csv')\n",
        "# unseen_data = load_dataset('your_dataset.csv')\n",
        "\n",
        "# # List of features to visualize for outliers\n",
        "# features_to_visualize = ['Prediction1', 'DR1_Prev_Entry', 'DR1_Prev_Week', 'DR1_2Weeks',\n",
        "#                          'DR1_Mov_Avg', 'DR1_Vert_Avg', 'Prev_Morning', 'Prev_Afternoon',\n",
        "#                          'Prev_Evening', 'Prev_Night', 'Year', 'Month', 'Day']\n",
        "\n",
        "# # Function to create box plots for a feature\n",
        "# def create_boxplots(feature):\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     sns.boxplot(x=train_test_data_no_outliers[feature])\n",
        "#     plt.title(f'Boxplot of {feature} After Removing Outliers in Train/Test Data')\n",
        "#     plt.show()\n",
        "\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     sns.boxplot(x=unseen_data_no_outliers[feature])\n",
        "#     plt.title(f'Boxplot of {feature} After Removing Outliers in Unseen Data')\n",
        "#     plt.show()\n",
        "\n",
        "# # Create box plots for each feature\n",
        "# for feature in features_to_visualize:\n",
        "#     create_boxplots(feature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY89ztlkqA-h"
      },
      "outputs": [],
      "source": [
        "# Cell 3.6: Handling Skewness (Commented out for potential future use)\n",
        "\n",
        "# # Define the base directory for file paths\n",
        "# base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# # Import necessary libraries\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from scipy.stats import skew\n",
        "\n",
        "# # Function to load a dataset\n",
        "# def load_dataset(filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     return pd.read_csv(full_path)\n",
        "\n",
        "# # Function to save the dataset at the specified path\n",
        "# def save_dataset(data, filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     data.to_csv(full_path, index=False)\n",
        "#     print(f\"Dataset saved to {full_path}\")\n",
        "\n",
        "# # Load datasets with relevant columns (replace with appropriate dataset)\n",
        "# train_test_data = load_dataset('your_dataset.csv')\n",
        "# unseen_data = load_dataset('your_dataset.csv')\n",
        "\n",
        "# # Function to calculate skewness and apply transformations\n",
        "# def handle_skewness(data):\n",
        "#     # Select only numeric columns excluding 'Date' and any 'Lines' columns\n",
        "#     numeric_feats = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "#     excluded_columns = ['Date']  # Add your actual date column name if different\n",
        "#     excluded_columns.extend([col for col in data.columns if col.startswith('Lines_') or col.startswith('Line_')])  # Add all 'Lines' related columns to exclusion list\n",
        "\n",
        "#     # Remove excluded columns from the list of numeric features\n",
        "#     numeric_feats = [feat for feat in numeric_feats if feat not in excluded_columns]\n",
        "\n",
        "#     # Calculate skewness only for numeric columns\n",
        "#     skewness = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "#     print(\"Skewness in the dataset:\")\n",
        "#     print(skewness)\n",
        "\n",
        "#     # Identify skewed features (you can adjust the threshold)\n",
        "#     skewed_features = skewness[abs(skewness) > 0.5]\n",
        "#     print(\"Skewed features before transformation:\")\n",
        "#     print(skewed_features)\n",
        "\n",
        "#     # Apply log transformation for positive skewness\n",
        "#     for feature in skewed_features.index:\n",
        "#         if skewness[feature] > 0:\n",
        "#             data[feature] = np.log1p(data[feature])\n",
        "#         elif skewness[feature] < 0:  # For negative skewness, consider other transformations\n",
        "#             # Implement other transformations as needed\n",
        "#             pass\n",
        "\n",
        "#     # Check skewness after transformation\n",
        "#     print(\"Skewness after transformation:\")\n",
        "#     print(data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False))\n",
        "\n",
        "#     return data\n",
        "\n",
        "# # Handling skewness in train_test_data_no_outliers\n",
        "# train_test_data_Handle_Skewness = handle_skewness(train_test_data_no_outliers)\n",
        "\n",
        "# # Handling skewness in unseen_data_no_outliers\n",
        "# unseen_data_Handle_Skewness = handle_skewness(unseen_data_no_outliers)\n",
        "\n",
        "# # Save the datasets with outliers removed (replace with appropriate dataset)\n",
        "# save_dataset(train_test_data, 'your_dataset.csv')\n",
        "# save_dataset(unseen_data, 'your_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OE-SPwcrI1G"
      },
      "outputs": [],
      "source": [
        "# Cell 3.7: Visualizing Skewness (Commented out for potential future use)\n",
        "\n",
        "# # Define the base directory for file paths\n",
        "# base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# # Function to load a dataset\n",
        "# def load_dataset(filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     return pd.read_csv(full_path)\n",
        "\n",
        "# # Function to save the dataset at the specified path\n",
        "# def save_dataset(data, filename):\n",
        "#     full_path = f'{base_dir}{filename}'\n",
        "#     data.to_csv(full_path, index=False)\n",
        "#     print(f\"Dataset saved to {full_path}\")\n",
        "\n",
        "# # Load datasets with relevant columns (replace with appropriate dataset)\n",
        "# train_test_data = load_dataset('your_dataset.csv')\n",
        "# unseen_data = load_dataset('your_dataset.csv')\n",
        "\n",
        "# # Function to visualize skewness before and after transformation\n",
        "# def visualize_skewness(data_before, data_after, feature):\n",
        "#     \"\"\"\n",
        "#     Visualize the skewness of a given feature before and after transformations.\n",
        "#     :param data_before: DataFrame before transformation\n",
        "#     :param data_after: DataFrame after transformation\n",
        "#     :param feature: The feature to visualize\n",
        "#     \"\"\"\n",
        "#     fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "#     # Before transformation\n",
        "#     sns.histplot(data_before[feature], kde=True, ax=ax[0], color='blue')\n",
        "#     ax[0].set_title(f'Distribution of {feature} Before Transformation')\n",
        "#     ax[0].set_xlabel(feature)\n",
        "#     ax[0].set_ylabel('Frequency')\n",
        "\n",
        "#     # After transformation\n",
        "#     sns.histplot(data_after[feature], kde=True, ax=ax[1], color='green')\n",
        "#     ax[1].set_title(f'Distribution of {feature} After Transformation')\n",
        "#     ax[1].set_xlabel(feature)\n",
        "#     ax[1].set_ylabel('Frequency')\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Assuming 'Rake_PE_Num_2' was a skewed feature that has been transformed\n",
        "# # Replace 'Rake_PE_Num_2' with actual feature name if different\n",
        "# # Load the original dataset for comparison\n",
        "# train_test_data = pd.read_csv(base_dir + 'A_Initial_Train_Test_Data.csv')  # The original data before skewness handling\n",
        "\n",
        "# # Visualize the skewness for 'Rake_PE_Num_2'\n",
        "# visualize_skewness(train_test_data, train_test_data_Handle_Skewness, 'Rake_PE_Num_2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cATviIj00fb"
      },
      "outputs": [],
      "source": [
        "# CELL 4.1: Splitting Data into Training, Validation, and Test Sets - Chronological Split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f\"{base_dir}{filename}\"\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save a dataset\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f\"{base_dir}{filename}\"\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load cleaned datasets\n",
        "train_test_data = load_dataset('C_Enhanced_Cleaning_and_Feature_Engineering_Train_Test_Data.csv')\n",
        "unseen_data = load_dataset('D_Enhanced_Cleaning_and_Feature_Engineering_Unseen_Data.csv')\n",
        "\n",
        "# Sort by 'Date'\n",
        "train_test_data.sort_values(by=['Date'], inplace=True)\n",
        "\n",
        "# Separate the training/testing dataset into features and target\n",
        "X = train_test_data.drop(['Prediction1'], axis=1)\n",
        "y = train_test_data['Prediction1']\n",
        "\n",
        "# Calculate split indices\n",
        "train_size = int(len(X) * 0.7)\n",
        "val_size = int(len(X) * 0.15)\n",
        "test_size = len(X) - train_size - val_size\n",
        "\n",
        "# Split the dataset chronologically\n",
        "X_train, y_train = X.iloc[:train_size], y.iloc[:train_size]\n",
        "X_val, y_val = X.iloc[train_size:train_size+val_size], y.iloc[train_size:train_size+val_size]\n",
        "X_test, y_test = X.iloc[train_size+val_size:], pd.Series([0] * (len(X) - train_size - val_size))\n",
        "unseen_features = unseen_data.drop(['Prediction1'], axis=1)\n",
        "unseen_target = unseen_data['Prediction1']  # This line should exist before the save_dataset calls\n",
        "\n",
        "# Create actual results datasets for evaluation purposes\n",
        "actual_results_train = train_test_data.iloc[:train_size][['Date', 'Draw1']].copy()\n",
        "actual_results_val = train_test_data.iloc[train_size:train_size+val_size][['Date', 'Draw1']].copy()\n",
        "actual_results_test = train_test_data.iloc[train_size+val_size:][['Date', 'Draw1']].copy()\n",
        "actual_results_unseen = unseen_data[['Date', 'Draw1']].copy()\n",
        "\n",
        "# Save the datasets\n",
        "save_dataset(X_train, 'E_Train_Features.csv')\n",
        "save_dataset(y_train.to_frame('Prediction1'), 'E_Train_Target.csv')\n",
        "save_dataset(X_val, 'F_Val_Features.csv')\n",
        "save_dataset(y_val.to_frame('Prediction1'), 'F_Val_Target.csv')\n",
        "save_dataset(X_test, 'G_Test_Features.csv')\n",
        "save_dataset(y_test.to_frame('Prediction1'), 'G_Test_Target.csv')\n",
        "save_dataset(unseen_features, 'H_Unseen_Features.csv')\n",
        "save_dataset(unseen_target.to_frame('Prediction1'), 'H_Unseen_Target.csv')\n",
        "\n",
        "# Save the actual results datasets\n",
        "save_dataset(actual_results_train, 'Actual_Results_Train.csv')\n",
        "save_dataset(actual_results_val, 'Actual_Results_Val.csv')\n",
        "save_dataset(actual_results_test, 'Actual_Results_Test.csv')\n",
        "save_dataset(actual_results_unseen, 'Actual_Results_Unseen.csv')\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"Shape of unseen_features:\", unseen_features.shape)\n",
        "print(\"Shape of unseen_target:\", unseen_target.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3fIhKKGi_h9"
      },
      "outputs": [],
      "source": [
        "# CELL 4.2: Handling \"Sensitive\" Columns and Applying 'Keep Only' Function After Data Splitting\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f\"{base_dir}{filename}\"\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save a dataset\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f\"{base_dir}{filename}\"\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the datasets after data splitting\n",
        "X_train = load_dataset('E_Train_Features.csv')\n",
        "y_train = load_dataset('E_Train_Target.csv')\n",
        "X_val = load_dataset('F_Val_Features.csv')\n",
        "y_val = load_dataset('F_Val_Target.csv')\n",
        "X_test = load_dataset('G_Test_Features.csv')\n",
        "y_test = load_dataset('G_Test_Target.csv')\n",
        "unseen_features = load_dataset('H_Unseen_Features.csv')\n",
        "unseen_target = load_dataset('H_Unseen_Target.csv')\n",
        "\n",
        "# Function to drop sensitive columns\n",
        "def drop_sensitive_columns(df, columns_to_drop):\n",
        "    return df.drop(columns=columns_to_drop, inplace=False)\n",
        "\n",
        "# Function to keep only the specified columns in the dataset\n",
        "def keep_only(df, columns_to_keep):\n",
        "    return df[columns_to_keep]\n",
        "\n",
        "# List of sensitive columns to drop (replace with actual column names as needed)\n",
        "sensitive_columns_to_drop = ['Draw1', 'Draw2', 'Draw3', 'Draw4']\n",
        "\n",
        "# List of columns to keep (adjust as necessary)\n",
        "columns_to_keep = ['Date', 'DR1_Prev_Week', 'DR1_2Weeks', 'DR1_Prev_Entry', 'DR1_Prev_Entry-2', 'DR1_Mov_Avg', 'DR1_Vert_Avg', 'DR4_Vert_Avg_shifted', 'Year', 'Month', 'Day', 'Prev_Morning', 'Prev_Afternoon', 'Prev_Evening', 'Prev_Night', 'Line_Prev_Entry', 'Line_PE_Num_1', 'Line_PE_Num_2', 'Line_PE_Num_3', 'Line_PE_Num_4', 'Spirit_PE_Num', 'Rake_PE_Num_1', 'Rake_PE_Num_2', 'Rake_PE_Num_3', 'Rake_PE_Num_4']\n",
        "\n",
        "# Drop sensitive columns for Training/Testing and Unseen Data\n",
        "X_train = drop_sensitive_columns(X_train, sensitive_columns_to_drop)\n",
        "X_val = drop_sensitive_columns(X_val, sensitive_columns_to_drop)\n",
        "X_test = drop_sensitive_columns(X_test, sensitive_columns_to_drop)\n",
        "unseen_features = drop_sensitive_columns(unseen_features, sensitive_columns_to_drop)\n",
        "\n",
        "# Apply 'Keep Only' for Training/Testing and Unseen Data\n",
        "X_train = keep_only(X_train, columns_to_keep)\n",
        "X_val = keep_only(X_val, columns_to_keep)\n",
        "X_test = keep_only(X_test, columns_to_keep)\n",
        "unseen_features = keep_only(unseen_features, columns_to_keep)\n",
        "\n",
        "# Save the datasets\n",
        "save_dataset(X_train, 'I_Train_Features.csv')\n",
        "save_dataset(y_train, 'I_Train_Target.csv') # No need for to_frame conversion\n",
        "save_dataset(X_val, 'J_Val_Features.csv')\n",
        "save_dataset(y_val, 'J_Val_Target.csv') # No need for to_frame conversion\n",
        "save_dataset(X_test, 'K_Test_Features.csv')\n",
        "save_dataset(y_test, 'K_Test_Target.csv')\n",
        "save_dataset(unseen_features, 'L_Unseen_Features.csv')\n",
        "save_dataset(unseen_target, 'L_Unseen_Target.csv')\n",
        "\n",
        "# Check the shape of the processed datasets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of unseen_features:\", unseen_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju7plGHcOSJG"
      },
      "outputs": [],
      "source": [
        "# CELL 5.1: Feature Engineering (Interaction terms, Polynomial features, Domain-specific transformations, Clustering-based features)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save datasets\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the datasets after data splitting\n",
        "X_train = load_dataset('I_Train_Features.csv')\n",
        "y_train = load_dataset('I_Train_Target.csv')\n",
        "X_val = load_dataset('J_Val_Features.csv')\n",
        "y_val = load_dataset('J_Val_Target.csv')\n",
        "X_test = load_dataset('K_Test_Features.csv')\n",
        "y_test = load_dataset('K_Test_Target.csv')\n",
        "unseen_features = load_dataset('L_Unseen_Features.csv')\n",
        "unseen_target = load_dataset('L_Unseen_Target.csv')\n",
        "\n",
        "def create_specific_interaction_terms(data):\n",
        "    # Define all interactions, including corrections and new additions\n",
        "    interactions = [\n",
        "        # Previous and corrected interactions\n",
        "        ('Day', 'Month'), ('Day', 'DR1_Prev_Week'), ('DR1_Prev_Week', 'DR1_2Weeks'),\n",
        "        ('DR4_Mov_Avg', 'Day'), ('DR4_Mov_Avg', 'DR1_Prev_Week'), ('DR4_Mov_Avg', 'Spirit_PE_Num'),\n",
        "        ('DR4_Vert_Avg_shifted', 'Day'), ('DR4_Vert_Avg_shifted', 'DR1_Prev_Week'), ('DR4_Vert_Avg_shifted', 'Spirit_PE_Num'),\n",
        "        ('Prev_Afternoon', 'Spirit_PE_Num'), ('DR1_Vert_Avg', 'Spirit_PE_Num'),\n",
        "        ('DR1_2Weeks', 'Rake_PE_Num_3'), ('DR1_Prev_Week', 'Prev_Morning'),\n",
        "        ('DR1_2Weeks', 'Prev_Afternoon'), ('DR1_Vert_Avg', 'Rake_PE_Num_2'),\n",
        "        ('DR1_Mov_Avg', 'Rake_PE_Num_4'), ('Day', 'Prev_Night'), ('Month', 'Prev_Night'),\n",
        "        ('Year', 'Spirit_PE_Num'), ('Year', 'DR1_Mov_Avg'),\n",
        "        # Missing interactions from the previous iteration\n",
        "        ('Day', 'Prev_Morning'), ('Day', 'Rake_PE_Num_2'), ('Day', 'DR1_Vert_Avg'),\n",
        "        # Latest interactions\n",
        "        ('DR1_Vert_Avg', 'Prev_Morning'), ('DR1_Vert_Avg', 'DR1_Prev_Week'),\n",
        "    ]\n",
        "\n",
        "    # Ensure inclusion of all Rake_PE_Num and Line_PE_Num interactions\n",
        "    for num in [1, 2, 3, 4]:\n",
        "        interactions.extend([\n",
        "            ('Prev_Morning', f'Rake_PE_Num_{num}'), ('Spirit_PE_Num', f'Rake_PE_Num_{num}'),\n",
        "            ('DR1_Prev_Week', f'Line_PE_Num_{num}')\n",
        "        ])\n",
        "\n",
        "    # Dynamically generate interaction terms\n",
        "    for feature_a, feature_b in interactions:\n",
        "        if feature_a in data.columns and feature_b in data.columns:\n",
        "            interaction_feature_name = f'interaction_{feature_a}_{feature_b}'\n",
        "            data[interaction_feature_name] = data[feature_a] * data[feature_b]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Update datasets with new interaction terms\n",
        "X_train_fe = create_specific_interaction_terms(X_train.copy())\n",
        "X_val_fe = create_specific_interaction_terms(X_val.copy())\n",
        "X_test_fe = create_specific_interaction_terms(X_test.copy())\n",
        "unseen_features_fe = create_specific_interaction_terms(unseen_features.copy())\n",
        "\n",
        "# Revised Polynomial and Clustering Features Section\n",
        "# Specify the feature list for this iteration's focus, including the new interactions\n",
        "polynomial_feature_list = ['DR1_Vert_Avg', 'Prev_Morning', 'DR1_Prev_Week', 'Day']\n",
        "clustering_feature_list = polynomial_feature_list\n",
        "\n",
        "# Apply polynomial and clustering features functions as defined previously\n",
        "def add_polynomial_features(data, feature_list, degree=2):\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    valid_features = [feature for feature in feature_list if feature in data.columns]\n",
        "    poly_features = poly.fit_transform(data[valid_features])\n",
        "    feature_names = poly.get_feature_names_out(valid_features)\n",
        "    data_poly = pd.DataFrame(poly_features, columns=feature_names, index=data.index)\n",
        "    return pd.concat([data, data_poly], axis=1)\n",
        "\n",
        "def add_clustering_features(data, feature_list, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    valid_features = [feature for feature in feature_list if feature in data.columns]\n",
        "    clusters = kmeans.fit_predict(data[valid_features])\n",
        "    data['cluster'] = clusters\n",
        "    return data\n",
        "\n",
        "# Apply polynomial and clustering features\n",
        "X_train_fe = add_polynomial_features(X_train_fe, polynomial_feature_list)\n",
        "X_val_fe = add_polynomial_features(X_val_fe, polynomial_feature_list)\n",
        "X_test_fe = add_polynomial_features(X_test_fe, polynomial_feature_list)\n",
        "unseen_features_fe = add_polynomial_features(unseen_features_fe, polynomial_feature_list)\n",
        "\n",
        "X_train_fe = add_clustering_features(X_train_fe, clustering_feature_list)\n",
        "X_val_fe = add_clustering_features(X_val_fe, clustering_feature_list)\n",
        "X_test_fe = add_clustering_features(X_test_fe, clustering_feature_list)\n",
        "unseen_features_fe = add_clustering_features(unseen_features_fe, clustering_feature_list)\n",
        "\n",
        "# Save the updated datasets with engineered features\n",
        "save_dataset(X_train_fe, 'M_Engineered_Train_Features.csv')\n",
        "save_dataset(y_train, 'M_Engineered_Train_Target.csv')\n",
        "save_dataset(X_val_fe, 'N_Engineered_Val_Features.csv')\n",
        "save_dataset(y_val, 'N_Engineered_Val_Target.csv')\n",
        "save_dataset(X_test_fe, 'O_Engineered_Test_Features.csv')\n",
        "save_dataset(y_test, 'O_Engineered_Test_Target.csv')\n",
        "save_dataset(unseen_features_fe, 'P_Engineered_Unseen_Features.csv')\n",
        "save_dataset(unseen_target, 'P_Engineered_Unseen_Target.csv')\n",
        "\n",
        "# Check the shape of the processed datasets\n",
        "print(\"Shape of X_train_fe:\", X_train_fe.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val_fe:\", X_val_fe.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "print(\"Shape of X_test_fe:\", X_test_fe.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"Shape of unseen_features_fe:\", unseen_features_fe.shape)\n",
        "print(\"Shape of unseen_target:\", unseen_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkZI8_X9OSXD"
      },
      "outputs": [],
      "source": [
        "# Cell 5.2: Review Newly Created Features\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load and save datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}/{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}/{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "    print(f\"Dataset saved to {full_path}\")\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets\n",
        "X_train_fe = load_dataset('M_Engineered_Train_Features.csv')\n",
        "y_train_fe = load_dataset('M_Engineered_Train_Target.csv')['Prediction1']\n",
        "X_val_fe = load_dataset('N_Engineered_Val_Features.csv')\n",
        "y_val_fe = load_dataset('N_Engineered_Val_Target.csv')['Prediction1']\n",
        "X_test_fe = load_dataset('O_Engineered_Test_Features.csv')\n",
        "y_test_fe = load_dataset('O_Engineered_Test_Target.csv')['Prediction1']\n",
        "unseen_features_fe = load_dataset('P_Engineered_Unseen_Features.csv')\n",
        "unseen_target_fe = load_dataset('P_Engineered_Unseen_Target.csv')['Prediction1']\n",
        "\n",
        "# List prefixes of new features based on your naming convention\n",
        "prefixes_of_new_features = ['interaction_', 'poly_', 'log_', 'cluster']\n",
        "\n",
        "# Extract the full list of new features based on these prefixes\n",
        "new_features_full_list = [col for col in X_train_fe.columns if any(col.startswith(prefix) for prefix in prefixes_of_new_features)]\n",
        "\n",
        "print(\"Full list of new features:\", new_features_full_list)\n",
        "\n",
        "# Visualizing some of the new features\n",
        "# ... [Insert visualization code here, similar to the histogram and scatter plot visualization in the original Cell 2.5b]\n",
        "\n",
        "# Extracting just the new features for correlation analysis from the training set\n",
        "new_features_corr_matrix = X_train_fe[new_features_full_list].corr()\n",
        "\n",
        "# Display the correlation matrix for the new features\n",
        "sns.heatmap(new_features_corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix for New Features')\n",
        "plt.show()\n",
        "\n",
        "# Check the shape of the processed datasets\n",
        "print(\"Shape of X_train_fe:\", X_train_fe.shape)\n",
        "print(\"Shape of y_train_fe:\", y_train.shape)\n",
        "print(\"Shape of X_val_fe:\", X_val_fe.shape)\n",
        "print(\"Shape of y_val_fe:\", y_val.shape)\n",
        "print(\"Shape of X_test_fe:\", X_test_fe.shape)\n",
        "print(\"Shape of y_test_fe:\", y_test.shape)\n",
        "print(\"Shape of unseen_features_fe:\", unseen_features_fe.shape)\n",
        "print(\"Shape of unseen_target_fe:\", unseen_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmoR6lwPYI9b"
      },
      "outputs": [],
      "source": [
        "# CELL 5.3: Enhanced Analysis of Newly Created Features\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load and save datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}/{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def save_dataset(df, filename):\n",
        "    full_path = f'{base_dir}/{filename}'\n",
        "    df.to_csv(full_path, index=False)\n",
        "    print(f\"Dataset saved to {full_path}\")\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets\n",
        "X_train_fe = load_dataset('M_Engineered_Train_Features.csv')\n",
        "y_train_fe = load_dataset('M_Engineered_Train_Target.csv')['Prediction1']\n",
        "X_val_fe = load_dataset('N_Engineered_Val_Features.csv')\n",
        "y_val_fe = load_dataset('N_Engineered_Val_Target.csv')['Prediction1']\n",
        "X_test_fe = load_dataset('O_Engineered_Test_Features.csv')\n",
        "y_test_fe = load_dataset('O_Engineered_Test_Target.csv')['Prediction1']\n",
        "unseen_features_fe = load_dataset('P_Engineered_Unseen_Features.csv')\n",
        "unseen_target_fe = load_dataset('P_Engineered_Unseen_Target.csv')['Prediction1']\n",
        "\n",
        "# Check the shape of the datasets\n",
        "print(\"Shape of X_train_fe:\", X_train_fe.shape)\n",
        "print(\"Shape of y_train_fe:\", y_train_fe.shape)\n",
        "print(\"Shape of X_val_fe:\", X_val_fe.shape)\n",
        "print(\"Shape of y_val_fe:\", y_val_fe.shape)\n",
        "print(\"Shape of X_test_fe:\", X_test_fe.shape)\n",
        "print(\"Shape of y_test_fe:\", y_test_fe.shape)\n",
        "print(\"Shape of unseen_features_fe:\", unseen_features_fe.shape)\n",
        "print(\"Shape of unseen_target_fe:\", unseen_target_fe.shape)\n",
        "\n",
        "# Combine training, validation, and test features for analysis\n",
        "combined_data_fe = pd.concat([X_train_fe, X_val_fe, X_test_fe])\n",
        "\n",
        "# Import necessary libraries\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "# List of new feature names (update this list based on the features you have created)\n",
        "new_feature_names = ['interaction_Day_Month', 'interaction_Day_DR1_Prev_Week', 'interaction_DR1_Prev_Week_DR1_2Weeks', 'interaction_DR4_Vert_Avg_shifted_Day', 'interaction_DR4_Vert_Avg_shifted_DR1_Prev_Week', 'interaction_DR4_Vert_Avg_shifted_Spirit_PE_Num', 'interaction_Prev_Afternoon_Spirit_PE_Num', 'interaction_DR1_Vert_Avg_Spirit_PE_Num', 'interaction_DR1_2Weeks_Rake_PE_Num_3', 'interaction_DR1_Prev_Week_Prev_Morning', 'interaction_DR1_2Weeks_Prev_Afternoon', 'interaction_DR1_Vert_Avg_Rake_PE_Num_2', 'interaction_DR1_Mov_Avg_Rake_PE_Num_4', 'interaction_Day_Prev_Night', 'interaction_Month_Prev_Night', 'interaction_Year_Spirit_PE_Num', 'interaction_Year_DR1_Mov_Avg', 'interaction_Day_Prev_Morning', 'interaction_Day_Rake_PE_Num_2', 'interaction_Day_DR1_Vert_Avg', 'interaction_DR1_Vert_Avg_Prev_Morning', 'interaction_DR1_Vert_Avg_DR1_Prev_Week', 'interaction_Prev_Morning_Rake_PE_Num_1', 'interaction_Spirit_PE_Num_Rake_PE_Num_1', 'interaction_DR1_Prev_Week_Line_PE_Num_1', 'interaction_Prev_Morning_Rake_PE_Num_2', 'interaction_Spirit_PE_Num_Rake_PE_Num_2', 'interaction_DR1_Prev_Week_Line_PE_Num_2', 'interaction_Prev_Morning_Rake_PE_Num_3', 'interaction_Spirit_PE_Num_Rake_PE_Num_3', 'interaction_DR1_Prev_Week_Line_PE_Num_3', 'interaction_Prev_Morning_Rake_PE_Num_4', 'interaction_Spirit_PE_Num_Rake_PE_Num_4', 'interaction_DR1_Prev_Week_Line_PE_Num_4', 'cluster']\n",
        "\n",
        "# VIF Calculation for New Features\n",
        "def calculate_vif(data, features):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = features\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(data[features].values, i) if data[features].std()[i] > 0 else float('inf') for i in range(len(features))]\n",
        "    return vif_data\n",
        "\n",
        "new_features_vif = calculate_vif(combined_data_fe, new_feature_names)\n",
        "print(\"VIF for new features:\")\n",
        "print(new_features_vif)\n",
        "\n",
        "# Boxplots for New Features\n",
        "for feature in new_feature_names:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(data=combined_data_fe[feature])\n",
        "    plt.title(f'Boxplot of {feature}')\n",
        "    plt.show()\n",
        "\n",
        "# Pair Plots for a Subset of New Features\n",
        "# Select a subset of features for pair plot (modify as needed)\n",
        "subset_features = new_feature_names[:4]  # Adjust the number as appropriate\n",
        "sns.pairplot(combined_data_fe[subset_features])\n",
        "plt.suptitle('Pair Plots of Selected New Features')\n",
        "plt.show()\n",
        "\n",
        "# Model-Based Feature Importance Analysis\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train a RandomForest model using the feature-engineered training data\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_fe[new_feature_names], y_train_fe)\n",
        "\n",
        "# Visualize Feature Importance\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "plt.title('Feature Importances in RandomForest Model')\n",
        "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "plt.yticks(range(len(indices)), [new_feature_names[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()\n",
        "\n",
        "# SHAP Values Analysis\n",
        "import shap\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(combined_data_fe[new_feature_names])\n",
        "\n",
        "# Plot SHAP values\n",
        "shap.summary_plot(shap_values, combined_data_fe[new_feature_names], plot_type=\"bar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpKyOvyDfyDN"
      },
      "outputs": [],
      "source": [
        "# CELL 5.4: Feature Importance Analysis and Important Features Extraction\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import pandas as pd\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# import numpy as np\n",
        "\n",
        "# # Define the base directory for file paths\n",
        "# base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# # Function to load and save a dataset\n",
        "# def load_dataset(filename):\n",
        "#     full_path = f'{base_dir}/{filename}'\n",
        "#     return pd.read_csv(full_path)\n",
        "\n",
        "# def save_dataset(df, filename):\n",
        "#     full_path = f'{base_dir}/{filename}'\n",
        "#     df.to_csv(full_path, index=False)\n",
        "\n",
        "# # Load datasets with relevant columns (replace with appropriate dataset)\n",
        "# X_train_fe = load_dataset('your_dataset.csv')\n",
        "# y_train_fe = load_dataset('your_dataset.csv')['Prediction1']\n",
        "# X_val_fe = load_dataset('your_dataset.csv')\n",
        "# y_val_fe = load_dataset('your_dataset.csv')['Prediction1']\n",
        "# X_test_fe = load_dataset('your_dataset.csv')\n",
        "# y_test_fe = load_dataset('your_dataset.csv')['Prediction1']\n",
        "# unseen_features_fe = load_dataset('your_dataset.csv')\n",
        "# unseen_target_fe = load_dataset('your_dataset.csv')['Prediction1']\n",
        "\n",
        "# # Train a RandomForest model for feature importance analysis\n",
        "# # rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# # X_train_rf = X_train_fe.drop(['Date'], axis=1)  # Exclude 'Date' column for training\n",
        "# # rf.fit(X_train_rf, y_train_fe)\n",
        "\n",
        "# # Get feature importances from the RandomForest model\n",
        "# # importances = rf.feature_importances_\n",
        "# # feature_names = X_train_rf.columns\n",
        "\n",
        "# # Summarize feature importances\n",
        "# # feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "# # feature_importances.sort_values('importance', ascending=False, inplace=True)\n",
        "\n",
        "# # Plot feature importances\n",
        "# # plt.figure(figsize=(12, 6))\n",
        "# # sns.barplot(x='importance', y='feature', data=feature_importances.head(20))\n",
        "# # plt.title('Top 20 Important Features')\n",
        "# # plt.show()\n",
        "\n",
        "# # Select the top N important features based on the RandomForest model\n",
        "# # top_features = feature_importances.head(20)['feature'].tolist()\n",
        "\n",
        "# # Reduced feature sets excluding 'Date' if it's present\n",
        "# # X_train_reduced = X_train_fe[top_features + ['Date']]\n",
        "# # X_val_reduced = X_val_fe[top_features + ['Date']]\n",
        "# # X_test_reduced = X_test_fe[top_features + ['Date']]\n",
        "# # unseen_features_reduced = unseen_features_fe[top_features + ['Date']]\n",
        "\n",
        "# # Save the reduced datasets, including the unseen data (replace with appropriate dataset)\n",
        "# # save_dataset(X_train_reduced, 'your_dataset.csv')\n",
        "# # save_dataset(pd.DataFrame(y_train_fe, columns=['Prediction1']), 'your_dataset.csv')\n",
        "# # save_dataset(X_val_reduced, 'your_dataset.csv')\n",
        "# # save_dataset(pd.DataFrame(y_val_fe, columns=['Prediction1']), 'your_dataset.csv')\n",
        "# # save_dataset(X_test_reduced, 'your_dataset.csv')\n",
        "# # save_dataset(pd.DataFrame(y_test_fe, columns=['Prediction1']), 'your_dataset.csv')\n",
        "# # save_dataset(unseen_features_reduced, 'your_dataset.csv')\n",
        "# # save_dataset(pd.DataFrame(unseen_target_fe, columns=['Prediction1']), 'your_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHH6UQkZn8L"
      },
      "outputs": [],
      "source": [
        "# CELL 5.5: Cross-Validation with Reduced Feature Set\n",
        "\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# import pandas as pd\n",
        "\n",
        "# # Define the base directory for file paths\n",
        "# base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# # Function to load a dataset\n",
        "# def load_dataset(filename):\n",
        "#     full_path = f'{base_dir}/{filename}'\n",
        "#     return pd.read_csv(full_path)\n",
        "\n",
        "# # Function to save a dataset\n",
        "# def save_dataset(df, filename):\n",
        "#     full_path = f'{base_dir}/{filename}'\n",
        "#     df.to_csv(full_path, index=False)\n",
        "\n",
        "# # Load datasets with relevant columns (replace with appropriate dataset filenames)\n",
        "# X_train_reduced = load_dataset('your_dataset.csv')\n",
        "# y_train_reduced = load_dataset('your_dataset.csv')['Prediction1']\n",
        "# # Additional datasets can be loaded as needed\n",
        "\n",
        "# # Initialize the RandomForest model with optimal hyperparameters\n",
        "# rf_model = RandomForestRegressor(\n",
        "#     n_estimators=200,  # Adjust with actual best parameters\n",
        "#     max_features='sqrt',\n",
        "#     max_depth=10,\n",
        "#     min_samples_split=5,\n",
        "#     min_samples_leaf=1,\n",
        "#     bootstrap=True,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# # Perform cross-validation\n",
        "# cv_scores = cross_val_score(rf_model, X_train_reduced, y_train_reduced, cv=5, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# # Calculate the mean and standard deviation of the cross-validation scores\n",
        "# cv_mae_mean = -cv_scores.mean()\n",
        "# cv_mae_std = cv_scores.std()\n",
        "\n",
        "# print(f\"Cross-validated MAE: {cv_mae_mean} +/- {cv_mae_std}\")\n",
        "\n",
        "# # Optional: Save any new datasets if necessary\n",
        "# # Example: save_dataset(X_train_reduced, 'Updated_Reduced_Train_Features_PostCV.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG8-jUCz8nWM"
      },
      "source": [
        "#Work In Progress\n",
        "21/02/24\n",
        "10:24am"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkqLfRHVmKYG"
      },
      "outputs": [],
      "source": [
        "# CELL 6.1.1: Model Training with Hyperparameter Tuning Values\n",
        "\n",
        "#!pip install shap\n",
        "\n",
        "#from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "#from xgboost import XGBRegressor\n",
        "#import pandas as pd\n",
        "#import shap\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the base directory for file paths\n",
        "#base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load datasets\n",
        "#def load_dataset(filename):\n",
        "#    full_path = f'{base_dir}/{filename}'\n",
        "#    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets correctly\n",
        "#X_train_fe = load_dataset('M_Engineered_Train_Features.csv').drop(columns='Date', errors='ignore')\n",
        "#y_train_fe = load_dataset('M_Engineered_Train_Target.csv')['Prediction1']\n",
        "#X_val_fe = load_dataset('N_Engineered_Val_Features.csv').drop(columns='Date', errors='ignore')\n",
        "#y_val_fe = load_dataset('N_Engineered_Val_Target.csv')['Prediction1']\n",
        "#X_test_fe = load_dataset('O_Engineered_Test_Features.csv').drop(columns='Date', errors='ignore')\n",
        "#y_test_fe = load_dataset('O_Engineered_Test_Target.csv')['Prediction1']\n",
        "#unseen_features_fe = load_dataset('P_Engineered_Unseen_Features.csv').drop(columns='Date', errors='ignore')\n",
        "#unseen_target_fe = load_dataset('P_Engineered_Unseen_Target.csv')['Prediction1']\n",
        "\n",
        "# Initialize RandomForestRegressor with upper quartile complexity parameters\n",
        "#rf_model = RandomForestRegressor(n_estimators=750, max_depth=23, min_samples_split=2, min_samples_leaf=3, random_state=42)\n",
        "#rf_model.fit(X_train_fe, y_train_fe)\n",
        "#print(\"RandomForestRegressor (Upper Quartile Complexity Params) trained.\")\n",
        "# Display feature importances\n",
        "#rf_importances = pd.Series(rf_model.feature_importances_, index=X_train_fe.columns)\n",
        "#rf_importances.nlargest(10).plot(kind='barh')\n",
        "#plt.title('Top 10 Feature Importances for RandomForestRegressor (Upper Quartile Complexity Params)')\n",
        "#plt.show()\n",
        "\n",
        "# Initialize XGBRegressor with upper quartile complexity parameters\n",
        "#xgb_model = XGBRegressor(n_estimators=750, max_depth=8, learning_rate=0.075, subsample=1.0, colsample_bytree=1.0, random_state=42)\n",
        "#xgb_model.fit(X_train_fe, y_train_fe)\n",
        "#print(\"XGBRegressor (Upper Quartile Complexity Params) trained.\")\n",
        "# Display feature importances\n",
        "#xgb_importances = pd.Series(xgb_model.feature_importances_, index=X_train_fe.columns)\n",
        "#xgb_importances.nlargest(10).plot(kind='barh')\n",
        "#plt.title('Top 10 Feature Importances for XGBRegressor (Upper Quartile Complexity Params)')\n",
        "#plt.show()\n",
        "\n",
        "# Initialize GradientBoostingRegressor with upper quartile complexity parameters\n",
        "#gbm_model = GradientBoostingRegressor(n_estimators=750, max_depth=8, learning_rate=0.075, subsample=1.0, random_state=42)\n",
        "#gbm_model.fit(X_train_fe, y_train_fe)\n",
        "#print(\"GradientBoostingRegressor (Upper Quartile Complexity Params) trained.\")\n",
        "# Display feature importances\n",
        "#gbm_importances = pd.Series(gbm_model.feature_importances_, index=X_train_fe.columns)\n",
        "#gbm_importances.nlargest(10).plot(kind='barh')\n",
        "#plt.title('Top 10 Feature Importances for GradientBoostingRegressor (Upper Quartile Complexity Params)')\n",
        "#plt.show()\n",
        "\n",
        "# SHAP Analysis for RandomForestRegressor (example)\n",
        "#explainer = shap.TreeExplainer(rf_model)\n",
        "#shap_values = explainer.shap_values(X_train_fe)\n",
        "#shap.summary_plot(shap_values, X_train_fe, plot_type=\"bar\")\n",
        "\n",
        "# Correcting the print statements to reflect the correct datasets\n",
        "#print(\"Shape of X_train_fe:\", X_train_fe.shape)\n",
        "#print(\"Shape of y_train_fe:\", y_train_fe.shape)\n",
        "#print(\"Shape of X_val_fe:\", X_val_fe.shape)\n",
        "#print(\"Shape of y_val_fe:\", y_val_fe.shape)\n",
        "#print(\"Shape of X_test_fe:\", X_test_fe.shape)\n",
        "#print(\"Shape of y_test_fe:\", y_test_fe.shape)\n",
        "#print(\"Shape of unseen_features_fe:\", unseen_features_fe.shape)\n",
        "#print(\"Shape of unseen_target_fe:\", unseen_target_fe.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLg8mfVnTq2S"
      },
      "outputs": [],
      "source": [
        "# CELL 6.1.1: Model Training with Hyperparameter Tuning Values\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}/{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets correctly\n",
        "X_train_fe = load_dataset('M_Engineered_Train_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_train_fe = load_dataset('M_Engineered_Train_Target.csv')['Prediction1']\n",
        "X_val_fe = load_dataset('N_Engineered_Val_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_val_fe = load_dataset('N_Engineered_Val_Target.csv')['Prediction1']\n",
        "X_test_fe = load_dataset('O_Engineered_Test_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_test_fe = load_dataset('O_Engineered_Test_Target.csv')['Prediction1']\n",
        "unseen_features_fe = load_dataset('P_Engineered_Unseen_Features.csv').drop(columns='Date', errors='ignore')\n",
        "unseen_target_fe = load_dataset('P_Engineered_Unseen_Target.csv')['Prediction1']\n",
        "\n",
        "# Define a function for training and evaluating a model\n",
        "def train_evaluate_rf(n_estimators, max_depth, min_samples_split, X_train, y_train, X_val, y_val):\n",
        "    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                               min_samples_split=min_samples_split, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return mse, rmse\n",
        "\n",
        "# Assuming n_estimators and max_depth are constants\n",
        "n_estimators_const = 300\n",
        "max_depth_const = 10\n",
        "\n",
        "# Variable parameter range for min_samples_split\n",
        "min_samples_split_values = [2, 5, 10, 15, 20]  # Assuming these are the values you want to iterate over\n",
        "\n",
        "# Train and evaluate the model for each min_samples_split value\n",
        "for min_samples in min_samples_split_values:\n",
        "    mse, rmse = train_evaluate_rf(n_estimators_const, max_depth_const, min_samples,\n",
        "                                  X_train_fe, y_train_fe, X_val_fe, y_val_fe)\n",
        "    print(f\"Trained RandomForestRegressor with min_samples_split={min_samples}. MSE: {mse}, RMSE: {rmse}\")\n",
        "# Train and evaluate the model for each min_samples_split value\n",
        "for min_samples in min_samples_split_values:\n",
        "    mse, rmse = train_evaluate_rf(n_estimators_const, max_depth_const, min_samples,\n",
        "                                  X_train_fe, y_train_fe, X_val_fe, y_val_fe)\n",
        "    print(f\"Trained RandomForestRegressor with min_samples_split={min_samples}. MSE: {mse}, RMSE: {rmse}\")\n",
        "\n",
        "# You can now analyze the mse_results to determine the best n_estimators value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta0LrEK9VJLd"
      },
      "outputs": [],
      "source": [
        "# CELL 6.1.2: Model Evaluation with Custom Metrics for Varying n_estimators\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}/{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets correctly\n",
        "X_train_fe = load_dataset('M_Engineered_Train_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_train_fe = load_dataset('M_Engineered_Train_Target.csv')['Prediction1']\n",
        "X_val_fe = load_dataset('N_Engineered_Val_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_val_fe = load_dataset('N_Engineered_Val_Target.csv')['Prediction1']\n",
        "X_test_fe = load_dataset('O_Engineered_Test_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_test_fe = load_dataset('O_Engineered_Test_Target.csv')['Prediction1']\n",
        "unseen_features_fe = load_dataset('P_Engineered_Unseen_Features.csv').drop(columns='Date', errors='ignore')\n",
        "unseen_target_fe = load_dataset('P_Engineered_Unseen_Target.csv')['Prediction1']\n",
        "actual_results_val = load_dataset('Actual_Results_Val.csv')\n",
        "\n",
        "# Initialize the models with different max_depth, keeping n_estimators and min_samples_split constant\n",
        "max_depth_values = [1, 8, 15, 23, 30]  # Example values, replace with your actual range\n",
        "models = {}\n",
        "for depth in max_depth_values:\n",
        "    models[depth] = RandomForestRegressor(n_estimators=300, max_depth=depth, min_samples_split=10, random_state=42)\n",
        "    models[depth].fit(X_train_fe, y_train_fe)\n",
        "\n",
        "# Custom Evaluation Metrics Function\n",
        "def calculate_custom_metrics(y_actual, y_pred):\n",
        "    units_error = np.abs(y_actual - y_pred).mean()\n",
        "    percentage_error = (units_error / y_actual.max()) * 100\n",
        "    mse = mean_squared_error(y_actual, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return units_error, percentage_error, mse, rmse\n",
        "\n",
        "# Calculate metrics for each min_samples_split value and store in a dictionary\n",
        "metrics = {}\n",
        "y_val_actual = actual_results_val['Draw1']  # Replace 'Draw1' with the actual target column name\n",
        "\n",
        "for min_samples in min_samples_split_values:\n",
        "    model = RandomForestRegressor(n_estimators=n_estimators_const, max_depth=max_depth_const,\n",
        "                                  min_samples_split=min_samples, random_state=42)\n",
        "    model.fit(X_train_fe, y_train_fe)\n",
        "    y_pred = model.predict(X_val_fe)\n",
        "    metrics[min_samples] = calculate_custom_metrics(y_val_actual, y_pred)\n",
        "\n",
        "# Display the metrics for each min_samples_split value\n",
        "for min_samples, (units_error, percentage_error, mse, rmse) in metrics.items():\n",
        "    print(f\"Model with min_samples_split={min_samples} - Units Error: {units_error}, Percentage Error: {percentage_error}%, MSE: {mse}, RMSE: {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxBceOnOUYKm"
      },
      "outputs": [],
      "source": [
        "# CELL 6.1.2: Model Evaluation with Custom Metrics\n",
        "\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "#from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "#base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load datasets\n",
        "#def load_dataset(filename):\n",
        "#    full_path = f'{base_dir}{filename}'\n",
        "#    return pd.read_csv(full_path)\n",
        "\n",
        "# Function to save datasets\n",
        "#def save_dataset(df, filename):\n",
        "#    full_path = f'{base_dir}{filename}'\n",
        "#    df.to_csv(full_path, index=False)\n",
        "\n",
        "# Load the validation feature set and actual results for validation set\n",
        "#X_val_fe = load_dataset('N_Engineered_Val_Features.csv').drop(columns='Date', errors='ignore')\n",
        "#actual_results_val = load_dataset('Actual_Results_Val.csv')\n",
        "\n",
        "# Assuming models are defined and loaded correctly elsewhere in your code\n",
        "# Generate and save predictions for each model\n",
        "#y_pred_rf = rf_model.predict(X_val_fe)\n",
        "#save_dataset(pd.DataFrame(y_pred_rf, columns=['Prediction1']), 'RF_Val_Predictions.csv')\n",
        "\n",
        "#y_pred_xgb = xgb_model.predict(X_val_fe)\n",
        "#save_dataset(pd.DataFrame(y_pred_xgb, columns=['Prediction1']), 'XGB_Val_Predictions.csv')\n",
        "\n",
        "#y_pred_gbm = gbm_model.predict(X_val_fe)\n",
        "#save_dataset(pd.DataFrame(y_pred_gbm, columns=['Prediction1']), 'GBM_Val_Predictions.csv')\n",
        "\n",
        "# Custom Evaluation Metrics Function\n",
        "#def calculate_custom_metrics(y_actual, y_pred):\n",
        "#    units_error = np.abs(y_actual - y_pred)\n",
        "#    percentage_error = (units_error / 36) * 100  # Assuming 36 is the maximum value in your case\n",
        "#    mse = mean_squared_error(y_actual, y_pred)\n",
        "#    rmse = np.sqrt(mse)\n",
        "#    return np.mean(units_error), np.mean(percentage_error), mse, rmse\n",
        "\n",
        "# Calculate actual 'Draw1' values for the validation set\n",
        "#y_val_actual = actual_results_val['Draw1']\n",
        "\n",
        "# Calculate metrics for each model\n",
        "#metrics_rf = calculate_custom_metrics(y_val_actual, y_pred_rf)\n",
        "#metrics_xgb = calculate_custom_metrics(y_val_actual, y_pred_xgb)\n",
        "#metrics_gbm = calculate_custom_metrics(y_val_actual, y_pred_gbm)\n",
        "\n",
        "# Display the metrics\n",
        "#print(f\"RandomForestRegressor - Units Error: {metrics_rf[0]}, Percentage Error: {metrics_rf[1]}%, MSE: {metrics_rf[2]}, RMSE: {metrics_rf[3]}\")\n",
        "#print(f\"XGBRegressor - Units Error: {metrics_xgb[0]}, Percentage Error: {metrics_xgb[1]}%, MSE: {metrics_xgb[2]}, RMSE: {metrics_xgb[3]}\")\n",
        "#print(f\"GradientBoostingRegressor - Units Error: {metrics_gbm[0]}, Percentage Error: {metrics_gbm[1]}%, MSE: {metrics_gbm[2]}, RMSE: {metrics_gbm[3]}\")\n",
        "\n",
        "# Note: Ensure `rf_model`, `xgb_model`, and `gbm_model` are correctly defined and trained models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPi0r8dSHP-x"
      },
      "outputs": [],
      "source": [
        "# CELL 6.1.3: LIME Explanations and Residual Plots\n",
        "\n",
        "!pip install lime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load datasets\n",
        "X_val = load_dataset('N_Engineered_Val_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_val_actual = load_dataset('Actual_Results_Val.csv')['Draw1']\n",
        "y_pred_rf = load_dataset('RF_Val_Predictions.csv')['Prediction1']\n",
        "y_pred_xgb = load_dataset('XGB_Val_Predictions.csv')['Prediction1']\n",
        "y_pred_gbm = load_dataset('GBM_Val_Predictions.csv')['Prediction1']\n",
        "\n",
        "# Create a Lime Explainer object\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    X_val.values,\n",
        "    feature_names=X_val.columns,\n",
        "    class_names=['Prediction'],\n",
        "    verbose=True,\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "# Choose a random instance to explain\n",
        "i = np.random.randint(0, X_val.shape[0])\n",
        "\n",
        "# Explain the prediction of this instance\n",
        "exp_rf = explainer.explain_instance(X_val.values[i], rf_model.predict, num_features=5)\n",
        "exp_xgb = explainer.explain_instance(X_val.values[i], xgb_model.predict, num_features=5)\n",
        "exp_gbm = explainer.explain_instance(X_val.values[i], gbm_model.predict, num_features=5)\n",
        "\n",
        "# Show the explanation\n",
        "exp_rf.show_in_notebook(show_table=True)\n",
        "exp_xgb.show_in_notebook(show_table=True)\n",
        "exp_gbm.show_in_notebook(show_table=True)\n",
        "\n",
        "# Residual Plots\n",
        "def plot_residuals(y_actual, y_pred, title):\n",
        "    residuals = y_actual - y_pred\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=y_actual, y=residuals, alpha=0.5)\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot residuals for each model\n",
        "plot_residuals(y_val_actual, y_pred_rf, 'RandomForestRegressor Residuals')\n",
        "plot_residuals(y_val_actual, y_pred_xgb, 'XGBRegressor Residuals')\n",
        "plot_residuals(y_val_actual, y_pred_gbm, 'GradientBoostingRegressor Residuals')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hth08qUpj11I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j-herQI2j2ul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj_u3KZh03RU"
      },
      "outputs": [],
      "source": [
        "# CELL 6.2: Model Evaluation (Learning Curves, SHAP Values, Statistical Measures)\n",
        "\n",
        "import math\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory path for loading\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets for training, validation, and testing\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('P_Reduced_Val_Features.csv').drop('Date', axis=1)\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "X_test = load_dataset('Q_Reduced_Test_Features.csv').drop('Date', axis=1)\n",
        "y_test = load_dataset('Q_Reduced_Test_Target.csv')['Prediction1']\n",
        "\n",
        "# Initialize and train models\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "xgb_model = XGBRegressor(random_state=42)\n",
        "gbm_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Define the function to evaluate the model and print key metrics\n",
        "def evaluate_model(model, X_train, y_train, X_val, y_val, feature_names):\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "    mae_val = mean_absolute_error(y_val, y_val_pred)\n",
        "    rmse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    rmse_val = math.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Training - MAE: {mae_train}, RMSE: {rmse_train}\")\n",
        "    print(f\"Validation - MAE: {mae_val}, RMSE: {rmse_val}\")\n",
        "\n",
        "# Define the function to plot learning curves and SHAP values\n",
        "def evaluate_model_with_curves_and_shap(model, X_train, y_train, X_val, y_val, features):\n",
        "    # Learning Curve\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, n_jobs=-1, cv=5, train_sizes=np.linspace(.1, 1.0, 10), verbose=0)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "    # SHAP Values (This part might take some time to compute)\n",
        "    explainer = shap.Explainer(model, X_train)\n",
        "    shap_values = explainer(X_val)\n",
        "    shap.summary_plot(shap_values, X_val, feature_names=features)\n",
        "\n",
        "    # Statistical Measures\n",
        "    predictions = model.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, predictions)\n",
        "    r2 = r2_score(y_val, predictions)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "    print(f\"R-squared: {r2}\")\n",
        "\n",
        "# Evaluate the models with both functions\n",
        "models = [rf_model, xgb_model, gbm_model]\n",
        "model_names = [\"RandomForestRegressor\", \"XGBRegressor\", \"GradientBoostingRegressor\"]\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    print(f\"Evaluating {name}:\")\n",
        "    evaluate_model(model, X_train, y_train, X_val, y_val, X_train.columns.tolist())\n",
        "    evaluate_model_with_curves_and_shap(model, X_train, y_train, X_val, y_val, X_train.columns.tolist())\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Calculate RMSE for each model on the test set in units\n",
        "rmse_rf_units = math.sqrt(mean_squared_error(y_test, rf_model.predict(X_test)))\n",
        "rmse_xgb_units = math.sqrt(mean_squared_error(y_test, xgb_model.predict(X_test)))\n",
        "rmse_gbr_units = math.sqrt(mean_squared_error(y_test, gbm_model.predict(X_test)))\n",
        "\n",
        "print(f\"RandomForestRegressor RMSE (Units): {rmse_rf_units}\")\n",
        "print(f\"XGBoost Regressor RMSE (Units): {rmse_xgb_units}\")\n",
        "print(f\"GBM Regressor RMSE (Units): {rmse_gbr_units}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnoVdBTqmKzU"
      },
      "outputs": [],
      "source": [
        "# CELL 6.3: Model Evaluation LIME (Local Interpretable Model-Agnostic Explanations)\n",
        "\n",
        "!pip install lime\n",
        "\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load and save a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the reduced datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)  # Exclude 'Date' column\n",
        "X_test = load_dataset('Q_Reduced_Test_Features.csv').drop('Date', axis=1)    # Exclude 'Date' column\n",
        "\n",
        "# Assuming rf_model is already trained with the reduced feature set\n",
        "\n",
        "# Initialize the explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.values,\n",
        "    feature_names=[name for name in X_train.columns if name != 'Date'],  # Exclude 'Date' column\n",
        "    class_names=['Prediction1'],  # or the actual class names if it's a classification problem\n",
        "    # categorical_features=categorical_indices,  # replace with the actual indices if you have categorical features\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "# Explain an individual prediction from the test set\n",
        "i = 10  # Index of the chosen instance\n",
        "exp = explainer.explain_instance(X_test.iloc[i].values, rf_model.predict, num_features=5)\n",
        "\n",
        "# Visualize the explanation\n",
        "exp.show_in_notebook(show_table=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr6BlJe0L0oi"
      },
      "outputs": [],
      "source": [
        "# CELL 7.1: ARIMA (Autoregressive Integrated Moving Average) Model Implementation\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv')\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')\n",
        "\n",
        "# Select the Target Variable for ARIMA\n",
        "arima_series = y_train['Prediction1']\n",
        "\n",
        "# Check for Stationarity and determine ARIMA parameters (p, d, q)\n",
        "plot_acf(arima_series)\n",
        "plot_pacf(arima_series)\n",
        "plt.show()\n",
        "\n",
        "# Assuming parameters are determined here (p, d, q)\n",
        "p, d, q = 1, 1, 1  # Replace with actual parameters determined\n",
        "\n",
        "# Fit the ARIMA model\n",
        "arima_model = ARIMA(arima_series, order=(p, d, q))\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "# Model Evaluation\n",
        "# Example: print(arima_result.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9HPN1YX32S2"
      },
      "outputs": [],
      "source": [
        "# CELL 7.2: SARIMA (Seasonal ARIMA) Model Implementation\n",
        "\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')\n",
        "\n",
        "# Select the Target Variable for SARIMA\n",
        "sarima_series = y_train['Prediction1']\n",
        "\n",
        "# Assuming you have seasonal data, you would need to perform EDA to determine these parameters\n",
        "# For this example, let's assume the data has a yearly seasonality with s=12\n",
        "\n",
        "# Define the SARIMA model parameters\n",
        "p, d, q = 1, 1, 1  # These are the ARIMA parameters\n",
        "P, D, Q, s = 1, 1, 1, 12  # These are the seasonal parameters\n",
        "\n",
        "# Fit the SARIMA model\n",
        "sarima_model = SARIMAX(sarima_series, order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
        "sarima_result = sarima_model.fit()\n",
        "\n",
        "# Model Evaluation\n",
        "# You can evaluate the model using metrics like AIC, BIC, etc.\n",
        "print(sarima_result.summary())\n",
        "\n",
        "# You might also want to perform diagnostics and validation\n",
        "sarima_result.plot_diagnostics(figsize=(15, 12))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCWs6bcUQiyF"
      },
      "outputs": [],
      "source": [
        "# CELL 7.3: Prophet Model Implementation\n",
        "\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the feature dataset to extract the 'Date' column\n",
        "X_train_features = load_dataset('O_Reduced_Train_Features.csv')\n",
        "\n",
        "# Load the target dataset\n",
        "y_train_target = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Assuming the datasets are aligned and have the same length\n",
        "# Combine 'Date' from features and 'Prediction1' from target into a single DataFrame\n",
        "prophet_df = pd.DataFrame({\n",
        "    'ds': pd.to_datetime(X_train_features['Date']),  # Convert 'Date' to datetime if not already\n",
        "    'y': y_train_target\n",
        "})\n",
        "\n",
        "# Fit the Prophet model\n",
        "prophet_model = Prophet()\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "# Create future dataframe for predictions\n",
        "future = prophet_model.make_future_dataframe(periods=365)  # Adjust periods as needed for your forecast horizon\n",
        "\n",
        "# Forecast\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# Plot the forecast\n",
        "fig1 = prophet_model.plot(forecast)\n",
        "\n",
        "# Plot the forecast components\n",
        "fig2 = prophet_model.plot_components(forecast)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gjX6K2cQjSV"
      },
      "outputs": [],
      "source": [
        "# CELL 7.4: Implementing Recurrent Neural Networks (RNNs)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv')\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')\n",
        "\n",
        "# Assuming 'Date' column exists directly in the features dataset\n",
        "y_train['Date'] = pd.to_datetime(X_train_features['Date'])\n",
        "\n",
        "# Prepare the target variable\n",
        "#y_train['Date'] = pd.to_datetime(y_train[['Year', 'Month', 'Day']])\n",
        "#y_train = y_train.set_index('Date')['Prediction1']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "y_train_scaled = scaler.fit_transform(y_train['Prediction1'].values.reshape(-1, 1))\n",
        "# Convert the time series data into supervised learning problem\n",
        "def create_dataset(series, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(series) - time_step - 1):\n",
        "        a = series[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(series[i + time_step, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Assuming time_step of 5\n",
        "time_step = 5\n",
        "X_train, y_train_rnn = create_dataset(y_train_scaled, time_step)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(50, return_sequences=True, input_shape=(time_step, 1)),\n",
        "    SimpleRNN(50),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(0.01), loss='mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train_rnn, epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_predict = model.predict(X_train)\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "\n",
        "# Calculate RMSE performance metrics\n",
        "math.sqrt(mean_squared_error(y_train_rnn, train_predict))\n",
        "\n",
        "# Note: You would typically also want to prepare a validation set and possibly a test set to evaluate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAvX_2HFL510"
      },
      "outputs": [],
      "source": [
        "# CELL 7.5: Implementing Long Short-Term Memory Neural Networks (LSTM)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets - assuming 'O_Engineered_Train_Features.csv' is preprocessed for LSTM\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape input to be [samples, time steps, features] for LSTM\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=32, verbose=2, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = model.predict(X_train_reshaped)\n",
        "train_pred = scaler.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler.inverse_transform(y_train_scaled)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zduATdfpL6Eo"
      },
      "outputs": [],
      "source": [
        "# CELL 7.6: 1D Convolutional Neural Networks (CNNs) for Time Series Forecasting Implementation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets - assuming 'O_Engineered_Train_Features.csv' is preprocessed for CNN\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape input to be [samples, time steps, features] for CNN\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=32, verbose=2, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = model.predict(X_train_reshaped)\n",
        "train_pred = scaler.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler.inverse_transform(y_train_scaled)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyBuxEFmkXlg"
      },
      "outputs": [],
      "source": [
        "# CELL 7.7: Simple Transformer with Attention for Time Series Forecasting Implementation\n",
        "\n",
        "!pip install tensorflow-addons\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape input for Transformer\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the Transformer block\n",
        "def transformer_block(inputs):\n",
        "    attention_output = MultiHeadAttention(num_heads=2, key_dim=64)(inputs, inputs)\n",
        "    attention_output = Dropout(0.1)(attention_output)\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
        "\n",
        "    outputs = GlobalAveragePooling1D()(attention_output)\n",
        "    outputs = Dropout(0.1)(outputs)\n",
        "    outputs = Dense(20, activation='relu')(outputs)\n",
        "    outputs = Dense(1)(outputs)\n",
        "    return outputs\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
        "\n",
        "# Transformer layers\n",
        "transformer_output = transformer_block(inputs)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=inputs, outputs=transformer_output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=32, verbose=2, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = model.predict(X_train_reshaped)\n",
        "train_pred = scaler.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler.inverse_transform(y_train_scaled)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YibbjAVLkXwK"
      },
      "outputs": [],
      "source": [
        "# CELL 7.8: Hybrid CNN-LSTM Model for Time Series Forecasting\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape input for the CNN-LSTM model\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the Hybrid CNN-LSTM model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=32, verbose=2, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = model.predict(X_train_reshaped)\n",
        "train_pred = scaler.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler.inverse_transform(y_train_scaled)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIbrfC9_kX9E"
      },
      "outputs": [],
      "source": [
        "# CELL 7.9: Extreme Gradient Boosting (XGBoost)\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the datasets into DMatrix, which is a data structure unique to XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "# Set the parameters for the XGBoost model\n",
        "params = {\n",
        "    'max_depth': 6,  # Maximum depth of a tree\n",
        "    'eta': 0.3,      # Learning rate\n",
        "    'objective': 'reg:squarederror',  # Regression with squared loss\n",
        "    'subsample': 0.8,  # Subsample ratio of the training instances\n",
        "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\n",
        "    'eval_metric': 'rmse'  # Evaluation metric\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "num_boost_round = 100\n",
        "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
        "model = xgb.train(params, dtrain, num_boost_round, evals=evals, early_stopping_rounds=10)\n",
        "\n",
        "# Predict on validation set\n",
        "y_pred = model.predict(dval)\n",
        "val_rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "print(f'Validation RMSE: {val_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A8efmPA0ive"
      },
      "outputs": [],
      "source": [
        "# CELL 7.10: Modified LightGBM  Model Implementation with Pre-Processed Data\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('P_Reduced_Val_Features.csv').drop('Date', axis=1)\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "\n",
        "# Initialize the LGBMRegressor\n",
        "lgb_model = LGBMRegressor(\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=100,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.9,\n",
        "    objective='regression',\n",
        "    metric='rmse'\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_lgb = lgb_model.predict(X_val)\n",
        "val_rmse_lgb = mean_squared_error(y_val, y_pred_lgb, squared=False)\n",
        "print(f'LightGBM Validation RMSE: {val_rmse_lgb}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCNkJALf05Xe"
      },
      "outputs": [],
      "source": [
        "# CELL 7.11: CatBoost Model Implementation\n",
        "\n",
        "!pip install catboost\n",
        "from catboost import CatBoostRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the datasets\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('P_Reduced_Val_Features.csv').drop('Date', axis=1)\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "\n",
        "# Initialize CatBoost Regressor\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    eval_metric='RMSE',\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_cat = catboost_model.predict(X_val)\n",
        "val_rmse_cat = mean_squared_error(y_val, y_pred_cat, squared=False)\n",
        "print(f'CatBoost Validation RMSE: {val_rmse_cat}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfjC7i0Aw2eX"
      },
      "outputs": [],
      "source": [
        "# CELL 8.1: Ensemble Models\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets with reduced features for training\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('P_Reduced_Val_Features.csv').drop('Date', axis=1)\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "\n",
        "# Initialize models\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "xgb_model = XGBRegressor(random_state=42)\n",
        "gbm_model = GradientBoostingRegressor(random_state=42)\n",
        "lgbm_model = LGBMRegressor(random_state=42)\n",
        "catboost_model = CatBoostRegressor(random_state=42, silent=True)\n",
        "\n",
        "# Train each model\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Create ensemble combinations\n",
        "ensemble1 = VotingRegressor([('rf', rf_model), ('xgb', xgb_model)])\n",
        "ensemble2 = VotingRegressor([('gbm', gbm_model), ('lgbm', lgbm_model)])\n",
        "ensemble3 = VotingRegressor([('rf', rf_model), ('cat', catboost_model), ('lgbm', lgbm_model)])\n",
        "ensemble4 = VotingRegressor([('xgb', xgb_model), ('gbm', gbm_model), ('cat', catboost_model)])\n",
        "\n",
        "# List of ensembles\n",
        "ensembles = [ensemble1, ensemble2, ensemble3, ensemble4]\n",
        "\n",
        "# Evaluate each ensemble\n",
        "for i, ensemble in enumerate(ensembles, 1):\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    predictions = ensemble.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, predictions)\n",
        "    r2 = r2_score(y_val, predictions)\n",
        "    print(f\"Ensemble {i} - MSE: {mse}, R-squared: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHfYLP7xipTE"
      },
      "outputs": [],
      "source": [
        "# CELL 8.2: Stacking Ensemble Model\n",
        "\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for file paths and load the datasets\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "X_train = load_dataset('O_Reduced_Train_Features.csv').drop('Date', axis=1)\n",
        "y_train = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('P_Reduced_Val_Features.csv').drop('Date', axis=1)\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('rf', RandomForestRegressor(random_state=42)),\n",
        "    ('xgb', XGBRegressor(random_state=42)),\n",
        "    ('gbm', GradientBoostingRegressor(random_state=42)),\n",
        "    ('lgbm', LGBMRegressor(random_state=42)),\n",
        "    ('catboost', CatBoostRegressor(random_state=42, silent=True))\n",
        "]\n",
        "\n",
        "# Initialize Stacking Regressor with a linear regression model as the final estimator\n",
        "stacked_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LinearRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Fit the stacked model\n",
        "stacked_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_stacked = stacked_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "stacked_mae = mean_absolute_error(y_val, y_pred_stacked)\n",
        "stacked_rmse = mean_squared_error(y_val, y_pred_stacked, squared=False)\n",
        "stacked_r2 = r2_score(y_val, y_pred_stacked)\n",
        "\n",
        "print(f\"Stacked Ensemble Model - MAE: {stacked_mae}, RMSE: {stacked_rmse}, R-squared: {stacked_r2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eCLYlirydcz"
      },
      "outputs": [],
      "source": [
        "# Cell 8.3: Stacking Ensemble with Time Series Models\n",
        "\n",
        "#from sklearn.ensemble import StackingRegressor\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.metrics import mean_squared_error, r2_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "#\n",
        "\n",
        "# Assume arima_predict, sarima_predict, prophet_predict, etc., are arrays containing predictions from corresponding models\n",
        "# You need to run ARIMA, SARIMA, Prophet, RNN, LSTM, CNN, etc., separately and store their predictions\n",
        "\n",
        "# Combine predictions as additional features\n",
        "#X_combined = np.column_stack([X_train, arima_predict, sarima_predict, prophet_predict, ...])\n",
        "\n",
        "# Define the base models for stacking\n",
        "#base_models = [\n",
        "#    ('xgb', XGBRegressor(random_state=42)),\n",
        "#    ('lgbm', LGBMRegressor(random_state=42)),\n",
        "#    ('catboost', CatBoostRegressor(random_state=42, silent=True)),\n",
        "    # Add other models as needed\n",
        "#]\n",
        "\n",
        "# Initialize the Stacking Regressor\n",
        "#stacked_model = StackingRegressor(\n",
        "#    estimators=base_models,\n",
        "#    final_estimator=LinearRegression(),\n",
        "#    cv=5\n",
        "#)\n",
        "\n",
        "# Fit the model\n",
        "#stacked_model.fit(X_combined, y_train)\n",
        "\n",
        "# Prepare the validation set in a similar manner\n",
        "#X_val_combined = np.column_stack([X_val, arima_val_predict, sarima_val_predict, prophet_val_predict, ...])\n",
        "\n",
        "# Make predictions and evaluate\n",
        "#y_pred_stacked = stacked_model.predict(X_val_combined)\n",
        "#stacked_rmse = mean_squared_error(y_val, y_pred_stacked, squared=False)\n",
        "#stacked_r2 = r2_score(y_val, y_pred_stacked)\n",
        "\n",
        "#print(f\"Stacked Ensemble Model - RMSE: {stacked_rmse}, R-squared: {stacked_r2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10zYbnc2tFkm"
      },
      "outputs": [],
      "source": [
        "# CELL 8.4:  Hyperparameter tuning for Gradient Boosting Machine (GBM) using Optuna\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from scipy.stats import randint as sp_randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for file paths\n",
        "#base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load and save datasets\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets\n",
        "X_train = load_dataset('M_Engineered_Train_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_train = load_dataset('M_Engineered_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('N_Engineered_Val_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_val = load_dataset('N_Engineered_Val_Target.csv')['Prediction1']\n",
        "X_test = load_dataset('O_Engineered_Test_Features.csv').drop(columns='Date', errors='ignore')\n",
        "y_test = load_dataset('O_Engineered_Test_Target.csv')['Prediction1']\n",
        "unseen_features = load_dataset('P_Engineered_Unseen_Features.csv').drop(columns='Date', errors='ignore')\n",
        "unseen_target = load_dataset('P_Engineered_Unseen_Target.csv')['Prediction1']\n",
        "\n",
        "# Define the hyperparameter space for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500, 800, 1000],\n",
        "    'max_depth': [5, 8, 15, 25, 30],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found by grid search:\", grid_search.best_params_)\n",
        "print(\"Best score found by grid search:\", grid_search.best_score_)\n",
        "\n",
        "# Define the hyperparameter distribution for Random Search\n",
        "param_dist = {\n",
        "    'n_estimators': sp_randint(100, 1000),\n",
        "    'max_depth': sp_randint(5, 30),\n",
        "    'min_samples_split': sp_randint(2, 20)\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found by random search:\", random_search.best_params_)\n",
        "print(\"Best score found by random search:\", random_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlGCltKvtFya"
      },
      "outputs": [],
      "source": [
        "# CELL 8.5: Hyperparameter Tuning for Time Series Models (ARIMA, SARIMA, Prophet)\n",
        "\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install pmdarima\n",
        "!pip install statsmodels\n",
        "!pip install prophet\n",
        "\n",
        "# Import libraries\n",
        "import pmdarima as pm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import warnings\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "\n",
        "# Define the base directory for file paths\n",
        "#base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets, ensuring 'Date' column is excluded if present\n",
        "X_train_features = load_dataset('K_Engineered_Train_Features.csv')\n",
        "y_train_target = load_dataset('K_Engineered_Train_Target.csv')['Prediction1']\n",
        "\n",
        "\n",
        "\n",
        "# Assuming 'Date' in X_train_features is in datetime format and aligned with y_train_target\n",
        "prophet_df = pd.DataFrame({\n",
        "    'ds': pd.to_datetime(X_train_features['Date']),\n",
        "    'y': y_train_target\n",
        "})\n",
        "\n",
        "# Correctly prepare time_series_data for ARIMA and SARIMA (This assumes you've transformed your target series accordingly)\n",
        "time_series_data = y_train_target\n",
        "\n",
        "# ARIMA Tuning\n",
        "auto_arima_model = pm.auto_arima(time_series_data, seasonal=False, m=12,\n",
        "                                 start_p=0, start_q=0, max_order=6,\n",
        "                                 test='adf', error_action='ignore',\n",
        "                                 suppress_warnings=True, stepwise=True)\n",
        "\n",
        "print(f\"Best ARIMA Model: {auto_arima_model.order}\")\n",
        "\n",
        "# SARIMA Tuning\n",
        "p = d = q = range(0, 3)\n",
        "seasonal_pdq_combinations = [(x[0], x[1], x[2], 12) for x in itertools.product(p, d, q)]\n",
        "\n",
        "best_aic = float(\"inf\")\n",
        "best_pdq = best_seasonal_pdq = None\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "for pdq in itertools.product(p, d, q):\n",
        "    for seasonal_pdq in seasonal_pdq_combinations:\n",
        "        try:\n",
        "            model = SARIMAX(time_series_data, order=pdq, seasonal_order=seasonal_pdq,\n",
        "                            enforce_stationarity=False, enforce_invertibility=False)\n",
        "            results = model.fit()\n",
        "            if results.aic < best_aic:\n",
        "                best_aic = results.aic\n",
        "                best_pdq, best_seasonal_pdq = pdq, seasonal_pdq\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f\"Best SARIMA Model: {best_pdq}, Seasonal Order: {best_seasonal_pdq}, Best AIC: {best_aic}\")\n",
        "\n",
        "# Prophet Tuning\n",
        "# Convert the time series data to Prophet's expected format\n",
        "df_prophet = pd.DataFrame({'ds': y_train_target.index, 'y': time_series_data})\n",
        "\n",
        "# Define parameter grid for Prophet\n",
        "param_grid = {\n",
        "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
        "    'seasonality_prior_scale': [0.01, 0.1, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_params = None\n",
        "lowest_error = float('inf')\n",
        "\n",
        "# Iterate over all combinations of hyperparameters\n",
        "for params in itertools.product(*param_grid.values()):\n",
        "   # Create and fit a Prophet model\n",
        "    m = Prophet(changepoint_prior_scale=params[0], seasonality_prior_scale=params[1])\n",
        "    m.fit(prophet_df)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    df_cv = cross_validation(m, initial='730 days', period='180 days', horizon='365 days')\n",
        "    df_p = performance_metrics(df_cv, rolling_window=1)\n",
        "    error_metric = df_p['rmse'].values[0]  # Example: using RMSE\n",
        "\n",
        "    # Update the best parameters if the current model is better\n",
        "    if error_metric < lowest_error:\n",
        "        lowest_error = error_metric\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best Prophet Parameters: {best_params}, Lowest Error: {lowest_error}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kL-Uxfb0xMw"
      },
      "outputs": [],
      "source": [
        "# CELL 8.6: Hyperparameter Tuning for Neural Network Models (RNNs, LSTMs, CNNs, and hybrid models)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, SimpleRNN, Conv1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the engineered feature sets and corresponding targets, ensuring 'Date' column is excluded if present\n",
        "X_train_features = load_dataset('K_Engineered_Train_Features.csv')\n",
        "y_train_target = load_dataset('K_Engineered_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('L_Engineered_Val_Features.csv')\n",
        "y_val = load_dataset('L_Engineered_Val_Target.csv')['Prediction1']\n",
        "\n",
        "n_timesteps = 24\n",
        "n_features = 20\n",
        "\n",
        "n_samples_train = len(X_train_features) - n_timesteps\n",
        "n_samples_val = len(X_val) - n_timesteps\n",
        "\n",
        "X_train_reshaped = np.array([X_train_features.iloc[i:i+n_timesteps].values for i in range(n_samples_train)])\n",
        "y_train_reshaped = y_train_target.iloc[n_timesteps:].values\n",
        "X_val_reshaped = np.array([X_val.iloc[i:i+n_timesteps].values for i in range(n_samples_val)])\n",
        "y_val_reshaped = y_val.iloc[n_timesteps:].values\n",
        "\n",
        "# Convert data to float32 for compatibility with TensorFlow\n",
        "X_train_reshaped = X_train_reshaped.astype('float32')\n",
        "y_train_reshaped = y_train_reshaped.astype('float32')\n",
        "X_val_reshaped = X_train_reshaped.astype('float32')\n",
        "y_val_reshaped = y_train_reshaped.astype('float32')\n",
        "\n",
        "def build_lstm_model(learning_rate, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=n_units, input_shape=(n_timesteps, n_features)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def build_rnn_model(learning_rate, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=n_units, input_shape=(n_timesteps, n_features)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def build_cnn_model(learning_rate, n_filters, kernel_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=n_filters, kernel_size=kernel_size, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "n_units_options = [50, 100, 150]\n",
        "n_filters_options = [32, 64]\n",
        "kernel_sizes = [3, 5]\n",
        "\n",
        "best_lstm_score = float('inf')\n",
        "best_lstm_params = {}\n",
        "best_rnn_score = float('inf')\n",
        "best_rnn_params = {}\n",
        "best_cnn_score = float('inf')\n",
        "best_cnn_params = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for units in n_units_options:\n",
        "        lstm_model = build_lstm_model(lr, units)\n",
        "        lstm_model.fit(X_train_reshaped, y_train_reshaped, epochs=10, batch_size=32, verbose=0)\n",
        "        mse = mean_squared_error(y_val_reshaped, lstm_model.predict(X_val_reshaped))\n",
        "        if mse < best_lstm_score:\n",
        "            best_lstm_score = mse\n",
        "            best_lstm_params = {'learning_rate': lr, 'n_units': units}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for units in n_units_options:\n",
        "        rnn_model = build_rnn_model(lr, units)\n",
        "        rnn_model.fit(X_train_reshaped, y_train_reshaped, epochs=10, batch_size=32, verbose=0)\n",
        "        mse = mean_squared_error(y_val_reshaped, rnn_model.predict(X_val_reshaped))\n",
        "        if mse < best_rnn_score:\n",
        "            best_rnn_score = mse\n",
        "            best_rnn_params = {'learning_rate': lr, 'n_units': units}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for n_filters in n_filters_options:\n",
        "        for kernel_size in kernel_sizes:\n",
        "            cnn_model = build_cnn_model(lr, n_filters, kernel_size)\n",
        "            cnn_model.fit(X_train_reshaped, y_train_reshaped, epochs=10, batch_size=32, verbose=0)\n",
        "            mse = mean_squared_error(y_val_reshaped, cnn_model.predict(X_val_reshaped))\n",
        "            if mse < best_cnn_score:\n",
        "                best_cnn_score = mse\n",
        "                best_cnn_params = {'learning_rate': lr, 'n_filters': n_filters, 'kernel_size': kernel_size}\n",
        "\n",
        "print(f\"Best LSTM Model: Parameters: {best_lstm_params}, Best MSE: {best_lstm_score}\")\n",
        "print(f\"Best RNN Model: Parameters: {best_rnn_params}, Best MSE: {best_rnn_score}\")\n",
        "print(f\"Best CNN Model: Parameters: {best_cnn_params}, Best MSE: {best_cnn_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmtOJ9yZUj3c"
      },
      "outputs": [],
      "source": [
        "# CELL 9.1: Retrain Regressor Models (RandomForestRegressor, XGBRegressor, GradientBoostingRegressor, LightGBM, and CatBoost)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train_features = load_dataset('O_Reduced_Train_Features.csv')\n",
        "y_train_target = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "X_val = load_dataset('P_Reduced_Val_Features.csv').drop('Date', axis=1)  # Excluding 'Date' for model input\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "\n",
        "missing_features = set(X_train.columns) - set(X_val.columns)\n",
        "print(\"Missing features in X_val:\", missing_features)\n",
        "\n",
        "# Best parameters for each regressor model\n",
        "rf_params = {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
        "xgb_params = {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
        "gbm_params = {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
        "lgb_params = {'learning_rate': 0.01, 'n_estimators': 100, 'num_leaves': 62}\n",
        "catboost_params = {'depth': 4, 'iterations': 100, 'learning_rate': 0.01}\n",
        "\n",
        "# Retrain RandomForest\n",
        "rf_model = RandomForestRegressor(**rf_params)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Retrain XGBoost\n",
        "xgb_model = XGBRegressor(**xgb_params)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Retrain GradientBoosting\n",
        "gbm_model = GradientBoostingRegressor(**gbm_params)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Retrain LightGBM\n",
        "lgb_model = LGBMRegressor(**lgb_params)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Retrain CatBoost\n",
        "catboost_model = CatBoostRegressor(**catboost_params)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Evaluate RandomForest\n",
        "rf_pred = rf_model.predict(X_val)\n",
        "rf_mae = mean_absolute_error(y_val, rf_pred)\n",
        "rf_rmse = mean_squared_error(y_val, rf_pred, squared=False)\n",
        "rf_r2 = r2_score(y_val, rf_pred)\n",
        "\n",
        "# Evaluate XGBoost\n",
        "xgb_pred = xgb_model.predict(X_val)\n",
        "xgb_mae = mean_absolute_error(y_val, xgb_pred)\n",
        "xgb_rmse = mean_squared_error(y_val, xgb_pred, squared=False)\n",
        "xgb_r2 = r2_score(y_val, xgb_pred)\n",
        "\n",
        "# Evaluate GradientBoosting\n",
        "gbm_pred = gbm_model.predict(X_val)\n",
        "gbm_mae = mean_absolute_error(y_val, gbm_pred)\n",
        "gbm_rmse = mean_squared_error(y_val, gbm_pred, squared=False)\n",
        "gbm_r2 = r2_score(y_val, gbm_pred)\n",
        "\n",
        "# Evaluate LightGBM\n",
        "lgb_pred = lgb_model.predict(X_val)\n",
        "lgb_mae = mean_absolute_error(y_val, lgb_pred)\n",
        "lgb_rmse = mean_squared_error(y_val, lgb_pred, squared=False)\n",
        "lgb_r2 = r2_score(y_val, lgb_pred)\n",
        "\n",
        "# Evaluate CatBoost\n",
        "catboost_pred = catboost_model.predict(X_val)\n",
        "catboost_mae = mean_absolute_error(y_val, catboost_pred)\n",
        "catboost_rmse = mean_squared_error(y_val, catboost_pred, squared=False)\n",
        "catboost_r2 = r2_score(y_val, catboost_pred)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(\"Regressor Model Evaluation Metrics:\")\n",
        "print(f\"RandomForest - MAE: {rf_mae}, RMSE: {rf_rmse}, R2: {rf_r2}\")\n",
        "print(f\"XGBoost - MAE: {xgb_mae}, RMSE: {xgb_rmse}, R2: {xgb_r2}\")\n",
        "print(f\"GradientBoosting - MAE: {gbm_mae}, RMSE: {gbm_rmse}, R2: {gbm_r2}\")\n",
        "print(f\"LightGBM - MAE: {lgb_mae}, RMSE: {lgb_rmse}, R2: {lgb_r2}\")\n",
        "print(f\"CatBoost - MAE: {catboost_mae}, RMSE: {catboost_rmse}, R2: {catboost_r2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90SClZlVEY5"
      },
      "outputs": [],
      "source": [
        "# CELL 9.2: Retrain Time Series Models (ARIMA, SARIMA, Prophet)\n",
        "\n",
        "# Import necessary libraries\n",
        "!pip install pmdarima\n",
        "import pmdarima as pm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Function to load dataset and create a DateTime index\n",
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Creating a DateTime index\n",
        "    if 'Year' in df.columns and 'Month' in df.columns and 'Day' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "        df.set_index('Date', inplace=True)\n",
        "        df.drop(['Year', 'Month', 'Day'], axis=1, inplace=True)  # Remove the original columns\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Function to load a dataset\n",
        "def load_dataset(filename):\n",
        "    full_path = f'{base_dir}{filename}'\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "# Load the datasets\n",
        "X_train_features = load_dataset('O_Reduced_Train_Features.csv')\n",
        "y_train_target = load_dataset('O_Reduced_Train_Target.csv')['Prediction1']\n",
        "y_val = load_dataset('P_Reduced_Val_Target.csv')['Prediction1']\n",
        "\n",
        "# Best parameters for each time series model\n",
        "arima_order = (4, 0, 0)\n",
        "sarima_order = (2, 0, 2)\n",
        "sarima_seasonal_order = (1, 1, 2, 12)\n",
        "prophet_params = {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 0.01}\n",
        "\n",
        "# Retrain ARIMA Model\n",
        "arima_model = pm.auto_arima(y_train_fe, seasonal=False, m=12, start_p=0, start_q=0, max_order=6, test='adf', error_action='ignore', suppress_warnings=True, stepwise=True)\n",
        "\n",
        "# Retrain SARIMA Model\n",
        "sarima_model = SARIMAX(y_train_fe, order=sarima_order, seasonal_order=sarima_seasonal_order)\n",
        "sarima_model_fit = sarima_model.fit()\n",
        "\n",
        "# Retrain Prophet Model\n",
        "prophet_model = Prophet(**prophet_params)\n",
        "\n",
        "# Prepare the dataset for Prophet\n",
        "prophet_train_df = y_train_fe.reset_index()  # Reset the index to make 'Date' a column\n",
        "prophet_train_df.rename(columns={'Date': 'ds', 'Prediction1': 'y'}, inplace=True)\n",
        "\n",
        "# Assuming 'Date' column in X_train_features is correctly aligned with y_train_target values\n",
        "# First, ensure X_train_features['Date'] is in datetime format\n",
        "X_train_features['Date'] = pd.to_datetime(X_train_features['Date'])\n",
        "\n",
        "# Then, create prophet_train_df directly using y_train_target\n",
        "prophet_train_df = pd.DataFrame({\n",
        "    'ds': X_train_features['Date'],\n",
        "    'y': y_train_target.values  # Use .values to ensure this works if y_train_target is a Series\n",
        "})\n",
        "\n",
        "# Fit the model\n",
        "prophet_model.fit(prophet_train_df)\n",
        "\n",
        "# Evaluate models\n",
        "\n",
        "# Reset the index to ensure 'Prediction1' is a column\n",
        "y_val_fe_reset = y_val_fe.reset_index()\n",
        "\n",
        "# Evaluate ARIMA\n",
        "arima_pred = arima_model.predict(n_periods=len(y_val_fe_reset))\n",
        "arima_mae = mean_absolute_error(y_val_fe_reset['Prediction1'], arima_pred)\n",
        "arima_rmse = mean_squared_error(y_val_fe_reset['Prediction1'], arima_pred, squared=False)\n",
        "\n",
        "# Evaluate SARIMA\n",
        "sarima_pred = sarima_model_fit.predict(start=y_val_fe_reset.index[0], end=y_val_fe_reset.index[-1])\n",
        "sarima_mae = mean_absolute_error(y_val_fe_reset['Prediction1'], sarima_pred)\n",
        "sarima_rmse = mean_squared_error(y_val_fe_reset['Prediction1'], sarima_pred, squared=False)\n",
        "\n",
        "# Evaluate Prophet\n",
        "future = prophet_model.make_future_dataframe(periods=len(y_val_fe_reset))\n",
        "prophet_pred = prophet_model.predict(future)\n",
        "prophet_mae = mean_absolute_error(y_val_fe_reset['Prediction1'], prophet_pred['yhat'][-len(y_val_fe_reset):])\n",
        "prophet_rmse = mean_squared_error(y_val_fe_reset['Prediction1'], prophet_pred['yhat'][-len(y_val_fe_reset):], squared=False)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(\"Time Series Model Evaluation Metrics:\")\n",
        "print(f\"ARIMA - MAE: {arima_mae}, RMSE: {arima_rmse}\")\n",
        "print(f\"SARIMA - MAE: {sarima_mae}, RMSE: {sarima_rmse}\")\n",
        "print(f\"Prophet - MAE: {prophet_mae}, RMSE: {prophet_rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Xnz5zUVW0a"
      },
      "outputs": [],
      "source": [
        "# CELL 9.3.1: Retrain LSTM Model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Load datasets directly into DataFrame\n",
        "X_train_df = pd.read_csv(base_dir + 'O_Reduced_Train_Features.csv')  # Keep as DataFrame\n",
        "y_train_series = pd.read_csv(base_dir + 'O_Reduced_Train_Target.csv')['Prediction1']  # This stays as Series\n",
        "\n",
        "# No need to convert to .values here, so we can keep the column names for now\n",
        "print(type(X_train_df))\n",
        "\n",
        "# Extract datetime column and keep it aside\n",
        "X_train_datetime = X_train_df[['Date']].copy()  # This assumes 'Date' is the correct column name\n",
        "X_train_numerical = X_train_df.drop('Date', axis=1)  # Drops 'Date' for scaling\n",
        "\n",
        "# Now you can scale X_train_numerical and y_train_series as needed\n",
        "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_numerical)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_series.values.reshape(-1, 1))  # Ensure y_train is a 2D array for scaling\n",
        "\n",
        "# Reshape input to be [samples, time steps, features] for LSTM\n",
        "# Be sure to update the reshaping based on the scaled data's shape\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "\n",
        "# Define the LSTM model with the corrected input shape\n",
        "lstm_model = Sequential([\n",
        "    LSTM(50, input_shape=(1, X_train_scaled.shape[1]), return_sequences=True),  # Adjusted to match the number of features\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
        "\n",
        "# Fit the model\n",
        "history = lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = lstm_model.predict(X_train_reshaped)\n",
        "train_pred = scaler_y.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler_y.inverse_transform(y_train_scaled.reshape(-1, 1))\n",
        "\n",
        "train_mae = mean_absolute_error(train_y, train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train MAE: {train_mae}, Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew7ZHgikLIGQ"
      },
      "outputs": [],
      "source": [
        "# CELL 9.3.2: Retrain RNN Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Load datasets into DataFrame\n",
        "X_train_df = pd.read_csv(base_dir + 'O_Reduced_Train_Features.csv')  # Keep as DataFrame\n",
        "y_train_series = pd.read_csv(base_dir + 'O_Reduced_Train_Target.csv')['Prediction1']  # Keep as Series\n",
        "\n",
        "# Separate 'Date' column if necessary and scale numerical features\n",
        "X_train_numerical = X_train_df.drop(columns=['Date'], errors='ignore')  # Drop 'Date' if it exists\n",
        "\n",
        "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_numerical)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_series.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape input for RNN\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "\n",
        "# Define the RNN model with corrected input shape\n",
        "rnn_model = Sequential([\n",
        "    SimpleRNN(50, input_shape=(1, X_train_scaled.shape[1]), return_sequences=True),\n",
        "    SimpleRNN(50, return_sequences=False),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(optimizer=Adam(lr=0.01), loss='mse')\n",
        "\n",
        "# Fit the model\n",
        "history = rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = rnn_model.predict(X_train_reshaped)\n",
        "train_pred = scaler_y.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler_y.inverse_transform(y_train_scaled.reshape(-1, 1))\n",
        "\n",
        "train_mae = mean_absolute_error(train_y, train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train MAE: {train_mae}, Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4HFryzZLYbN"
      },
      "outputs": [],
      "source": [
        "# CELL 9.3.3: Retrain CNN Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Load datasets into DataFrame\n",
        "X_train_df = pd.read_csv(base_dir + 'O_Reduced_Train_Features.csv')  # Keep as DataFrame\n",
        "y_train_series = pd.read_csv(base_dir + 'O_Reduced_Train_Target.csv')['Prediction1']  # Keep as Series\n",
        "\n",
        "# Exclude 'Date' column if present\n",
        "X_train_numerical = X_train_df.drop(columns=['Date'], errors='ignore')\n",
        "\n",
        "# Normalizing the data\n",
        "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_numerical)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_series.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape data for CNN\n",
        "# Note: CNNs expect 3D input, so we reshape it to [samples, timesteps, features]\n",
        "# Since it's not a time series, 'timesteps' can be 1 or considered as features for Conv1D\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the CNN model with the corrected input shape\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer=Adam(lr=0.01), loss='mse')  # Adjusted learning rate for stability\n",
        "\n",
        "# Fit the model\n",
        "history = cnn_model.fit(X_train_reshaped, y_train_scaled, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_pred_scaled = cnn_model.predict(X_train_reshaped)\n",
        "train_pred = scaler_y.inverse_transform(train_pred_scaled)\n",
        "train_y = scaler_y.inverse_transform(y_train_scaled.reshape(-1, 1))\n",
        "\n",
        "train_mae = mean_absolute_error(train_y, train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n",
        "print(f'Train MAE: {train_mae}, Train RMSE: {train_rmse}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5dCfdIiHnN2"
      },
      "source": [
        "# ***Save Point***\n",
        "## 07/02/24\n",
        "## 4:00am"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wry9TRvzs5bW"
      },
      "outputs": [],
      "source": [
        "# CELL 10.1: Distribution Analysis of all Features\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Load the datasets\n",
        "X_train_reduced = pd.read_csv(base_dir + 'O_Reduced_Train_Features.csv')\n",
        "y_train_reduced = pd.read_csv(base_dir + 'O_Reduced_Train_Target.csv')\n",
        "\n",
        "# Combine the features and target for easier plotting\n",
        "data_train = pd.concat([X_train_reduced, y_train_reduced], axis=1)\n",
        "\n",
        "# Specify features for detailed analysis\n",
        "features = [\n",
        "    \"Day\", \"Month\", \"Year\", \"Date\",\n",
        "    \"interaction_DR1_Prev_Week_Rake_PE_Num_4\",\n",
        "    \"interaction_DR1_Prev_Week_Spirit_PE_Num\",\n",
        "    \"interaction_DR1_2Weeks_Prev_Afternoon\",\n",
        "    \"interaction_Prev_Afternoon_Spirit_PE_Num\",\n",
        "    \"interaction_Prev_Afternoon_Rake_PE_Num_3\",\n",
        "    \"interaction_DR1_Prev_Week_Rake_PE_Num_2\",\n",
        "    \"interaction_Prev_Morning_Rake_PE_Num_2\",\n",
        "    \"interaction_DR1_2Weeks_Spirit_PE_Num\",\n",
        "    \"interaction_DR1_Vert_Avg_Prev_Afternoon\",\n",
        "    \"interaction_Prev_Morning_Spirit_PE_Num\",\n",
        "    \"interaction_DR1_Prev_Week_Prev_Afternoon\",\n",
        "    \"interaction_DR1_Vert_Avg_Rake_PE_Num_2\",\n",
        "    \"interaction_DR1_Mov_Avg_Rake_PE_Num_1\",\n",
        "    \"interaction_DR1_Mov_Avg_Spirit_PE_Num\",\n",
        "    \"interaction_DR1_Prev_Week_Rake_PE_Num_1\",\n",
        "    \"interaction_DR1_Mov_Avg_Rake_PE_Num_2\",\n",
        "    \"interaction_DR1_2Weeks_DR1_Prev_Entry-2\"\n",
        "]\n",
        "\n",
        "# Plotting\n",
        "for feature in features:\n",
        "    if feature in data_train.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(data=data_train, x=feature, bins=30, kde=True)\n",
        "        plt.title(f'Distribution of {feature} vs Prediction1')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.xlabel(feature)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVW2mFQbulKS"
      },
      "outputs": [],
      "source": [
        "# CELL 10.2: Implement feature scoring and selection\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Define the base directory for file paths\n",
        "base_dir = '/content/drive/My Drive/Predictive_Modeling_Four_Draws/Morning_Draw_Model/Draw1_Predictive_Model/'\n",
        "\n",
        "# Load the datasets\n",
        "X_train = pd.read_csv(base_dir + 'K_Engineered_Train_Features.csv')\n",
        "y_train = pd.read_csv(base_dir + 'K_Engineered_Train_Target.csv')['Prediction1']\n",
        "\n",
        "# Initialize a DataFrame to hold the scores\n",
        "feature_scores = pd.DataFrame(index=X_train.columns, columns=['correlation_score'])\n",
        "\n",
        "# Function to score feature based on correlation with the target\n",
        "def score_correlation(feature_data, target_data):\n",
        "    correlation, _ = pearsonr(feature_data, target_data)\n",
        "    # Directly use the correlation coefficient as the score\n",
        "    return abs(correlation)\n",
        "\n",
        "# Calculate correlation scores for each feature\n",
        "for feature in X_train.select_dtypes(include=['number']).columns:\n",
        "    feature_data = X_train[feature]\n",
        "    feature_scores.loc[feature, 'correlation_score'] = score_correlation(feature_data, y_train)\n",
        "\n",
        "# Sort the features based on the correlation score\n",
        "sorted_features = feature_scores.sort_values('correlation_score', ascending=False)\n",
        "\n",
        "# Select the top N features for modeling\n",
        "N = 20  # Number of features to select\n",
        "top_features = sorted_features.head(N).index.tolist()\n",
        "\n",
        "# Output the top features\n",
        "print(f\"Top {N} features based on correlation score:\\n\", top_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-cVJyGW8i09"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8+Whu5GAT/AytEg33EmL9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}